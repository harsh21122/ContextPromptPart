wandb: Currently logged in as: harsh21122. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /ssd-scratch/harsh21122/simple_run/ContextPromptPart/wandb/run-20230209_234622-1220jycj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-terrain-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/harsh21122/part_segmentation
wandb: üöÄ View run at https://wandb.ai/harsh21122/part_segmentation/runs/1220jycj
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Arguments are :  Namespace(batch_size=10, epochs=300, starting_epoch=1, base_lr=0.03, clip_model='RN50', score_map_type='attention', result_dir='./Results/', model_dir='/home/harsh21122/tmp/ContextPromptPart_model', dataset_dir='/home/harsh21122/tmp/cat_dataset', resume=False, model_name='../ContextPromptPart_model/last_model', calc_accuracy_training=False, multi_step_scheduler=True, lr_decay=0.1, milestones=[50, 100, 150, 200, 250, 300], wandb=True, temperature=0.19, layer_len=-1, ref_layer1='relu3_2', ref_layer2='relu5_4', ref_weight1=0.33, ref_weight2=1.0, lamda_contrastive=10.0, lamda_cross=1.0, weight_decay=0.01)
Using device :  cuda
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7f69dc0812c0>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
Printing parameters and their gradient
gamma True
image_encoder.conv1.weight False
image_encoder.bn1.weight False
image_encoder.bn1.bias False
image_encoder.conv2.weight False
image_encoder.bn2.weight False
image_encoder.bn2.bias False
image_encoder.conv3.weight False
image_encoder.bn3.weight False
image_encoder.bn3.bias False
image_encoder.layer1.0.conv1.weight False
image_encoder.layer1.0.bn1.weight False
image_encoder.layer1.0.bn1.bias False
image_encoder.layer1.0.conv2.weight False
image_encoder.layer1.0.bn2.weight False
image_encoder.layer1.0.bn2.bias False
image_encoder.layer1.0.conv3.weight False
image_encoder.layer1.0.bn3.weight False
image_encoder.layer1.0.bn3.bias False
image_encoder.layer1.0.downsample.0.weight False
image_encoder.layer1.0.downsample.1.weight False
image_encoder.layer1.0.downsample.1.bias False
image_encoder.layer1.1.conv1.weight False
image_encoder.layer1.1.bn1.weight False
image_encoder.layer1.1.bn1.bias False
image_encoder.layer1.1.conv2.weight False
image_encoder.layer1.1.bn2.weight False
image_encoder.layer1.1.bn2.bias False
image_encoder.layer1.1.conv3.weight False
image_encoder.layer1.1.bn3.weight False
image_encoder.layer1.1.bn3.bias False
image_encoder.layer1.2.conv1.weight False
image_encoder.layer1.2.bn1.weight False
image_encoder.layer1.2.bn1.bias False
image_encoder.layer1.2.conv2.weight False
image_encoder.layer1.2.bn2.weight False
image_encoder.layer1.2.bn2.bias False
image_encoder.layer1.2.conv3.weight False
image_encoder.layer1.2.bn3.weight False
image_encoder.layer1.2.bn3.bias False
image_encoder.layer2.0.conv1.weight False
image_encoder.layer2.0.bn1.weight False
image_encoder.layer2.0.bn1.bias False
image_encoder.layer2.0.conv2.weight False
image_encoder.layer2.0.bn2.weight False
image_encoder.layer2.0.bn2.bias False
image_encoder.layer2.0.conv3.weight False
image_encoder.layer2.0.bn3.weight False
image_encoder.layer2.0.bn3.bias False
image_encoder.layer2.0.downsample.0.weight False
image_encoder.layer2.0.downsample.1.weight False
image_encoder.layer2.0.downsample.1.bias False
image_encoder.layer2.1.conv1.weight False
image_encoder.layer2.1.bn1.weight False
image_encoder.layer2.1.bn1.bias False
image_encoder.layer2.1.conv2.weight False
image_encoder.layer2.1.bn2.weight False
image_encoder.layer2.1.bn2.bias False
image_encoder.layer2.1.conv3.weight False
image_encoder.layer2.1.bn3.weight False
image_encoder.layer2.1.bn3.bias False
image_encoder.layer2.2.conv1.weight False
image_encoder.layer2.2.bn1.weight False
image_encoder.layer2.2.bn1.bias False
image_encoder.layer2.2.conv2.weight False
image_encoder.layer2.2.bn2.weight False
image_encoder.layer2.2.bn2.bias False
image_encoder.layer2.2.conv3.weight False
image_encoder.layer2.2.bn3.weight False
image_encoder.layer2.2.bn3.bias False
image_encoder.layer2.3.conv1.weight False
image_encoder.layer2.3.bn1.weight False
image_encoder.layer2.3.bn1.bias False
image_encoder.layer2.3.conv2.weight False
image_encoder.layer2.3.bn2.weight False
image_encoder.layer2.3.bn2.bias False
image_encoder.layer2.3.conv3.weight False
image_encoder.layer2.3.bn3.weight False
image_encoder.layer2.3.bn3.bias False
image_encoder.layer3.0.conv1.weight False
image_encoder.layer3.0.bn1.weight False
image_encoder.layer3.0.bn1.bias False
image_encoder.layer3.0.conv2.weight False
image_encoder.layer3.0.bn2.weight False
image_encoder.layer3.0.bn2.bias False
image_encoder.layer3.0.conv3.weight False
image_encoder.layer3.0.bn3.weight False
image_encoder.layer3.0.bn3.bias False
image_encoder.layer3.0.downsample.0.weight False
image_encoder.layer3.0.downsample.1.weight False
image_encoder.layer3.0.downsample.1.bias False
image_encoder.layer3.1.conv1.weight False
image_encoder.layer3.1.bn1.weight False
image_encoder.layer3.1.bn1.bias False
image_encoder.layer3.1.conv2.weight False
image_encoder.layer3.1.bn2.weight False
image_encoder.layer3.1.bn2.bias False
image_encoder.layer3.1.conv3.weight False
image_encoder.layer3.1.bn3.weight False
image_encoder.layer3.1.bn3.bias False
image_encoder.layer3.2.conv1.weight False
image_encoder.layer3.2.bn1.weight False
image_encoder.layer3.2.bn1.bias False
image_encoder.layer3.2.conv2.weight False
image_encoder.layer3.2.bn2.weight False
image_encoder.layer3.2.bn2.bias False
image_encoder.layer3.2.conv3.weight False
image_encoder.layer3.2.bn3.weight False
image_encoder.layer3.2.bn3.bias False
image_encoder.layer3.3.conv1.weight False
image_encoder.layer3.3.bn1.weight False
image_encoder.layer3.3.bn1.bias False
image_encoder.layer3.3.conv2.weight False
image_encoder.layer3.3.bn2.weight False
image_encoder.layer3.3.bn2.bias False
image_encoder.layer3.3.conv3.weight False
image_encoder.layer3.3.bn3.weight False
image_encoder.layer3.3.bn3.bias False
image_encoder.layer3.4.conv1.weight False
image_encoder.layer3.4.bn1.weight False
image_encoder.layer3.4.bn1.bias False
image_encoder.layer3.4.conv2.weight False
image_encoder.layer3.4.bn2.weight False
image_encoder.layer3.4.bn2.bias False
image_encoder.layer3.4.conv3.weight False
image_encoder.layer3.4.bn3.weight False
image_encoder.layer3.4.bn3.bias False
image_encoder.layer3.5.conv1.weight False
image_encoder.layer3.5.bn1.weight False
image_encoder.layer3.5.bn1.bias False
image_encoder.layer3.5.conv2.weight False
image_encoder.layer3.5.bn2.weight False
image_encoder.layer3.5.bn2.bias False
image_encoder.layer3.5.conv3.weight False
image_encoder.layer3.5.bn3.weight False
image_encoder.layer3.5.bn3.bias False
image_encoder.layer4.0.conv1.weight False
image_encoder.layer4.0.bn1.weight False
image_encoder.layer4.0.bn1.bias False
image_encoder.layer4.0.conv2.weight False
image_encoder.layer4.0.bn2.weight False
image_encoder.layer4.0.bn2.bias False
image_encoder.layer4.0.conv3.weight False
image_encoder.layer4.0.bn3.weight False
image_encoder.layer4.0.bn3.bias False
image_encoder.layer4.0.downsample.0.weight False
image_encoder.layer4.0.downsample.1.weight False
image_encoder.layer4.0.downsample.1.bias False
image_encoder.layer4.1.conv1.weight False
image_encoder.layer4.1.bn1.weight False
image_encoder.layer4.1.bn1.bias False
image_encoder.layer4.1.conv2.weight False
image_encoder.layer4.1.bn2.weight False
image_encoder.layer4.1.bn2.bias False
image_encoder.layer4.1.conv3.weight False
image_encoder.layer4.1.bn3.weight False
image_encoder.layer4.1.bn3.bias False
image_encoder.layer4.2.conv1.weight False
image_encoder.layer4.2.bn1.weight False
image_encoder.layer4.2.bn1.bias False
image_encoder.layer4.2.conv2.weight False
image_encoder.layer4.2.bn2.weight False
image_encoder.layer4.2.bn2.bias False
image_encoder.layer4.2.conv3.weight False
image_encoder.layer4.2.bn3.weight False
image_encoder.layer4.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv1.weight True
decoder.bn1.weight True
decoder.bn1.bias True
decoder.layer1.0.conv1.weight True
decoder.layer1.0.bn1.weight True
decoder.layer1.0.bn1.bias True
decoder.layer1.0.conv2.weight True
decoder.layer1.0.bn2.weight True
decoder.layer1.0.bn2.bias True
decoder.layer1.0.conv3.weight True
decoder.layer1.0.bn3.weight True
decoder.layer1.0.bn3.bias True
decoder.layer1.0.shortcut.0.weight True
decoder.layer1.0.shortcut.1.weight True
decoder.layer1.0.shortcut.1.bias True
decoder.layer1.1.conv1.weight True
decoder.layer1.1.bn1.weight True
decoder.layer1.1.bn1.bias True
decoder.layer1.1.conv2.weight True
decoder.layer1.1.bn2.weight True
decoder.layer1.1.bn2.bias True
decoder.layer1.1.conv3.weight True
decoder.layer1.1.bn3.weight True
decoder.layer1.1.bn3.bias True
decoder.layer1.2.conv1.weight True
decoder.layer1.2.bn1.weight True
decoder.layer1.2.bn1.bias True
decoder.layer1.2.conv2.weight True
decoder.layer1.2.bn2.weight True
decoder.layer1.2.bn2.bias True
decoder.layer1.2.conv3.weight True
decoder.layer1.2.bn3.weight True
decoder.layer1.2.bn3.bias True
decoder.layer2.0.conv1.weight True
decoder.layer2.0.bn1.weight True
decoder.layer2.0.bn1.bias True
decoder.layer2.0.conv2.weight True
decoder.layer2.0.bn2.weight True
decoder.layer2.0.bn2.bias True
decoder.layer2.0.conv3.weight True
decoder.layer2.0.bn3.weight True
decoder.layer2.0.bn3.bias True
decoder.layer2.0.shortcut.0.weight True
decoder.layer2.0.shortcut.1.weight True
decoder.layer2.0.shortcut.1.bias True
decoder.layer2.1.conv1.weight True
decoder.layer2.1.bn1.weight True
decoder.layer2.1.bn1.bias True
decoder.layer2.1.conv2.weight True
decoder.layer2.1.bn2.weight True
decoder.layer2.1.bn2.bias True
decoder.layer2.1.conv3.weight True
decoder.layer2.1.bn3.weight True
decoder.layer2.1.bn3.bias True
decoder.layer2.2.conv1.weight True
decoder.layer2.2.bn1.weight True
decoder.layer2.2.bn1.bias True
decoder.layer2.2.conv2.weight True
decoder.layer2.2.bn2.weight True
decoder.layer2.2.bn2.bias True
decoder.layer2.2.conv3.weight True
decoder.layer2.2.bn3.weight True
decoder.layer2.2.bn3.bias True
decoder.layer2.3.conv1.weight True
decoder.layer2.3.bn1.weight True
decoder.layer2.3.bn1.bias True
decoder.layer2.3.conv2.weight True
decoder.layer2.3.bn2.weight True
decoder.layer2.3.bn2.bias True
decoder.layer2.3.conv3.weight True
decoder.layer2.3.bn3.weight True
decoder.layer2.3.bn3.bias True
decoder.layer3.0.conv1.weight True
decoder.layer3.0.bn1.weight True
decoder.layer3.0.bn1.bias True
decoder.layer3.0.conv2.weight True
decoder.layer3.0.bn2.weight True
decoder.layer3.0.bn2.bias True
decoder.layer3.0.conv3.weight True
decoder.layer3.0.bn3.weight True
decoder.layer3.0.bn3.bias True
decoder.layer3.0.shortcut.0.weight True
decoder.layer3.0.shortcut.1.weight True
decoder.layer3.0.shortcut.1.bias True
decoder.layer3.1.conv1.weight True
decoder.layer3.1.bn1.weight True
decoder.layer3.1.bn1.bias True
decoder.layer3.1.conv2.weight True
decoder.layer3.1.bn2.weight True
decoder.layer3.1.bn2.bias True
decoder.layer3.1.conv3.weight True
decoder.layer3.1.bn3.weight True
decoder.layer3.1.bn3.bias True
decoder.layer3.2.conv1.weight True
decoder.layer3.2.bn1.weight True
decoder.layer3.2.bn1.bias True
decoder.layer3.2.conv2.weight True
decoder.layer3.2.bn2.weight True
decoder.layer3.2.bn2.bias True
decoder.layer3.2.conv3.weight True
decoder.layer3.2.bn3.weight True
decoder.layer3.2.bn3.bias True
decoder.layer3.3.conv1.weight True
decoder.layer3.3.bn1.weight True
decoder.layer3.3.bn1.bias True
decoder.layer3.3.conv2.weight True
decoder.layer3.3.bn2.weight True
decoder.layer3.3.bn2.bias True
decoder.layer3.3.conv3.weight True
decoder.layer3.3.bn3.weight True
decoder.layer3.3.bn3.bias True
decoder.layer3.4.conv1.weight True
decoder.layer3.4.bn1.weight True
decoder.layer3.4.bn1.bias True
decoder.layer3.4.conv2.weight True
decoder.layer3.4.bn2.weight True
decoder.layer3.4.bn2.bias True
decoder.layer3.4.conv3.weight True
decoder.layer3.4.bn3.weight True
decoder.layer3.4.bn3.bias True
decoder.layer3.5.conv1.weight True
decoder.layer3.5.bn1.weight True
decoder.layer3.5.bn1.bias True
decoder.layer3.5.conv2.weight True
decoder.layer3.5.bn2.weight True
decoder.layer3.5.bn2.bias True
decoder.layer3.5.conv3.weight True
decoder.layer3.5.bn3.weight True
decoder.layer3.5.bn3.bias True
decoder.layer4.0.conv1.weight True
decoder.layer4.0.bn1.weight True
decoder.layer4.0.bn1.bias True
decoder.layer4.0.conv2.weight True
decoder.layer4.0.bn2.weight True
decoder.layer4.0.bn2.bias True
decoder.layer4.0.conv3.weight True
decoder.layer4.0.bn3.weight True
decoder.layer4.0.bn3.bias True
decoder.layer4.0.shortcut.0.weight True
decoder.layer4.0.shortcut.1.weight True
decoder.layer4.0.shortcut.1.bias True
decoder.layer4.1.conv1.weight True
decoder.layer4.1.bn1.weight True
decoder.layer4.1.bn1.bias True
decoder.layer4.1.conv2.weight True
decoder.layer4.1.bn2.weight True
decoder.layer4.1.bn2.bias True
decoder.layer4.1.conv3.weight True
decoder.layer4.1.bn3.weight True
decoder.layer4.1.bn3.bias True
decoder.layer4.2.conv1.weight True
decoder.layer4.2.bn1.weight True
decoder.layer4.2.bn1.bias True
decoder.layer4.2.conv2.weight True
decoder.layer4.2.bn2.weight True
decoder.layer4.2.bn2.bias True
decoder.layer4.2.conv3.weight True
decoder.layer4.2.bn3.weight True
decoder.layer4.2.bn3.bias True
decoder.toplayer.weight True
decoder.toplayer.bias True
decoder.smooth1.weight True
decoder.smooth1.bias True
decoder.smooth2.weight True
decoder.smooth2.bias True
decoder.smooth3.weight True
decoder.smooth3.bias True
decoder.latlayer1.weight True
decoder.latlayer1.bias True
decoder.latlayer2.weight True
decoder.latlayer2.bias True
decoder.latlayer3.weight True
decoder.latlayer3.bias True
decoder.semantic_branch.weight True
decoder.semantic_branch.bias True
decoder.conv2.weight True
decoder.conv2.bias True
decoder.conv3.weight True
decoder.conv3.bias True
decoder.gn1.weight True
decoder.gn1.bias True
decoder.gn2.weight True
decoder.gn2.bias True
Total epochs to be executed :  300
EPOCH 1:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.2258737087249756 4.343443870544434 45.66031265258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  3.395491600036621 4.413319110870361 47.528682708740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  3.0551862716674805 4.169425964355469 44.749446868896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.705953598022461 4.3494439125061035 46.20039367675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.1118781566619873 4.213383197784424 44.24570846557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.4028515815734863 4.1279497146606445 43.68234634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.6353487968444824 4.1642985343933105 44.27833557128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.6091268062591553 4.301302909851074 45.62215805053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.708830714225769 4.322967529296875 44.938507080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6290806531906128 4.102241039276123 42.651493072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4367562532424927 4.178643703460693 43.22319412231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5889314413070679 4.084466934204102 42.43360137939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.883960247039795 4.423448085784912 46.11844253540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.8737436532974243 4.315703392028809 45.03078079223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.5767390727996826 4.234775066375732 44.92449188232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.602532386779785 4.268839359283447 45.290924072265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3898096084594727 4.3969597816467285 45.359405517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6728218793869019 4.261545181274414 44.288272857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5947109460830688 4.3084492683410645 44.679203033447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7651934623718262 4.076024055480957 42.52543258666992
  batch 20 loss: 1.7651934623718262, 4.076024055480957, 42.52543258666992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7243914604187012 4.096289157867432 42.68728256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5979684591293335 3.9816017150878906 41.41398620605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.466189980506897 4.196450233459473 43.43069076538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7149152755737305 4.134551048278809 43.060428619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6454998254776 4.319308280944824 44.838584899902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4609545469284058 4.0053839683532715 41.514793395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4476261138916016 4.16924524307251 43.14007568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3786020278930664 3.9510374069213867 40.88897705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  0.9551365375518799 4.0625901222229 41.58103561401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.366479516029358 4.03373384475708 41.70381546020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.2027620077133179 4.063462734222412 41.8373908996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.422635793685913 4.225795745849609 43.68059158325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.2859892845153809 3.993950605392456 41.225494384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3693557977676392 4.382543087005615 45.194786071777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3107300996780396 4.275003433227539 44.06076431274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4334076642990112 4.283782005310059 44.2712287902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3615823984146118 4.1338419914245605 42.70000457763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5023066997528076 3.929581880569458 40.798126220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4585955142974854 4.009316921234131 41.55176544189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6161820888519287 3.9464616775512695 41.08079528808594
  batch 40 loss: 1.6161820888519287, 3.9464616775512695, 41.08079528808594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.0070860385894775 3.8079159259796143 40.086246490478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.140883445739746 3.8119499683380127 40.26038360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.459474563598633 3.7485594749450684 39.945068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.2141804695129395 3.695549964904785 39.169677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.340266227722168 3.7609941959381104 39.95021057128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  2.009808301925659 4.130025863647461 43.31006622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7545300722122192 3.327110767364502 35.025638580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7333368062973022 3.9696366786956787 41.42970275878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6991316080093384 4.28154993057251 44.51462936401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3702740669250488 3.6490590572357178 37.860862731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4720515012741089 4.134681701660156 42.81886672973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.507681965827942 3.3145430088043213 34.653114318847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4117672443389893 4.3489909172058105 44.901676177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6567431688308716 4.227629661560059 43.933040618896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4255298376083374 3.532618761062622 36.75171661376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6682924032211304 3.7009756565093994 38.67805099487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4705827236175537 3.6157760620117188 37.62834167480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4214091300964355 4.07292366027832 42.1506462097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4578146934509277 3.8876657485961914 40.334468841552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.8273428678512573 3.370753526687622 35.53487777709961
  batch 60 loss: 1.8273428678512573, 3.370753526687622, 35.53487777709961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3353770971298218 3.7085986137390137 38.421363830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4294519424438477 3.6483514308929443 37.9129638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4946211576461792 3.1537868976593018 33.03248977661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3491989374160767 3.268596887588501 34.0351676940918
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.2677998542785645 2.6098644733428955 27.366443634033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.309791922569275 3.472276449203491 36.032554626464844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3662728071212769 3.5116493701934814 36.482765197753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3907371759414673 3.5049281120300293 36.44001770019531
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.3936831951141357 3.2585909366607666 33.97959518432617
Total LOSS train 41.75073025043194 valid 35.73373317718506
CE LOSS train 1.7463195085525514 valid 0.34842079877853394
Contrastive LOSS train 4.0004410853752725 valid 0.8146477341651917
Saved best model. Old loss 1000000.0 and new best loss 35.73373317718506
EPOCH 2:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4806358814239502 3.4025814533233643 35.50645065307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5136207342147827 3.558316230773926 37.09678268432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4033399820327759 2.9147512912750244 30.550851821899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4530377388000488 3.436624765396118 35.81928634643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.513967514038086 3.543255090713501 36.94651794433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3986382484436035 3.7646217346191406 39.044857025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4657459259033203 3.3267836570739746 34.73358154296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4145971536636353 3.172611951828003 33.140716552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4574003219604492 3.4978203773498535 36.435604095458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4396677017211914 3.2467453479766846 33.90711975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.466036081314087 3.8321831226348877 39.78786849975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4784975051879883 3.385922431945801 35.33771896362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4373729228973389 3.236314535140991 33.80051803588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4531017541885376 3.059008836746216 32.043190002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5853453874588013 3.062523126602173 32.210575103759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.508571982383728 3.299504518508911 34.50361633300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4500380754470825 3.66019868850708 38.052024841308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5021770000457764 3.545849084854126 36.960670471191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4933300018310547 3.0299570560455322 31.79290008544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5604313611984253 2.9592785835266113 31.153215408325195
  batch 20 loss: 1.5604313611984253, 2.9592785835266113, 31.153215408325195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5006601810455322 2.8224384784698486 29.72504425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.475895881652832 3.2198588848114014 33.67448425292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4801818132400513 2.819706439971924 29.67724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5025255680084229 3.3293938636779785 34.79646301269531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5963934659957886 3.667203426361084 38.268428802490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4675328731536865 3.6299970149993896 37.76750183105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5374364852905273 3.555136203765869 37.08879852294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.492583155632019 4.156981945037842 43.06240463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3928563594818115 3.582042694091797 37.21328353881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5264571905136108 3.600835084915161 37.53480911254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3669520616531372 2.9172463417053223 30.53941535949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.526607632637024 2.7468671798706055 28.99527931213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4879332780838013 3.763559579849243 39.123531341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.508463740348816 3.628549337387085 37.7939567565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4318740367889404 3.2203633785247803 33.63550567626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4852665662765503 2.9987709522247314 31.47297477722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.479386806488037 3.8937339782714844 40.416725158691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.609736680984497 2.7106447219848633 28.716184616088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6421648263931274 3.211178779602051 33.75395202636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6377341747283936 2.9078738689422607 30.716472625732422
  batch 40 loss: 1.6377341747283936, 2.9078738689422607, 30.716472625732422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5722718238830566 3.1753952503204346 33.32622528076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.548718810081482 3.1511270999908447 33.05998992919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5791382789611816 3.0077123641967773 31.656261444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.605751395225525 3.1908531188964844 33.5142822265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5856529474258423 3.6046066284179688 37.631717681884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606135368347168 2.6988096237182617 28.59423065185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6606948375701904 3.888686418533325 40.54755783081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5562009811401367 3.388392686843872 35.440128326416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.65391206741333 2.4720776081085205 26.37468719482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5834243297576904 2.9113757610321045 30.697181701660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6396287679672241 2.560650587081909 27.246135711669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6348564624786377 2.6804404258728027 28.43926239013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.584725022315979 3.295241355895996 34.537139892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5997060537338257 4.234135150909424 43.94105529785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5153404474258423 4.274898052215576 44.264320373535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5730646848678589 4.26603364944458 44.2333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4860421419143677 4.154635429382324 43.03239822387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4474787712097168 3.918581962585449 40.63330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4481728076934814 3.2141454219818115 33.58962631225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6608637571334839 3.044783353805542 32.10869598388672
  batch 60 loss: 1.6608637571334839, 3.044783353805542, 32.10869598388672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4366650581359863 3.617711067199707 37.613773345947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.527144432067871 3.3869788646698 35.396934509277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4977128505706787 3.412836790084839 35.62607955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4590108394622803 3.641645908355713 37.87546920776367
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4149794578552246 2.7303497791290283 28.718477249145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5126478672027588 3.3093388080596924 34.60603713989258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5366801023483276 3.239190101623535 33.9285774230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5428197383880615 3.1035985946655273 32.57880401611328
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5701311826705933 3.241762638092041 33.987754821777344
Total LOSS train 34.93684396010179 valid 33.77529335021973
CE LOSS train 1.5154075145721435 valid 0.3925327956676483
Contrastive LOSS train 3.342143667661227 valid 0.8104406595230103
Saved best model. Old loss 35.73373317718506 and new best loss 33.77529335021973
EPOCH 3:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5691652297973633 2.939936876296997 30.96853256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5901262760162354 3.8417952060699463 40.008079528808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5017176866531372 4.042050838470459 41.92222595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5129073858261108 2.9614267349243164 31.127174377441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5556381940841675 3.30664324760437 34.6220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4306875467300415 3.4578921794891357 36.00960922241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5209397077560425 3.1404643058776855 32.92558288574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4698776006698608 3.4593896865844727 36.06377410888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.456191062927246 2.785552501678467 29.311717987060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5299959182739258 3.2418949604034424 33.948944091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4449774026870728 3.3995044231414795 35.44002151489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.471400499343872 3.726501226425171 38.736412048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.46869957447052 3.403226375579834 35.5009651184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4802991151809692 2.6667535305023193 28.14783477783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.598158597946167 2.954141855239868 31.139575958251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5856925249099731 3.2277328968048096 33.86302185058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.436250925064087 2.3996481895446777 25.4327335357666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5085830688476562 2.430241346359253 25.810997009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4651823043823242 3.5832982063293457 37.29816436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5746089220046997 3.3339715003967285 34.91432189941406
  batch 20 loss: 1.5746089220046997, 3.3339715003967285, 34.91432189941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4954111576080322 3.383718967437744 35.33259963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4278090000152588 4.337639808654785 44.80420684814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.477065920829773 3.3236680030822754 34.7137451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5159839391708374 3.625126361846924 37.76724624633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.598740577697754 3.2413580417633057 34.01232147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.482563853263855 3.1371829509735107 32.854393005371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5318715572357178 4.082625865936279 42.358131408691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5152586698532104 4.116085529327393 42.67611312866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3770815134048462 2.732235908508301 28.699440002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5813864469528198 3.792311191558838 39.50449752807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3783875703811646 3.237964630126953 33.758033752441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5371708869934082 3.3595387935638428 35.1325569152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4632866382598877 3.9852306842803955 41.31559371948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4577240943908691 3.530886173248291 36.76658630371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.37539541721344 2.431136131286621 25.686756134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4031238555908203 3.5672988891601562 37.07611083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3976025581359863 3.072641134262085 32.12401580810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5501738786697388 3.2558748722076416 34.108924865722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5244463682174683 3.5797693729400635 37.322139739990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5693447589874268 4.149776935577393 43.06711196899414
  batch 40 loss: 1.5693447589874268, 4.149776935577393, 43.06711196899414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4849551916122437 3.4819324016571045 36.30427932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4281995296478271 3.8768506050109863 40.19670486450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4371930360794067 3.060047149658203 32.03766632080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4557278156280518 3.457332134246826 36.029048919677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.393009066581726 3.4360482692718506 35.75349044799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4679020643234253 4.085357189178467 42.321475982666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5686002969741821 3.019676446914673 31.765363693237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4343265295028687 3.3364012241363525 34.79833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5726265907287598 3.2900876998901367 34.473506927490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.45072340965271 2.2066969871520996 23.5176944732666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5524438619613647 3.1182403564453125 32.73484802246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5053293704986572 3.4832422733306885 36.33775329589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.46164071559906 3.954843044281006 41.01007080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5071735382080078 3.2169761657714844 33.67693328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4452511072158813 2.8886942863464355 30.332195281982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5511329174041748 2.7214224338531494 28.765356063842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.434464931488037 2.957737922668457 31.011844635009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4030884504318237 3.0549941062927246 31.95302963256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4206792116165161 4.145981311798096 42.8804931640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5948355197906494 3.3607442378997803 35.20227813720703
  batch 60 loss: 1.5948355197906494, 3.3607442378997803, 35.20227813720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.391965627670288 4.036424160003662 41.75620651245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4651098251342773 3.7033185958862305 38.49829864501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.411120891571045 3.2172281742095947 33.583404541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3925784826278687 3.630831480026245 37.70089340209961
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.334761619567871 2.876330614089966 30.098068237304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4238616228103638 3.087810754776001 32.30196762084961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.453682780265808 2.9043257236480713 30.496938705444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4603784084320068 3.0066189765930176 31.526567459106445
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.4473849534988403 2.971313953399658 31.160524368286133
Total LOSS train 35.00017735407903 valid 31.37149953842163
CE LOSS train 1.4830118124301617 valid 0.3618462383747101
Contrastive LOSS train 3.3517165477459248 valid 0.7428284883499146
Saved best model. Old loss 33.77529335021973 and new best loss 31.37149953842163
EPOCH 4:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4875216484069824 2.8437612056732178 29.925134658813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5173442363739014 3.4003477096557617 35.52082443237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4461772441864014 3.3393394947052 34.83957290649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4651334285736084 3.2120063304901123 33.58519744873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5221874713897705 3.5337915420532227 36.86009979248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4013205766677856 2.953253984451294 30.933860778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5065267086029053 2.9610114097595215 31.116641998291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4670687913894653 2.5823819637298584 27.290889739990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4487957954406738 3.626434326171875 37.713138580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5228455066680908 3.199906587600708 33.52191162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4203851222991943 3.0295934677124023 31.716320037841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.41608726978302 3.216925621032715 33.58534622192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.424031138420105 3.214040517807007 33.56443405151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4556716680526733 3.0572898387908936 32.02857208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.583121657371521 2.891860008239746 30.50172233581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5789684057235718 3.1869993209838867 33.4489631652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4373582601547241 2.7072434425354004 28.50979232788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4953902959823608 3.4875552654266357 36.37094497680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4445492029190063 3.3442068099975586 34.886619567871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.569071650505066 2.8527145385742188 30.096216201782227
  batch 20 loss: 1.569071650505066, 2.8527145385742188, 30.096216201782227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5265700817108154 3.762495994567871 39.15153121948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4534692764282227 2.7496862411499023 28.950332641601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.500152349472046 3.556194543838501 37.06209945678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5551060438156128 3.3492093086242676 35.04719924926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6048563718795776 2.7647461891174316 29.252317428588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4958012104034424 3.1898796558380127 33.394596099853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5244313478469849 3.5575361251831055 37.099796295166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5362735986709595 2.8459715843200684 29.995990753173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.400831937789917 3.351149797439575 34.912330627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.592645287513733 3.3760643005371094 35.35328674316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4147183895111084 3.5015976428985596 36.430694580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5786492824554443 3.170366048812866 33.282310485839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.526152491569519 3.3979530334472656 35.50568389892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.51458740234375 3.3275632858276367 34.79022216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4303017854690552 3.234685182571411 33.77715301513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4096535444259644 3.8443360328674316 39.85301208496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4304728507995605 2.983267307281494 31.263145446777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5883150100708008 2.9262261390686035 30.850574493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6140472888946533 2.6883275508880615 28.497323989868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.638188362121582 3.1820895671844482 33.459083557128906
  batch 40 loss: 1.638188362121582, 3.1820895671844482, 33.459083557128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5555623769760132 3.155423879623413 33.10980224609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5251941680908203 3.3658974170684814 35.184165954589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5163990259170532 3.413113594055176 35.64753341674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5428448915481567 3.2217066287994385 33.75991439819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4703596830368042 2.8960680961608887 30.431041717529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5257278680801392 3.2274839878082275 33.800567626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6270525455474854 3.5174901485443115 36.80195236206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5315006971359253 2.967432737350464 31.205827713012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6325589418411255 3.4801416397094727 36.43397521972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5265363454818726 2.850407600402832 30.03061294555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6200355291366577 3.148561954498291 33.105655670166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.576492190361023 3.7022018432617188 38.5985107421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5248185396194458 2.5662450790405273 27.18726921081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5872081518173218 4.1519904136657715 43.107112884521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5177282094955444 3.312588930130005 34.64361572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6194707155227661 3.3766376972198486 35.38584899902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4967875480651855 3.3231701850891113 34.72848892211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4555846452713013 3.4458796977996826 35.91438293457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4777754545211792 3.2757227420806885 34.23500442504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6404401063919067 3.0035531520843506 31.67597198486328
  batch 60 loss: 1.6404401063919067, 3.0035531520843506, 31.67597198486328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4737913608551025 3.245619297027588 33.92998504638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5528132915496826 3.4688453674316406 36.241268157958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4808226823806763 2.7022855281829834 28.503679275512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4784542322158813 3.099964141845703 32.47809600830078
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4333884716033936 2.183974027633667 23.273128509521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5131982564926147 3.4991462230682373 36.504661560058594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.539810061454773 3.189521312713623 33.43502426147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5449230670928955 3.4098098278045654 35.64302062988281
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5366398096084595 3.180515766143799 33.341796875
Total LOSS train 33.43628152700571 valid 34.731125831604004
CE LOSS train 1.512832733301016 valid 0.38415995240211487
Contrastive LOSS train 3.1923448415902946 valid 0.7951289415359497
EPOCH 5:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5706775188446045 3.085463285446167 32.42531204223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6026298999786377 3.5510551929473877 37.113182067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5171338319778442 3.9498178958892822 41.015316009521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5240216255187988 2.870501756668091 30.22903823852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5667191743850708 3.439262866973877 35.959346771240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4637577533721924 3.537858486175537 36.842342376708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5502036809921265 3.8320705890655518 39.87091064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.511069655418396 3.2023885250091553 33.53495407104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4980849027633667 3.4591281414031982 36.0893669128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.582499384880066 3.5034635066986084 36.61713409423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4769501686096191 3.0753164291381836 32.2301139831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4656925201416016 3.6005825996398926 37.471519470214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4579375982284546 3.7435388565063477 38.89332580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4649401903152466 3.081806182861328 32.28300094604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.595667839050293 2.9904773235321045 31.500442504882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5626788139343262 3.1859781742095947 33.42245864868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4312679767608643 2.786146879196167 29.29273796081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.488492488861084 3.186985969543457 33.35835266113281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4306459426879883 3.5447003841400146 36.877647399902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5579923391342163 3.272473096847534 34.28272247314453
  batch 20 loss: 1.5579923391342163, 3.272473096847534, 34.28272247314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4861747026443481 3.0018093585968018 31.504268646240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.415592074394226 3.365025281906128 35.06584548950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4559273719787598 3.1750030517578125 33.20595932006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4999150037765503 4.001287460327148 41.51279067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5467660427093506 3.4934322834014893 36.48108673095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4464179277420044 3.1575703620910645 33.02212142944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5018736124038696 3.167607545852661 33.177947998046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4843724966049194 3.2418341636657715 33.902713775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3788361549377441 3.4028584957122803 35.40742111206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5680352449417114 3.5091755390167236 36.6597900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.373978614807129 3.4989631175994873 36.363609313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5318127870559692 3.567884683609009 37.210662841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4728819131851196 3.7950057983398438 39.42293930053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.467650055885315 4.06160306930542 42.083683013916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.3923838138580322 3.300485372543335 34.39723587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.432194471359253 3.423234224319458 35.66453552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4396147727966309 4.101550579071045 42.45512008666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.568596363067627 3.406724452972412 35.635841369628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5788276195526123 3.1966590881347656 33.54541778564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6063933372497559 3.049102544784546 32.09741973876953
  batch 40 loss: 1.6063933372497559, 3.049102544784546, 32.09741973876953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5191659927368164 3.0671613216400146 32.19078063964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4872058629989624 3.396986246109009 35.457069396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4711344242095947 3.9885737895965576 41.35687255859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4876810312271118 3.5279290676116943 36.766971588134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4461357593536377 3.73852276802063 38.831363677978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4927477836608887 3.709831953048706 38.591068267822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5842818021774292 3.8447465896606445 40.03174591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4726654291152954 3.751248598098755 38.98514938354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5851901769638062 3.3088443279266357 34.67363357543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4725770950317383 2.9313371181488037 30.78594970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5675349235534668 3.2587313652038574 34.154850006103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5490953922271729 3.3570239543914795 35.11933517456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5060051679611206 3.1804840564727783 33.31084442138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5685970783233643 3.569549083709717 37.26408767700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5091801881790161 3.0797970294952393 32.307151794433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6138808727264404 3.5415053367614746 37.028934478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4929594993591309 3.2674448490142822 34.16740798950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4769865274429321 3.475306749343872 36.23005294799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.512907862663269 4.328524589538574 44.79815673828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6069221496582031 3.7686219215393066 39.29314041137695
  batch 60 loss: 1.6069221496582031, 3.7686219215393066, 39.29314041137695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4534943103790283 3.6423704624176025 37.877201080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5280472040176392 3.906717538833618 40.59522247314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4598697423934937 4.302138805389404 44.481258392333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4394495487213135 3.449481964111328 35.934268951416016
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4032710790634155 3.0263750553131104 31.667020797729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4817262887954712 3.2762794494628906 34.24452209472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5030828714370728 3.4836435317993164 36.339515686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5025302171707153 3.585515260696411 37.35768127441406
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.501777172088623 3.6983346939086914 38.48512268066406
Total LOSS train 36.00038727980394 valid 36.60671043395996
CE LOSS train 1.5026814552453849 valid 0.37544429302215576
Contrastive LOSS train 3.4497705716353195 valid 0.9245836734771729
EPOCH 6:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5288734436035156 3.651153564453125 38.040409088134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5700323581695557 3.7582528591156006 39.15256118774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.514595627784729 3.270716667175293 34.221763610839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5404139757156372 3.037546396255493 31.915878295898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6037068367004395 3.4113032817840576 35.716739654541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.524178147315979 3.311474323272705 34.638919830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5985851287841797 3.6268832683563232 37.86741638183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5566176176071167 3.171290397644043 33.2695198059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5439774990081787 3.1325876712799072 32.86985397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6179267168045044 3.770433187484741 39.32225799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.525725245475769 3.70894455909729 38.61517333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5281364917755127 2.8073625564575195 29.601762771606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5247169733047485 3.5182340145111084 36.707054138183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5416033267974854 3.8323519229888916 39.8651237487793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6457393169403076 3.7670438289642334 39.31617736816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633027195930481 3.329747200012207 34.93049621582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5343834161758423 2.7392897605895996 28.927282333374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5758687257766724 2.6562507152557373 28.138376235961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5183025598526 3.2633883953094482 34.15218734741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6399437189102173 2.7178289890289307 28.818233489990234
  batch 20 loss: 1.6399437189102173, 2.7178289890289307, 28.818233489990234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5859475135803223 4.490203857421875 46.48798751831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.525342583656311 4.203345775604248 43.558799743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5746448040008545 3.6506638526916504 38.08128356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5930804014205933 3.485260248184204 36.445682525634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643602728843689 3.94032883644104 41.04689407348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5516746044158936 3.3360049724578857 34.91172409057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5721096992492676 3.1208412647247314 32.780521392822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5834602117538452 2.9915390014648438 31.498849868774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4809272289276123 2.7252275943756104 28.73320198059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6391342878341675 3.5177557468414307 36.81669235229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.454588770866394 2.7639214992523193 29.09380340576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5903749465942383 3.613266706466675 37.72304153442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5571640729904175 3.3685975074768066 35.24313735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5499640703201294 3.2058260440826416 33.60822677612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4856384992599487 3.256828546524048 34.053924560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.495287299156189 2.9298324584960938 30.793611526489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5192667245864868 3.2978453636169434 34.49772262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6241003274917603 2.359673500061035 25.220834732055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466299295425415 3.6675515174865723 38.3221435546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6761082410812378 3.817310333251953 39.849212646484375
  batch 40 loss: 1.6761082410812378, 3.817310333251953, 39.849212646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.566777229309082 3.1895039081573486 33.461814880371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5506255626678467 3.6871371269226074 38.4219970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.548741340637207 3.469896078109741 36.24769973754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5446778535842896 3.6666338443756104 38.21101760864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5194087028503418 2.8812687397003174 30.332096099853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.589472770690918 3.2738146781921387 34.32762145996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6566437482833862 3.6764557361602783 38.421199798583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.558725118637085 3.851283073425293 40.071556091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6261930465698242 3.617823600769043 37.8044319152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5505244731903076 3.555372953414917 37.10425567626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6189945936203003 3.9092533588409424 40.71152877807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6077544689178467 3.438674211502075 35.9944953918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5591208934783936 2.4201595783233643 25.76071548461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.628692865371704 2.834038019180298 29.969072341918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5737569332122803 3.3595645427703857 35.169403076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6503068208694458 3.3309385776519775 34.959693908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.53660249710083 3.775135040283203 39.2879524230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5249454975128174 3.9154052734375 40.67899703979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.540272831916809 3.5020341873168945 36.560611724853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6550283432006836 3.0748441219329834 32.40346908569336
  batch 60 loss: 1.6550283432006836, 3.0748441219329834, 32.40346908569336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5210895538330078 2.7960007190704346 29.481096267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5681300163269043 3.554730176925659 37.11543273925781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5188761949539185 3.140305280685425 32.92192840576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5157692432403564 3.308725118637085 34.60301971435547
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4731955528259277 2.318389415740967 24.65709114074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.549164056777954 3.399319648742676 35.5423583984375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5719209909439087 2.8962721824645996 30.534643173217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5674164295196533 2.7950186729431152 29.51760482788086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5604997873306274 2.8968262672424316 30.52876091003418
Total LOSS train 35.069733546330376 valid 31.530841827392578
CE LOSS train 1.5664573449354906 valid 0.39012494683265686
Contrastive LOSS train 3.350327623807467 valid 0.7242065668106079
EPOCH 7:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5912731885910034 2.8549835681915283 30.1411075592041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6190320253372192 3.303971290588379 34.65874481201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.556926965713501 2.5747649669647217 27.304576873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5659528970718384 2.791544198989868 29.481393814086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6009279489517212 2.983870267868042 31.43963050842285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5400245189666748 3.1851258277893066 33.39128112792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.600598931312561 2.7950737476348877 29.55133628845215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5552880764007568 3.2391793727874756 33.947078704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5369362831115723 3.190192461013794 33.43886184692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5859153270721436 2.718153953552246 28.767454147338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5049306154251099 3.2617790699005127 34.12272262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5164777040481567 3.500209093093872 36.51856994628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.510258674621582 3.927128553390503 40.78154373168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5288647413253784 4.132107257843018 42.849937438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6217869520187378 3.688445806503296 38.50624465942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5914922952651978 3.441232442855835 36.003814697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4843922853469849 3.3396759033203125 34.88115310668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5301239490509033 2.949305295944214 31.023178100585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4851621389389038 3.7202658653259277 38.68782043457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5994547605514526 3.71700382232666 38.76948928833008
  batch 20 loss: 1.5994547605514526, 3.71700382232666, 38.76948928833008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5259078741073608 2.937756299972534 30.903470993041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4841020107269287 3.106356143951416 32.54766082763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.517844319343567 2.825788974761963 29.775733947753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5529780387878418 3.2768568992614746 34.32154846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6074142456054688 3.874903440475464 40.356449127197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5268771648406982 4.171564102172852 43.24251937866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5386788845062256 4.122101306915283 42.75968933105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5416796207427979 3.482851982116699 36.370201110839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4333113431930542 3.1783485412597656 33.216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6056153774261475 3.4761979579925537 36.367591857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4584455490112305 3.5676424503326416 37.13487243652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5744110345840454 3.834350109100342 39.917911529541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5386594533920288 3.7161335945129395 38.69999313354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5361257791519165 3.1458580493927 32.99470520019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4823049306869507 3.3582637310028076 35.06494140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5070281028747559 3.752192258834839 39.02894973754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5019354820251465 3.68005108833313 38.30244445800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6138877868652344 3.4386470317840576 36.00035858154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6141270399093628 2.879910469055176 30.413230895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.63504958152771 3.150923252105713 33.14427947998047
  batch 40 loss: 1.63504958152771, 3.150923252105713, 33.14427947998047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5710750818252563 3.184147596359253 33.41255187988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5418068170547485 3.3424065113067627 34.96586990356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5303149223327637 3.366272211074829 35.19303894042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5486868619918823 3.59158992767334 37.4645881652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5014123916625977 3.5125021934509277 36.626434326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5455389022827148 3.1769707202911377 33.31524658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6050331592559814 4.160248279571533 43.207515716552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.529312014579773 3.5719501972198486 37.24881362915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6454565525054932 3.5650205612182617 37.2956657409668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.533492922782898 2.7668964862823486 29.202457427978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6042722463607788 3.711510419845581 38.71937561035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5814321041107178 3.7444775104522705 39.026206970214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.537397861480713 3.1206088066101074 32.74348831176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5964958667755127 3.32572340965271 34.853729248046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5297293663024902 3.699105978012085 38.52078628540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6167596578598022 3.0527563095092773 32.14432144165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.526464819908142 2.798431634902954 29.510780334472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5231317281723022 3.603268623352051 37.555816650390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5493766069412231 3.912731885910034 40.67669677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6415212154388428 3.2282304763793945 33.923824310302734
  batch 60 loss: 1.6415212154388428, 3.2282304763793945, 33.923824310302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5262385606765747 3.0861406326293945 32.38764572143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5828531980514526 2.9791042804718018 31.3738956451416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.573050618171692 3.4535794258117676 36.10884475708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.551371693611145 3.0145530700683594 31.696903228759766
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.513370394706726 2.6060025691986084 27.573396682739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5670826435089111 3.396479845046997 35.53187942504883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5937387943267822 3.309748411178589 34.69122314453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.591335654258728 3.5445282459259033 37.036617279052734
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5849064588546753 3.244009256362915 34.025001525878906
Total LOSS train 35.070418196458085 valid 35.32118034362793
CE LOSS train 1.5527353763580323 valid 0.3962266147136688
Contrastive LOSS train 3.3517683102534366 valid 0.8110023140907288
EPOCH 8:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6049875020980835 3.1467437744140625 33.072425842285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6267850399017334 3.066898822784424 32.295772552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5811372995376587 3.2868146896362305 34.44928741455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5859806537628174 3.452151298522949 36.10749435424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6213123798370361 2.92897367477417 30.91105079650879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5610339641571045 3.116245985031128 32.72349548339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.613223910331726 3.049375295639038 32.10697555541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5711992979049683 3.214545249938965 33.716651916503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5673775672912598 3.429736375808716 35.864742279052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6156015396118164 3.758007287979126 39.195674896240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.551684021949768 3.2429494857788086 33.98118209838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5454851388931274 3.0924272537231445 32.469757080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5406442880630493 2.7181296348571777 28.721940994262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5550390481948853 3.263408899307251 34.18912887573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6382919549942017 3.604194402694702 37.680233001708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6191208362579346 3.6113524436950684 37.73264694213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5332375764846802 3.8759617805480957 40.29285430908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.575893521308899 3.447286367416382 36.04875564575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.533966064453125 3.2442057132720947 33.97602462768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6250276565551758 3.3823158740997314 35.448184967041016
  batch 20 loss: 1.6250276565551758, 3.3823158740997314, 35.448184967041016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.575545072555542 2.9356162548065186 30.93170738220215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5416860580444336 3.5103046894073486 36.64473342895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5726253986358643 3.394233465194702 35.514957427978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5909124612808228 3.0472311973571777 32.06322479248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6408727169036865 3.4903781414031982 36.544654846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5661170482635498 3.4512784481048584 36.07889938354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5974292755126953 3.9019570350646973 40.61699676513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5802992582321167 3.4436545372009277 36.016845703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5095833539962769 3.364621877670288 35.155799865722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6371504068374634 2.925224542617798 30.889394760131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5102537870407104 2.837977886199951 29.890033721923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6196547746658325 2.9189839363098145 30.809494018554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.591354489326477 2.446587562561035 26.05722999572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5957056283950806 3.8015782833099365 39.611488342285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5362074375152588 2.879314422607422 30.3293514251709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.558902621269226 3.543381929397583 36.99272155761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.567918062210083 3.360131025314331 35.169227600097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494982242584229 3.3213346004486084 34.86284255981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6533069610595703 2.971658945083618 31.369895935058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6658531427383423 3.45184326171875 36.18428421020508
  batch 40 loss: 1.6658531427383423, 3.45184326171875, 36.18428421020508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6177912950515747 3.073503017425537 32.352821350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5993413925170898 2.9451639652252197 31.050979614257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6039785146713257 3.1665773391723633 33.269752502441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6073654890060425 3.5255441665649414 36.8628044128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5920435190200806 3.4566597938537598 36.15864181518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6074388027191162 3.205016851425171 33.6576042175293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6525743007659912 2.4108338356018066 25.76091194152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5986504554748535 2.73323655128479 28.931015014648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6700830459594727 3.524535894393921 36.91543960571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5848678350448608 3.3677377700805664 35.262245178222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670011281967163 4.1600141525268555 43.2701530456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6326727867126465 3.562114715576172 37.25381851196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5852220058441162 3.412921905517578 35.714439392089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629572868347168 3.4339778423309326 35.96935272216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6039551496505737 3.4617934226989746 36.22188949584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6800508499145508 3.195439338684082 33.63444519042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6468231678009033 3.8776681423187256 40.42350387573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5998724699020386 3.1449966430664062 33.04983901977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6317447423934937 3.3622467517852783 35.25421142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7233976125717163 2.761817216873169 29.341569900512695
  batch 60 loss: 1.7233976125717163, 2.761817216873169, 29.341569900512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606352686882019 3.4011011123657227 35.61736297607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.653908371925354 3.8607802391052246 40.26171112060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.589154601097107 2.9507434368133545 31.096590042114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6054123640060425 3.1285018920898438 32.89043045043945
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5720505714416504 2.593947649002075 27.511526107788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182893514633179 3.528395414352417 36.902244567871094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6468865871429443 3.5054497718811035 36.70138168334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6457130908966064 3.6493399143218994 38.13911437988281
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6314582824707031 3.866581678390503 40.29727554321289
Total LOSS train 34.314632650522086 valid 38.0100040435791
CE LOSS train 1.5998806403233454 valid 0.4078645706176758
Contrastive LOSS train 3.271475230730497 valid 0.9666454195976257
EPOCH 9:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6565574407577515 3.5054919719696045 36.71147918701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6708430051803589 3.003046751022339 31.701311111450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6087558269500732 2.9367873668670654 30.97662925720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6363662481307983 3.2899324893951416 34.53569412231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643858790397644 3.4945907592773438 36.58976745605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5801782608032227 2.9671754837036133 31.251934051513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6443449258804321 2.918473958969116 30.829084396362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6178815364837646 3.3278138637542725 34.896018981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5933303833007812 3.214153528213501 33.734867095947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6219172477722168 3.3457045555114746 35.07896423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.558859944343567 3.4566056728363037 36.124916076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5562012195587158 2.9699981212615967 31.256183624267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5574589967727661 3.4425103664398193 35.98256301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5759820938110352 3.6786110401153564 38.362091064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6537448167800903 3.4395530223846436 36.04927444458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6403864622116089 3.363720417022705 35.277587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5513163805007935 2.777437925338745 29.32569694519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5890228748321533 3.007575273513794 31.664775848388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5493123531341553 3.2814438343048096 34.36375045776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6190211772918701 3.094585418701172 32.564876556396484
  batch 20 loss: 1.6190211772918701, 3.094585418701172, 32.564876556396484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5619919300079346 3.804037570953369 39.60236740112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5385067462921143 3.9315669536590576 40.85417556762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5693027973175049 3.502455711364746 36.5938606262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5926094055175781 3.7122108936309814 38.714717864990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6274077892303467 2.919602870941162 30.823436737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5699424743652344 2.9040606021881104 30.61054801940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5912790298461914 3.205751657485962 33.64879608154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.574770450592041 3.006118059158325 31.635950088500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5036063194274902 3.2437074184417725 33.940677642822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6129748821258545 2.961935520172119 31.232328414916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.497496247291565 3.6595914363861084 38.09341049194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6009595394134521 4.000169277191162 41.60265350341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5490143299102783 4.370921611785889 45.25823211669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.539050579071045 3.6559624671936035 38.09867477416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4967987537384033 4.225770950317383 43.75450897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5159249305725098 4.0245280265808105 41.761207580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5206838846206665 3.7206382751464844 38.72706604003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6070929765701294 3.362645149230957 35.233543395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6114870309829712 2.5579168796539307 27.190654754638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6237214803695679 3.3127360343933105 34.75108337402344
  batch 40 loss: 1.6237214803695679, 3.3127360343933105, 34.75108337402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5616687536239624 3.527545690536499 36.83712387084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5319181680679321 3.443570137023926 35.96761703491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5319058895111084 3.4669556617736816 36.20146179199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5449113845825195 3.4562137126922607 36.10704803466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5186307430267334 3.241305112838745 33.93168258666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5658414363861084 3.7766451835632324 39.33229446411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6170579195022583 3.34442400932312 35.06129837036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5436320304870605 3.3327505588531494 34.87113952636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6335667371749878 3.605471611022949 37.68828582763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.549565076828003 3.115037679672241 32.69994354248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5981931686401367 3.378162145614624 35.37981414794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5879055261611938 3.2646193504333496 34.234100341796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5528637170791626 3.028447151184082 31.83733558654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.603447437286377 2.9631526470184326 31.234973907470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5441821813583374 3.6405136585235596 37.949317932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6238946914672852 3.4916598796844482 36.54049301147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5567681789398193 3.398055076599121 35.53731918334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5448484420776367 3.4560630321502686 36.1054801940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.559012770652771 3.6634597778320312 38.19361114501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.655303716659546 2.928452253341675 30.93982696533203
  batch 60 loss: 1.655303716659546, 2.928452253341675, 30.93982696533203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5388975143432617 3.155435800552368 33.09325408935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5858064889907837 3.3164052963256836 34.74986267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5600003004074097 3.314857244491577 34.70857238769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5423921346664429 2.790860176086426 29.45099449157715
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5253740549087524 3.0103981494903564 31.62935447692871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.56195867061615 3.5738306045532227 37.300262451171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.582655429840088 3.0761070251464844 32.343727111816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5774898529052734 3.1213936805725098 32.79142761230469
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5797972679138184 2.5306029319763184 26.885828018188477
Total LOSS train 35.07211638230544 valid 32.33031129837036
CE LOSS train 1.5785776926920965 valid 0.3949493169784546
Contrastive LOSS train 3.3493538489708534 valid 0.6326507329940796
EPOCH 10:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.60088050365448 2.80293607711792 29.63024139404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6129751205444336 3.878467559814453 40.39765167236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5734763145446777 3.2244226932525635 33.81770324707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5890495777130127 2.7331764698028564 28.920814514160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6152890920639038 2.872703790664673 30.342327117919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5508599281311035 2.5526010990142822 27.07686996459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6062978506088257 3.304600477218628 34.65230178833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.575792908668518 2.7186927795410156 28.762720108032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5719265937805176 3.56614089012146 37.233333587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.616569995880127 3.523864984512329 36.855220794677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5671615600585938 3.6457440853118896 38.024600982666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5761170387268066 3.38533353805542 35.4294548034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5777535438537598 3.0345299243927 31.923051834106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5813111066818237 3.528428792953491 36.865596771240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652909755706787 3.5190439224243164 36.843345642089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6410685777664185 3.2819812297821045 34.460880279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5621412992477417 3.1479735374450684 33.04187774658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6041340827941895 3.3981616497039795 35.585750579833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5588983297348022 3.9938161373138428 41.4970588684082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.625316858291626 4.229737758636475 43.92269515991211
  batch 20 loss: 1.625316858291626, 4.229737758636475, 43.92269515991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.588308334350586 4.1105122566223145 42.69342803955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5537368059158325 3.8811216354370117 40.36495590209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5737156867980957 3.409846782684326 35.672183990478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5997692346572876 2.7424302101135254 29.024070739746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6446576118469238 2.9169697761535645 30.814353942871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5935404300689697 3.7043344974517822 38.63688659667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6195423603057861 3.9279370307922363 40.89891052246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6067386865615845 3.29669451713562 34.57368469238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5332419872283936 2.7596306800842285 29.129547119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6470636129379272 3.0316405296325684 31.963470458984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5428653955459595 3.464984655380249 36.192710876464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6387935876846313 3.5954830646514893 37.593624114990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5944243669509888 3.2236742973327637 33.83116912841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5881786346435547 3.398948907852173 35.577667236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5561801195144653 3.1618759632110596 33.1749382019043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5750035047531128 3.4694173336029053 36.2691764831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5734549760818481 3.940744161605835 40.98089599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.647045373916626 3.3901796340942383 35.5488395690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6482768058776855 3.5559890270233154 37.208168029785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6586915254592896 3.368443250656128 35.34312438964844
  batch 40 loss: 1.6586915254592896, 3.368443250656128, 35.34312438964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.609376072883606 3.0957632064819336 32.56700897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5830328464508057 3.392669200897217 35.509727478027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.581036925315857 3.073728322982788 32.318321228027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5954172611236572 3.35147762298584 35.11019515991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5722646713256836 3.535815477371216 36.930419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6095110177993774 3.4916062355041504 36.52557373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644546389579773 3.0071191787719727 31.71573829650879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5861921310424805 3.559180498123169 37.17799758911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6579405069351196 3.371954917907715 35.37749099731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5793086290359497 3.8950867652893066 40.530174255371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6205055713653564 3.1394379138946533 33.01488494873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6115190982818604 2.823396921157837 29.845487594604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5827654600143433 3.207547187805176 33.658233642578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6234108209609985 3.5833077430725098 37.456485748291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5736346244812012 3.029266119003296 31.866296768188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6460628509521484 2.7545783519744873 29.19184684753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5811405181884766 3.6387736797332764 37.96887969970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5633524656295776 3.4544405937194824 36.107757568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5795596837997437 3.012004852294922 31.699607849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.667943000793457 3.1832776069641113 33.50071716308594
  batch 60 loss: 1.667943000793457, 3.1832776069641113, 33.50071716308594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5525480508804321 3.6044180393218994 37.59672927856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5904704332351685 3.05210018157959 32.111473083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5734829902648926 3.50276255607605 36.60110855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.554630994796753 3.3006324768066406 34.56095504760742
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.534193754196167 2.819887161254883 29.733064651489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5708057880401611 2.857164144515991 30.14244842529297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.592524528503418 3.0942065715789795 32.53459167480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5843418836593628 2.80430269241333 29.62736701965332
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5953319072723389 2.5454187393188477 27.049518585205078
Total LOSS train 34.914638137817384 valid 29.838481426239014
CE LOSS train 1.5956462438289936 valid 0.3988329768180847
Contrastive LOSS train 3.331899206454937 valid 0.6363546848297119
Saved best model. Old loss 31.37149953842163 and new best loss 29.838481426239014
EPOCH 11:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.610903024673462 2.673227548599243 28.343177795410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62899911403656 3.4259324073791504 35.88832092285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5914568901062012 2.9623100757598877 31.214557647705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6028350591659546 2.835326671600342 29.95610237121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6321032047271729 3.079411029815674 32.426212310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5777329206466675 3.319434404373169 34.77207565307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6274662017822266 2.8238699436187744 29.866165161132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5932762622833252 2.491724967956543 26.510526657104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5893887281417847 2.812303066253662 29.712419509887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624162197113037 3.641584873199463 38.040008544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5752201080322266 2.9847400188446045 31.42262077331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5763877630233765 2.574054718017578 27.31693458557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.576456904411316 2.9384536743164062 30.96099281311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5853416919708252 3.3016576766967773 34.60192108154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6552066802978516 3.3889622688293457 35.544830322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6518454551696777 3.3392066955566406 35.04391098022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.579524278640747 2.9905242919921875 31.48476791381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6093533039093018 3.100274085998535 32.61209487915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5731327533721924 2.881845235824585 30.391586303710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6382147073745728 3.169530153274536 33.33351516723633
  batch 20 loss: 1.6382147073745728, 3.169530153274536, 33.33351516723633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.594663381576538 3.466378927230835 36.25844955444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5674418210983276 2.7971794605255127 29.539236068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5938085317611694 3.221599578857422 33.8098030090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6194281578063965 2.5508952140808105 27.128381729125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.656079888343811 3.02066969871521 31.862777709960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6009612083435059 3.2375991344451904 33.976951599121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6181477308273315 3.394200086593628 35.560150146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6090202331542969 2.3707494735717773 25.31651496887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.549638271331787 3.9735472202301025 41.28511047363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6460683345794678 3.003241539001465 31.678483963012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5598509311676025 3.2230124473571777 33.78997802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6344494819641113 4.0196404457092285 41.83085250854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6059634685516357 2.974123239517212 31.34719467163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6009821891784668 3.705362558364868 38.65460968017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5638012886047363 3.4658737182617188 36.222537994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5788935422897339 3.5280964374542236 36.85985565185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.576767086982727 2.9913170337677 31.48993682861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6489561796188354 3.725255250930786 38.90150833129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543469429016113 3.6938252449035645 38.59259796142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6647388935089111 3.2313623428344727 33.97835922241211
  batch 40 loss: 1.6647388935089111, 3.2313623428344727, 33.97835922241211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6220977306365967 3.0129740238189697 31.7518367767334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.601678729057312 3.136082172393799 32.96249771118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5963940620422363 2.9091873168945312 30.68826675415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6086515188217163 3.5939154624938965 37.54780578613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5879148244857788 2.5402209758758545 26.99012565612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6223208904266357 3.4997477531433105 36.61980056762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6554231643676758 2.721731662750244 28.872737884521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6064971685409546 2.974221706390381 31.34871482849121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6779896020889282 3.883969306945801 40.51768112182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6141347885131836 2.9627838134765625 31.241973876953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6541026830673218 3.613473653793335 37.78883743286133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432850360870361 3.3161232471466064 34.80451583862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.617158055305481 3.2258732318878174 33.87588882446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6509389877319336 3.399653673171997 35.64747619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182923316955566 2.8241631984710693 29.85992431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776360273361206 3.2620668411254883 34.298301696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6268672943115234 2.9722416400909424 31.34928321838379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6164480447769165 3.3931212425231934 35.54766082763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6302410364151 3.1311557292938232 32.94179916381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6960656642913818 2.7161176204681396 28.857242584228516
  batch 60 loss: 1.6960656642913818, 2.7161176204681396, 28.857242584228516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182525157928467 3.491147041320801 36.529720306396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6454368829727173 3.123802900314331 32.88346481323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6245090961456299 3.0215871334075928 31.840381622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6149243116378784 2.360128879547119 25.216211318969727
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5945334434509277 2.843261957168579 30.02715301513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6211745738983154 3.2559049129486084 34.18022155761719
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6370291709899902 2.952913761138916 31.166166305541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6323652267456055 2.5442755222320557 27.075119018554688
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6356151103973389 2.777076244354248 29.4063777923584
Total LOSS train 33.03900513282189 valid 30.456971168518066
CE LOSS train 1.6148432108072135 valid 0.4089037775993347
Contrastive LOSS train 3.1424162314488338 valid 0.694269061088562
EPOCH 12:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6502388715744019 2.6589457988739014 28.239696502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661991834640503 2.8908846378326416 30.570837020874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.634440541267395 2.300035238265991 24.634794235229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6433360576629639 3.1035306453704834 32.67864227294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670680046081543 2.812084913253784 29.79153060913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6303510665893555 3.1697118282318115 33.32746887207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669273018836975 3.0968282222747803 32.63755416870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6477810144424438 2.609574317932129 27.7435245513916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641656756401062 3.313214063644409 34.77379608154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693135499954224 2.9720957279205322 31.390270233154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633711338043213 3.371344804763794 35.34716033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6256471872329712 3.3115525245666504 34.741172790527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6268212795257568 3.019757032394409 31.824392318725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6321953535079956 2.8427724838256836 30.059919357299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6821188926696777 3.7032437324523926 38.71455383300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684053659439087 3.4350483417510986 36.03453826904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6287848949432373 3.253971815109253 34.16850280761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6562081575393677 3.3701353073120117 35.35756301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6289715766906738 3.534144878387451 36.970420837402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6806979179382324 3.4606525897979736 36.28722381591797
  batch 20 loss: 1.6806979179382324, 3.4606525897979736, 36.28722381591797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6518610715866089 2.9822065830230713 31.473926544189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6334445476531982 3.080280303955078 32.436248779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6500226259231567 2.8652467727661133 30.302490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6656442880630493 3.1656334400177 33.32197952270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6880037784576416 2.6342196464538574 28.030200958251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6490460634231567 3.239124059677124 34.040287017822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6619749069213867 3.2272045612335205 33.93402099609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6553223133087158 3.0151712894439697 31.807035446166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6126493215560913 3.0969645977020264 32.58229446411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682509422302246 3.6835036277770996 38.517547607421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6106557846069336 4.040000915527344 42.01066589355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6660345792770386 3.88675856590271 40.53361892700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6401265859603882 3.23278546333313 33.967979431152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6412866115570068 3.2118403911590576 33.75968933105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6119592189788818 3.243513822555542 34.047096252441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6296919584274292 3.649852752685547 38.12821960449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6330811977386475 3.3254570960998535 34.88764953613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6847091913223267 3.7627367973327637 39.312076568603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688699722290039 3.035006046295166 32.03875732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7009307146072388 3.3198401927948 34.89933395385742
  batch 40 loss: 1.7009307146072388, 3.3198401927948, 34.89933395385742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6669114828109741 3.1250650882720947 32.91756057739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6560930013656616 3.061607837677002 32.27217102050781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6616052389144897 2.7257702350616455 28.919307708740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6696698665618896 3.202019453048706 33.68986511230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6492401361465454 3.04084849357605 32.05772399902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.674025297164917 3.5027108192443848 36.701133728027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.697934865951538 3.085515260696411 32.55308532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6624281406402588 3.4728286266326904 36.390716552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.713383674621582 3.144594430923462 33.15932846069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6653668880462646 3.707348108291626 38.73884963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6906818151474 2.9802932739257812 31.493614196777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6831128597259521 2.869133472442627 30.374446868896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6649858951568604 3.087933301925659 32.54431915283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6905218362808228 3.779012441635132 39.48064422607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6569786071777344 3.0391080379486084 32.048057556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993781328201294 3.7234623432159424 38.93400192260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664819598197937 3.739527463912964 39.06009292602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6503190994262695 3.4554097652435303 36.20441436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6623181104660034 3.394789457321167 35.61021423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7193446159362793 3.023167848587036 31.95102310180664
  batch 60 loss: 1.7193446159362793, 3.023167848587036, 31.95102310180664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6526033878326416 3.1088054180145264 32.740657806396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6790019273757935 2.6717581748962402 28.39658546447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629807949066162 3.265308380126953 34.316062927246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6549615859985352 3.0807831287384033 32.462791442871094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6434541940689087 2.754683494567871 29.190288543701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6640037298202515 2.756873607635498 29.23274040222168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.674426555633545 3.035937786102295 32.03380584716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6724681854248047 2.856727123260498 30.2397403717041
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6732630729675293 2.8427376747131348 30.10063934326172
Total LOSS train 33.65433287987342 valid 30.401731491088867
CE LOSS train 1.6592007380265457 valid 0.4183157682418823
Contrastive LOSS train 3.199513233624972 valid 0.7106844186782837
EPOCH 13:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68473219871521 2.851963520050049 30.20436668395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6937705278396606 3.321822166442871 34.91199493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6753416061401367 3.331806182861328 34.993404388427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6799142360687256 3.367825984954834 35.35817337036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691811203956604 3.2539162635803223 34.23097229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.657037377357483 2.552804470062256 27.185083389282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6900906562805176 2.8802428245544434 30.49251937866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6659053564071655 2.696882486343384 28.634729385375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629383563995361 3.0045061111450195 31.70800018310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688137412071228 3.146819591522217 33.156333923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6619631052017212 3.3752970695495605 35.41493606567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6637948751449585 3.2814207077026367 34.478004455566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6656101942062378 3.349379062652588 35.159400939941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6686981916427612 3.334202527999878 35.01072311401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7121553421020508 3.452098846435547 36.2331428527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7078791856765747 3.2670469284057617 34.37834930419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6628888845443726 3.6898183822631836 38.561073303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6850361824035645 3.2851641178131104 34.536678314208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6477681398391724 3.0751986503601074 32.399757385253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6965168714523315 2.7485458850860596 29.181974411010742
  batch 20 loss: 1.6965168714523315, 2.7485458850860596, 29.181974411010742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6750656366348267 2.861057758331299 30.285642623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6534669399261475 2.7363131046295166 29.016597747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6761958599090576 3.3107221126556396 34.783416748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6936531066894531 3.2201173305511475 33.89482498168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7197883129119873 3.5927734375 37.64752197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6816456317901611 3.0454416275024414 32.13606262207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6951853036880493 3.5548787117004395 37.24397277832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684626817703247 2.8459441661834717 30.14406967163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6538872718811035 3.634835720062256 38.00224685668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7099424600601196 3.564213991165161 37.352081298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652358055114746 2.8918297290802 30.570655822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7033888101577759 3.291980266571045 34.623191833496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6950492858886719 3.2085623741149902 33.78067398071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.699657678604126 3.0683817863464355 32.38347625732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6752241849899292 3.2810449600219727 34.48567199707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6868798732757568 3.180629014968872 33.493167877197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6942445039749146 3.5372531414031982 37.066776275634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.731697678565979 2.682760000228882 28.559297561645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7340384721755981 2.9519670009613037 31.25370979309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7399307489395142 3.0390663146972656 32.130592346191406
  batch 40 loss: 1.7399307489395142, 3.0390663146972656, 32.130592346191406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7167391777038574 2.8714563846588135 30.431303024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7029218673706055 3.4399147033691406 36.10206985473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7017096281051636 3.2206106185913086 33.90781784057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.706526517868042 3.1428823471069336 33.13534927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6977914571762085 3.164797306060791 33.34576416015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7077394723892212 3.4185893535614014 35.89363479614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7294729948043823 3.1889660358428955 33.61913299560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6966317892074585 3.267077684402466 34.367408752441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.727964162826538 3.3505702018737793 35.233665466308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6971100568771362 3.2460436820983887 34.15754699707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7215510606765747 3.650132894515991 38.222877502441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7139171361923218 3.4970974922180176 36.68489074707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6898460388183594 3.4509363174438477 36.1992073059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7003908157348633 3.5155575275421143 36.85596466064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6865408420562744 3.333543062210083 35.02197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7191150188446045 3.457113742828369 36.290252685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6914176940917969 3.6210217475891113 37.901634216308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6831003427505493 3.627525568008423 37.95835876464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6824463605880737 3.520268678665161 36.8851318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7304445505142212 3.132117509841919 33.05162048339844
  batch 60 loss: 1.7304445505142212, 3.132117509841919, 33.05162048339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682377815246582 3.434605121612549 36.02842712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7064145803451538 2.742976188659668 29.13617706298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6924617290496826 3.1518478393554688 33.210941314697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6882169246673584 3.312088966369629 34.809104919433594
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6867129802703857 3.098755121231079 32.67426300048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6964384317398071 3.141801118850708 33.11444854736328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7087253332138062 3.037322521209717 32.08195114135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7048934698104858 3.192084789276123 33.6257438659668
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.703946590423584 2.8214471340179443 29.918418884277344
Total LOSS train 33.94165825477013 valid 32.18514060974121
CE LOSS train 1.6919611930847167 valid 0.425986647605896
Contrastive LOSS train 3.2249696988325853 valid 0.7053617835044861
EPOCH 14:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7132881879806519 3.0501301288604736 32.2145881652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.721989631652832 2.66081166267395 28.33010482788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7072874307632446 3.0768508911132812 32.47579574584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7047539949417114 3.252408742904663 34.22883987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7147445678710938 3.9108593463897705 40.82333755493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6922175884246826 3.585880756378174 37.551025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7167646884918213 3.15783953666687 33.295162200927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6918184757232666 2.5909171104431152 27.600990295410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6899189949035645 3.436038017272949 36.05030059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7162498235702515 3.5070509910583496 36.78676223754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6887742280960083 3.6833090782165527 38.52186584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6876497268676758 3.682307481765747 38.51072311401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687418818473816 3.988600492477417 41.57342529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6991764307022095 3.4970602989196777 36.66978073120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7326762676239014 3.8381524085998535 40.114200592041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7257976531982422 3.354259490966797 35.268394470214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692613124847412 3.203519344329834 33.727806091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.717551350593567 3.4713475704193115 36.431026458740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7033015489578247 3.9093716144561768 40.79701614379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7418272495269775 3.8475708961486816 40.21753692626953
  batch 20 loss: 1.7418272495269775, 3.8475708961486816, 40.21753692626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.730224847793579 3.3565545082092285 35.29576873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7105342149734497 3.0448195934295654 32.158729553222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7283861637115479 2.8384971618652344 30.113357543945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7479616403579712 2.955925703048706 31.307218551635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7771366834640503 3.8669378757476807 40.44651794433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7466524839401245 3.0389387607574463 32.13603973388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.746025562286377 3.2013731002807617 33.75975799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7576863765716553 2.9342544078826904 31.100231170654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7372322082519531 3.153649091720581 33.27372360229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7751009464263916 3.565596342086792 37.43106460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7360432147979736 3.0805890560150146 32.541934967041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7657058238983154 2.689192771911621 28.65763282775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7658644914627075 3.009772777557373 31.86359405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7711561918258667 3.269294261932373 34.4640998840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7652148008346558 3.4544389247894287 36.309600830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772359848022461 2.900667905807495 30.77903938293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7661818265914917 3.2894814014434814 34.66099548339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7754647731781006 3.2834999561309814 34.6104621887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7820279598236084 3.4937100410461426 36.7191276550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.785825252532959 3.95590877532959 41.344913482666016
  batch 40 loss: 1.785825252532959, 3.95590877532959, 41.344913482666016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7619997262954712 3.084264039993286 32.60464096069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.758866548538208 3.7141904830932617 38.90077209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.764116883277893 3.6411046981811523 38.175167083740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7516505718231201 3.199967622756958 33.75132751464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7377958297729492 3.428429365158081 36.022090911865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7390059232711792 3.294322967529297 34.68223571777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.747212290763855 3.5091161727905273 36.838375091552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7252243757247925 3.7922303676605225 39.647525787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7447487115859985 3.4043047428131104 35.78779602050781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7230618000030518 2.8153464794158936 29.876527786254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7486140727996826 3.26527738571167 34.401390075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7346067428588867 3.682910919189453 38.563716888427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7171952724456787 3.3235526084899902 34.952720642089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7315257787704468 3.0523762702941895 32.255287170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7138712406158447 3.6019015312194824 37.732887268066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7364354133605957 3.972362518310547 41.460060119628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7072217464447021 3.0847010612487793 32.55423355102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696589708328247 3.0079562664031982 31.776153564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7017508745193481 3.7549326419830322 39.25107955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7352299690246582 2.9143266677856445 30.878496170043945
  batch 60 loss: 1.7352299690246582, 2.9143266677856445, 30.878496170043945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682805061340332 2.882380723953247 30.506610870361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6999598741531372 3.6056928634643555 37.75688934326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6905375719070435 3.601715087890625 37.70768737792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6805909872055054 3.1330323219299316 33.01091384887695
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.670332670211792 2.7440786361694336 29.11111831665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687514066696167 2.56099009513855 27.297414779663086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6978001594543457 2.7239272594451904 28.93707275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694917917251587 2.522932291030884 26.924240112304688
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6941306591033936 2.807945489883423 29.773584365844727
Total LOSS train 35.05181840749887 valid 28.233078002929688
CE LOSS train 1.7290700729076678 valid 0.4235326647758484
Contrastive LOSS train 3.332274811084454 valid 0.7019863724708557
Saved best model. Old loss 29.838481426239014 and new best loss 28.233078002929688
EPOCH 15:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7042315006256104 2.8909401893615723 30.613632202148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710899829864502 3.4272236824035645 35.98313522338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6950674057006836 3.057302713394165 32.26809310913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6958019733428955 2.9152615070343018 30.848417282104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055811882019043 2.7459733486175537 29.165315628051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.677176833152771 3.0130975246429443 31.80815315246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7046637535095215 3.6128909587860107 37.83357238769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6931473016738892 3.5367493629455566 37.060638427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6851134300231934 2.494783878326416 26.632951736450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7008585929870605 2.503166675567627 26.732524871826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671185851097107 3.051464796066284 32.18583297729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6721054315567017 3.628967523574829 37.9617805480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6643072366714478 3.1276206970214844 32.940513610839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6682038307189941 3.4056966304779053 35.72517013549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7039741277694702 3.078172445297241 32.48569869995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7066181898117065 3.19254469871521 33.63206481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6662569046020508 3.1340556144714355 33.006813049316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6882262229919434 2.8932690620422363 30.62091636657715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6679065227508545 2.8650858402252197 30.318763732910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6925891637802124 3.3822083473205566 35.514671325683594
  batch 20 loss: 1.6925891637802124, 3.3822083473205566, 35.514671325683594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6622029542922974 3.056584119796753 32.22804641723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6538465023040771 3.0968010425567627 32.621856689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669276237487793 3.0109245777130127 31.778522491455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6771790981292725 3.5917937755584717 37.595115661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6900627613067627 3.4519219398498535 36.20928192138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6651304960250854 3.6524221897125244 38.189353942871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6833690404891968 3.673633575439453 38.41970443725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6648648977279663 3.9940505027770996 41.605369567871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6278560161590576 3.7805631160736084 39.43348693847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6912765502929688 3.8593931198120117 40.28520965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6337454319000244 3.168769121170044 33.32143783569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6879833936691284 3.1674585342407227 33.36256790161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672253131866455 3.0381593704223633 32.05384826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6640374660491943 3.4418113231658936 36.082149505615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6319997310638428 3.2962803840637207 34.59480285644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6470048427581787 2.9739489555358887 31.38649559020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6382936239242554 3.6560187339782715 38.198482513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6856671571731567 3.4006776809692383 35.69244384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6885364055633545 3.3063440322875977 34.751976013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702985405921936 3.5086679458618164 36.7896614074707
  batch 40 loss: 1.702985405921936, 3.5086679458618164, 36.7896614074707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6680498123168945 3.5333354473114014 37.00140380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6523873805999756 3.5929906368255615 37.58229064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6597309112548828 3.0277910232543945 31.937641143798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6655769348144531 3.2611260414123535 34.27683639526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494165658950806 2.854968547821045 30.1991024017334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669371485710144 3.477917432785034 36.44854736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6890796422958374 4.006651401519775 41.755592346191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6559253931045532 4.048387050628662 42.13979721069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7015448808670044 3.861114263534546 40.312686920166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6519209146499634 3.133148431777954 32.98340606689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6836203336715698 3.0040619373321533 31.724239349365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6729542016983032 3.6003243923187256 37.67619705200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652693510055542 3.2842202186584473 34.494895935058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6738903522491455 3.2737486362457275 34.411376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.64312744140625 3.2923190593719482 34.56631851196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6875672340393066 2.6388444900512695 28.076011657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6476659774780273 3.0027852058410645 31.675518035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629884123802185 3.5317416191101074 36.94729995727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6436047554016113 3.0712387561798096 32.35599136352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7001290321350098 3.661609411239624 38.31622314453125
  batch 60 loss: 1.7001290321350098, 3.661609411239624, 38.31622314453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6307411193847656 3.2932732105255127 34.563472747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6539638042449951 3.1056110858917236 32.71007537841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6377981901168823 3.5312600135803223 36.95039749145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6210403442382812 3.1012399196624756 32.63343811035156
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6096667051315308 2.8574724197387695 30.184391021728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6334228515625 2.763425588607788 29.26767921447754
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646437168121338 2.853318929672241 30.17962646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6425107717514038 2.773268461227417 29.37519645690918
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6512959003448486 2.715772867202759 28.809024810791016
Total LOSS train 34.45940959636982 valid 29.40788173675537
CE LOSS train 1.6701974997153648 valid 0.41282397508621216
Contrastive LOSS train 3.278921233690702 valid 0.6789432168006897
EPOCH 16:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661914348602295 2.8457727432250977 30.11964225769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6783348321914673 3.319899082183838 34.877323150634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6512446403503418 2.949413299560547 31.14537811279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6536970138549805 3.2276124954223633 33.9298210144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671226143836975 3.35672664642334 35.238494873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6350700855255127 3.173388957977295 33.368961334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6745235919952393 4.092648983001709 42.60101318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494026184082031 3.3886361122131348 35.535762786865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6483478546142578 2.4974305629730225 26.62265396118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6760964393615723 2.9592671394348145 31.268766403198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6381598711013794 3.22274112701416 33.865570068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6408815383911133 3.0128417015075684 31.769298553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6394412517547607 3.289665937423706 34.536102294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6373906135559082 3.783949851989746 39.476890563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686335563659668 3.463758707046509 36.32392501831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6840459108352661 4.228880882263184 43.97285842895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6377983093261719 3.4085888862609863 35.72368621826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6628762483596802 2.831282377243042 29.97570037841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6424782276153564 3.437053680419922 36.01301574707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6949225664138794 3.7705976963043213 39.400901794433594
  batch 20 loss: 1.6949225664138794, 3.7705976963043213, 39.400901794433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6659049987792969 3.1610798835754395 33.276702880859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6488045454025269 3.1016149520874023 32.664955139160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6643308401107788 3.206922769546509 33.733558654785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6815097332000732 3.2713100910186768 34.39461135864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7154412269592285 3.43634295463562 36.07887268066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669252872467041 3.2244770526885986 33.914024353027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6912556886672974 3.517608880996704 36.867347717285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68490731716156 3.691810369491577 38.60300827026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6454670429229736 3.448719024658203 36.13265609741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7120459079742432 2.7399280071258545 29.111326217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6478989124298096 3.2071688175201416 33.71958923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7008543014526367 3.6998298168182373 38.699153900146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6845543384552002 3.013322353363037 31.817779541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6867600679397583 3.362607717514038 35.312835693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663997769355774 3.7494006156921387 39.15800476074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807750463485718 3.720797538757324 38.88875198364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676190972328186 2.9285690784454346 30.961881637573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.719099760055542 2.7323060035705566 29.042158126831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7160662412643433 3.233670473098755 34.05276870727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7274249792099 3.2040905952453613 33.76832962036133
  batch 40 loss: 1.7274249792099, 3.2040905952453613, 33.76832962036133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6997493505477905 3.374695062637329 35.44670104980469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6774824857711792 3.4014134407043457 35.69161605834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6797664165496826 3.466261148452759 36.34238052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6751525402069092 3.079437494277954 32.46952819824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673266053199768 2.923851251602173 30.91177749633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6870334148406982 3.5508596897125244 37.19563293457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7099390029907227 3.2070884704589844 33.78082275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6793495416641235 3.720838785171509 38.88773727416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7134175300598145 3.4933295249938965 36.64671325683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671245813369751 3.161543846130371 33.286685943603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.713292121887207 3.4024243354797363 35.73753356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6947376728057861 3.1741206645965576 33.435943603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6765623092651367 2.6628975868225098 28.305538177490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6915276050567627 3.1550683975219727 33.242210388183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665266990661621 3.5336191654205322 37.001461029052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.701430082321167 2.875559091567993 30.457019805908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665013074874878 3.369494676589966 35.35995864868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6486186981201172 3.5322232246398926 36.970848083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.656402587890625 3.269089698791504 34.34729766845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055792808532715 3.4462075233459473 36.16765213012695
  batch 60 loss: 1.7055792808532715, 3.4462075233459473, 36.16765213012695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6397281885147095 2.8602511882781982 30.24224090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6679154634475708 2.830235242843628 29.97026824951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6534044742584229 2.5561764240264893 27.215167999267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6487040519714355 2.844491481781006 30.09362030029297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6505143642425537 3.462106943130493 36.271583557128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6724916696548462 3.5109188556671143 36.781681060791016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6838804483413696 3.468458414077759 36.368465423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679609775543213 3.2515439987182617 34.19505310058594
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6839652061462402 3.3424835205078125 35.10879898071289
Total LOSS train 34.32987726651705 valid 35.61349964141846
CE LOSS train 1.6737204515016997 valid 0.42099130153656006
Contrastive LOSS train 3.2656156649956336 valid 0.8356208801269531
EPOCH 17:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6958290338516235 3.554541826248169 37.24124526977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7030593156814575 3.1365339756011963 33.068397521972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776443719863892 2.6851048469543457 28.5286922454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6825743913650513 2.9520697593688965 31.20327377319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7005223035812378 3.2344117164611816 34.044639587402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6681723594665527 3.177281618118286 33.44098663330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695589542388916 3.378330707550049 35.47889709472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6732934713363647 2.6005876064300537 27.679170608520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698795557022095 3.0451035499572754 32.120914459228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6884838342666626 2.7423388957977295 29.111873626708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6610380411148071 3.336925506591797 35.03029251098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6555973291397095 3.240398406982422 34.0595817565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6412519216537476 3.4992105960845947 36.633358001708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6441847085952759 2.964024782180786 31.28443145751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691739559173584 3.4865989685058594 36.5577278137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6964600086212158 4.3345184326171875 45.04164505004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6416821479797363 3.1579201221466064 33.220882415771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6702044010162354 2.9824888706207275 31.495092391967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6464601755142212 3.109772205352783 32.74418258666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6816596984863281 3.0715768337249756 32.397430419921875
  batch 20 loss: 1.6816596984863281, 3.0715768337249756, 32.397430419921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6571221351623535 3.1543548107147217 33.2006721496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6369999647140503 3.5547447204589844 37.1844482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6555637121200562 3.3433375358581543 35.08893966674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6673955917358398 3.2154836654663086 33.822235107421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6956037282943726 2.9145302772521973 30.840906143188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6550490856170654 3.6942179203033447 38.59722900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6683690547943115 3.6069397926330566 37.73776626586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6648670434951782 3.7231862545013428 38.896728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62628173828125 3.3333511352539062 34.95979309082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696524977684021 3.4291539192199707 35.98806381225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6266168355941772 3.2003135681152344 33.62975311279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6845253705978394 4.008092403411865 41.76544952392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.662401556968689 3.244274139404297 34.10514450073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6655693054199219 3.1237380504608154 32.902950286865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641963005065918 2.952608346939087 31.168045043945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6541922092437744 3.1649742126464844 33.30393600463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.65545654296875 3.339226007461548 35.0477180480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7043852806091309 2.9609954357147217 31.314340591430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7051690816879272 2.9878287315368652 31.583457946777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.713922381401062 3.5534422397613525 37.24834442138672
  batch 40 loss: 1.713922381401062, 3.5534422397613525, 37.24834442138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807557344436646 3.208988666534424 33.7706413269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6728794574737549 3.0138401985168457 31.811281204223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6716715097427368 3.3969674110412598 35.6413459777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6756261587142944 3.5303266048431396 36.97888946533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6550142765045166 2.65449595451355 28.199974060058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6791833639144897 3.2527382373809814 34.206565856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7060106992721558 3.494860887527466 36.65461730957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6674081087112427 3.0475997924804688 32.14340591430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7095119953155518 3.8148694038391113 39.85820388793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.666760802268982 3.186462640762329 33.53138732910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6933842897415161 3.32637882232666 34.95717239379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6858160495758057 3.476491689682007 36.45073318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639094352722168 3.241598606109619 34.07989501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6949399709701538 3.3104870319366455 34.799808502197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6752129793167114 3.1571991443634033 33.2472038269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7135848999023438 2.7319581508636475 29.033166885375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6729412078857422 3.2427241802215576 34.100181579589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659128189086914 3.5232276916503906 36.89140319824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6747015714645386 3.612515926361084 37.799861907958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7280086278915405 3.387193441390991 35.59994125366211
  batch 60 loss: 1.7280086278915405, 3.387193441390991, 35.59994125366211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6604121923446655 3.132946252822876 32.98987579345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6858127117156982 3.623363494873047 37.91944885253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6628947257995605 3.419887065887451 35.86176681518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6496156454086304 3.337313652038574 35.02275466918945
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.640049695968628 3.1145339012145996 32.7853889465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6652657985687256 3.3299827575683594 34.965091705322266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6802680492401123 3.0809218883514404 32.48948669433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6779053211212158 3.3059282302856445 34.737186431884766
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.670035481452942 3.3251852989196777 34.92189025878906
Total LOSS train 34.35543925945576 valid 34.27841377258301
CE LOSS train 1.6733621395551241 valid 0.4175088703632355
Contrastive LOSS train 3.2682077114398664 valid 0.8312963247299194
EPOCH 18:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6884989738464355 3.223085880279541 33.91935729980469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6932672262191772 3.758575677871704 39.27902603149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6724919080734253 2.753434896469116 29.20684051513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6817635297775269 3.205191135406494 33.733673095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6936604976654053 3.15501070022583 33.24376678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6655441522598267 3.2324841022491455 33.99038314819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6986314058303833 2.9298298358917236 30.996929168701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6742099523544312 2.8797008991241455 30.47121810913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6572089195251465 2.8455655574798584 30.112865447998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.685319185256958 2.8226916790008545 29.9122371673584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645459771156311 2.9162821769714355 30.80828285217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6484683752059937 2.9653494358062744 31.30196189880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6447969675064087 3.5444369316101074 37.089168548583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6469953060150146 3.2644102573394775 34.291099548339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981838941574097 3.31693172454834 34.86750411987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6983752250671387 2.815030813217163 29.848684310913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6510368585586548 2.915231943130493 30.803356170654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672881841659546 3.0328519344329834 32.001399993896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6530308723449707 3.308713436126709 34.74016571044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6924388408660889 3.1386032104492188 33.07847213745117
  batch 20 loss: 1.6924388408660889, 3.1386032104492188, 33.07847213745117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6633669137954712 2.5929644107818604 27.59300994873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6425384283065796 2.6096577644348145 27.73911476135254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6617718935012817 2.9563405513763428 31.225177764892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.675474762916565 3.4532582759857178 36.20805740356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6958664655685425 3.639528512954712 38.09115219116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6553421020507812 3.5366463661193848 37.02180480957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665187120437622 3.239677906036377 34.06196594238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6617035865783691 2.9946484565734863 31.608186721801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6230453252792358 3.068713665008545 32.310184478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6938953399658203 3.031524658203125 32.00914001464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6291241645812988 3.594881772994995 37.57794189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6838529109954834 2.9375956058502197 31.0598087310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664617657661438 3.7221710681915283 38.886329650878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6635851860046387 3.404634714126587 35.70993423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6384154558181763 3.0605015754699707 32.243431091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6495710611343384 3.14270281791687 33.07659912109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6534539461135864 3.0469472408294678 32.122928619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7052651643753052 3.137152910232544 33.0767936706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7018712759017944 3.576664924621582 37.4685173034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7139825820922852 3.2938904762268066 34.65288543701172
  batch 40 loss: 1.7139825820922852, 3.2938904762268066, 34.65288543701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6785515546798706 2.8543527126312256 30.222078323364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629122495651245 3.0981292724609375 32.644203186035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6574828624725342 2.8640549182891846 30.298030853271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6634700298309326 3.120366334915161 32.86713409423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6486079692840576 2.5830764770507812 27.479372024536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.668400764465332 3.245657444000244 34.12497329711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6999247074127197 3.537046194076538 37.07038497924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698864698410034 2.899733066558838 30.66721534729004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710553765296936 3.404658555984497 35.757137298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776844263076782 2.7322192192077637 28.9998779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.705331802368164 2.9695019721984863 31.40035057067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6955424547195435 3.2863686084747314 34.559226989746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6758933067321777 2.989095449447632 31.566848754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6951497793197632 3.7655889987945557 39.35103988647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6680681705474854 3.171875 33.386817932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7196996212005615 3.280398368835449 34.523685455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6764994859695435 3.463787317276001 36.31437301635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6667778491973877 4.038829326629639 42.05507278442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6767947673797607 3.627953290939331 37.956329345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7354917526245117 3.482085704803467 36.55635070800781
  batch 60 loss: 1.7354917526245117, 3.482085704803467, 36.55635070800781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.667100429534912 3.6621453762054443 38.288551330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6915112733840942 3.7582242488861084 39.27375411987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679748773574829 3.163785219192505 33.31760025024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.677698016166687 3.168428897857666 33.36198425292969
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.660003662109375 2.535862922668457 27.018632888793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6914700269699097 3.1992390155792236 33.683860778808594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7061436176300049 3.248061180114746 34.1867561340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.700608491897583 3.338632822036743 35.086936950683594
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6958622932434082 3.0804860591888428 32.50072479248047
Total LOSS train 33.48462125338041 valid 33.864569664001465
CE LOSS train 1.6743535536986132 valid 0.42396557331085205
Contrastive LOSS train 3.1810267815223106 valid 0.7701215147972107
EPOCH 19:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7127156257629395 3.207688570022583 33.78960037231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7223351001739502 3.5929408073425293 37.6517448425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6953905820846558 2.8428399562835693 30.123790740966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055885791778564 3.008335590362549 31.788944244384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7228418588638306 3.3418326377868652 35.141170501708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695143461227417 2.954892635345459 31.244070053100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7255547046661377 3.6126112937927246 37.85166931152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7156803607940674 2.4245877265930176 25.961557388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7162688970565796 2.641277313232422 28.12904167175293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.735787272453308 2.618255138397217 27.918338775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7042826414108276 2.8305552005767822 30.00983428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6899666786193848 3.4186646938323975 35.87661361694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.685312032699585 3.5070579051971436 36.755889892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6861094236373901 2.650074005126953 28.18684959411621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7306983470916748 2.850257635116577 30.233274459838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7482490539550781 3.4193825721740723 35.942073822021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7215570211410522 3.7411909103393555 39.13346862792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7653578519821167 3.9512853622436523 41.27821350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7181241512298584 3.43021821975708 36.02030563354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7424893379211426 3.2738709449768066 34.481197357177734
  batch 20 loss: 1.7424893379211426, 3.2738709449768066, 34.481197357177734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7036337852478027 3.4647679328918457 36.35131072998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68233060836792 2.8923165798187256 30.605497360229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7031688690185547 2.954557180404663 31.248741149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.722931981086731 3.300595998764038 34.72888946533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.74606192111969 3.54353928565979 37.18145751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6989892721176147 3.342027425765991 35.1192626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.714717149734497 3.5516152381896973 37.23086929321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7010842561721802 3.4591736793518066 36.29281997680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6774847507476807 3.417905330657959 35.85654067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7273105382919312 3.5179948806762695 36.907257080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6702195405960083 2.9234795570373535 30.905014038085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.715591549873352 3.6441705226898193 38.15729522705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7023093700408936 2.981044054031372 31.51274871826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.69728684425354 3.781759023666382 39.51487731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.666698694229126 3.8058433532714844 39.72513198852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6673543453216553 3.700043201446533 38.66778564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663597583770752 3.321364641189575 34.87724304199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7132004499435425 3.690857410430908 38.62177276611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7143189907073975 3.0314247608184814 32.028564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7281626462936401 3.6564526557922363 38.292686462402344
  batch 40 loss: 1.7281626462936401, 3.6564526557922363, 38.292686462402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6901358366012573 3.1901512145996094 33.59164810180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6806871891021729 3.305101156234741 34.73169708251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6764185428619385 2.6346418857574463 28.022836685180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6865205764770508 2.937849760055542 31.065017700195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6781928539276123 3.5755038261413574 37.433231353759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7021514177322388 3.5637850761413574 37.340003967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.725874423980713 3.684011697769165 38.56599426269531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694446325302124 2.8047499656677246 29.741947174072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7247991561889648 3.2925214767456055 34.65001678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6983699798583984 3.3952126502990723 35.65049743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733011245727539 3.3772475719451904 35.50548553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7237071990966797 3.3284034729003906 35.00774383544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.699750542640686 3.2913644313812256 34.6133918762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7154422998428345 2.5394821166992188 27.11026382446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6957712173461914 2.7272257804870605 28.968029022216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7297651767730713 3.0922317504882812 32.65208435058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6939313411712646 3.5986969470977783 37.68090057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6813013553619385 4.027005195617676 41.951351165771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6919082403182983 2.825791597366333 29.9498233795166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.737187147140503 3.1331698894500732 33.068885803222656
  batch 60 loss: 1.737187147140503, 3.1331698894500732, 33.068885803222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6728566884994507 3.709956169128418 38.77241897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6925886869430542 3.6324565410614014 38.017154693603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6705079078674316 3.7636845111846924 39.30735397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.677078366279602 2.92177152633667 30.894794464111328
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.659191370010376 2.5117547512054443 26.7767391204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6740156412124634 2.9571709632873535 31.245723724365234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6855980157852173 2.8124918937683105 29.810518264770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6849658489227295 2.946910858154297 31.15407371520996
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6807303428649902 2.783790111541748 29.518632888793945
Total LOSS train 34.344811160747824 valid 30.432237148284912
CE LOSS train 1.7044230956297655 valid 0.42018258571624756
Contrastive LOSS train 3.2640388121971715 valid 0.695947527885437
EPOCH 20:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7010385990142822 2.906682014465332 30.767858505249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055734395980835 3.3742170333862305 35.44774627685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6815061569213867 3.182905912399292 33.51056671142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6902027130126953 3.735640525817871 39.046607971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7053972482681274 3.3300769329071045 35.006168365478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672764539718628 2.9468905925750732 31.14167022705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7056989669799805 3.389540433883667 35.601104736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6853638887405396 2.786334991455078 29.54871368408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6772394180297852 2.736530303955078 29.04254150390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7084897756576538 3.215026617050171 33.8587532043457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693583726882935 3.030268907546997 31.972047805786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6655665636062622 3.7872314453125 39.537879943847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660515308380127 3.3202590942382812 34.86310577392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6662248373031616 3.9161274433135986 40.82749938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7036917209625244 3.322293519973755 34.92662811279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7006083726882935 3.383104085922241 35.531646728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6474549770355225 2.9294493198394775 30.94194793701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6705631017684937 3.094041109085083 32.6109733581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6458576917648315 2.681593894958496 28.461795806884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6925528049468994 2.7368483543395996 29.061037063598633
  batch 20 loss: 1.6925528049468994, 2.7368483543395996, 29.061037063598633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6700127124786377 3.1090307235717773 32.760318756103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6491855382919312 3.327230215072632 34.921485900878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669226050376892 3.28068208694458 34.47604751586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6825846433639526 2.5599524974823 27.2821102142334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7080014944076538 2.6783900260925293 28.49190330505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6727080345153809 2.706801176071167 28.740720748901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.680492877960205 3.0402092933654785 32.082584381103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6730091571807861 2.8884940147399902 30.557950973510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6247397661209106 3.3486061096191406 35.110801696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6883354187011719 3.731940507888794 39.00774002075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6294440031051636 3.1090407371520996 32.719852447509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6887222528457642 2.8952393531799316 30.641115188598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6727770566940308 3.067875623703003 32.351531982421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672616720199585 3.4983561038970947 36.65617752075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6498236656188965 3.20934796333313 33.74330139160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6574485301971436 3.035127878189087 32.00872802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652940034866333 3.4029200077056885 35.6821403503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7028632164001465 3.253373146057129 34.23659133911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993050575256348 3.1647932529449463 33.34723663330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7047895193099976 3.772129535675049 39.426082611083984
  batch 40 loss: 1.7047895193099976, 3.772129535675049, 39.426082611083984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6716750860214233 3.6428582668304443 38.100257873535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6618152856826782 3.135746955871582 33.019283294677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6611000299453735 3.591930627822876 37.580406188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6679658889770508 3.491502046585083 36.582984924316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6527851819992065 2.919898271560669 30.85176658630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672333836555481 2.9264516830444336 30.936851501464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698673129081726 3.193965435028076 33.638328552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6666624546051025 3.0629165172576904 32.29582977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7090961933135986 3.5054500102996826 36.76359558105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671245813369751 3.1308867931365967 32.9801139831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698740005493164 3.684102773666382 38.539764404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6881009340286255 3.3517868518829346 35.205970764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6685521602630615 3.541250705718994 37.081058502197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.69014573097229 3.9933738708496094 41.62388610839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6646716594696045 3.4932618141174316 36.5972900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7074379920959473 3.123614549636841 32.94358444213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6682569980621338 3.442077398300171 36.08903121948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6556578874588013 3.0071909427642822 31.727567672729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.667696475982666 2.8639092445373535 30.306787490844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.71918785572052 3.389617919921875 35.6153678894043
  batch 60 loss: 1.71918785572052, 3.389617919921875, 35.6153678894043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6552119255065918 3.1651804447174072 33.30701446533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678279161453247 3.2345595359802246 34.02387619018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639509201049805 3.154339551925659 33.20734786987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543437242507935 3.0969321727752686 32.62366485595703
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6374531984329224 2.7656309604644775 29.29376220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6617388725280762 3.1301608085632324 32.963348388671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672300100326538 3.4009621143341064 35.68191909790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6701669692993164 2.8518896102905273 30.189064025878906
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6723934412002563 2.9239797592163086 30.91219139099121
Total LOSS train 33.7982477921706 valid 32.436630725860596
CE LOSS train 1.6762420580937312 valid 0.4180983603000641
Contrastive LOSS train 3.212200586612408 valid 0.7309949398040771
EPOCH 21:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6851274967193604 3.09735107421875 32.65863800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6955437660217285 3.0458059310913086 32.153602600097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6728824377059937 3.0597751140594482 32.270633697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.681960940361023 3.5813992023468018 37.49595260620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6973720788955688 3.0794425010681152 32.491798400878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.667245626449585 3.0550055503845215 32.21730041503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695909023284912 3.445164442062378 36.147552490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6751595735549927 2.952303171157837 31.198190689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671628475189209 3.4746007919311523 36.41763687133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695426106452942 3.406592607498169 35.7613525390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6677385568618774 2.7915384769439697 29.58312225341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6680071353912354 3.1402463912963867 33.070472717285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693395376205444 2.9019038677215576 30.688379287719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6743909120559692 3.4995527267456055 36.669921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7073899507522583 3.6710662841796875 38.418052673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7114622592926025 3.5334575176239014 37.04603958129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6647111177444458 3.4553098678588867 36.217811584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6841765642166138 3.286266803741455 34.546844482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6640214920043945 3.1159682273864746 32.82370376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7075101137161255 3.1476151943206787 33.18366241455078
  batch 20 loss: 1.7075101137161255, 3.1476151943206787, 33.18366241455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6856918334960938 3.220381736755371 33.88951110839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6713167428970337 3.5444743633270264 37.11606216430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68418550491333 3.0971176624298096 32.65536117553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6978442668914795 2.794372081756592 29.641565322875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7197387218475342 3.225053310394287 33.970272064208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6894688606262207 3.154733180999756 33.23680114746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7074155807495117 4.031559944152832 42.023014068603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.704519510269165 3.705430507659912 38.758827209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.677488923072815 4.156296253204346 43.24045181274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7259703874588013 3.6049399375915527 37.77537155151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6707160472869873 3.707501173019409 38.7457275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7166895866394043 3.495302438735962 36.669715881347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.709495186805725 3.422185182571411 35.93134689331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6991305351257324 3.8278133869171143 39.977264404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6921147108078003 3.3338027000427246 35.03014373779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6984469890594482 3.375260353088379 35.4510498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6998976469039917 3.0597336292266846 32.29723358154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.728588581085205 3.3819148540496826 35.54773712158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.720328450202942 3.3236212730407715 34.95654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7211531400680542 3.698125123977661 38.7024040222168
  batch 40 loss: 1.7211531400680542, 3.698125123977661, 38.7024040222168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7008501291275024 3.4487953186035156 36.188804626464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6836212873458862 3.288478374481201 34.56840515136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6927425861358643 3.4589943885803223 36.282684326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6987941265106201 3.4246931076049805 35.94572830200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7189698219299316 3.7469794750213623 39.18876647949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7193526029586792 3.6664671897888184 38.38402557373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.723284363746643 3.727713108062744 39.00041580200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.720400333404541 3.714658498764038 38.86698532104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7280282974243164 3.5515928268432617 37.24395751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.69385826587677 3.3126401901245117 34.82026290893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7135354280471802 3.2405617237091064 34.1191520690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702237844467163 3.185561180114746 33.5578498840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6741162538528442 3.2897043228149414 34.57115936279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6964755058288574 3.163347005844116 33.3299446105957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.685852289199829 3.6152145862579346 37.83799743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7090363502502441 3.27175235748291 34.42655944824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6817152500152588 3.3927359580993652 35.60907745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6743804216384888 3.4505867958068848 36.18024826049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6670228242874146 3.390249013900757 35.56951141357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7042872905731201 3.0667409896850586 32.37169647216797
  batch 60 loss: 1.7042872905731201, 3.0667409896850586, 32.37169647216797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6441545486450195 3.004672050476074 31.690876007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698404550552368 3.2556939125061035 34.22677993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6482034921646118 3.4472622871398926 36.120826721191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6550014019012451 3.2277350425720215 33.93235397338867
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6410942077636719 2.6712052822113037 28.353147506713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6563047170639038 3.4946694374084473 36.602996826171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665124773979187 3.561943292617798 37.2845573425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6701607704162598 3.5086185932159424 36.75634765625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.660335898399353 3.4550485610961914 36.210819244384766
Total LOSS train 35.21637367835412 valid 36.713680267333984
CE LOSS train 1.6911393807484554 valid 0.41508397459983826
Contrastive LOSS train 3.3525233818934512 valid 0.8637621402740479
EPOCH 22:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673294186592102 3.6774182319641113 38.44747543334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6831440925598145 3.501323699951172 36.696380615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.656973958015442 2.895643472671509 30.6134090423584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6617554426193237 4.149473190307617 43.15648651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6762135028839111 3.8513810634613037 40.19002151489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6251335144042969 3.881643533706665 40.44157028198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6601274013519287 3.6001477241516113 37.66160202026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6396528482437134 2.4628286361694336 26.2679386138916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6491425037384033 2.903792142868042 30.68706512451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6785398721694946 3.630784511566162 37.986385345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6203843355178833 3.004103422164917 31.661418914794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.613329529762268 3.959958791732788 41.21291732788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182851791381836 3.4020087718963623 35.63837432861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626988172531128 3.887251853942871 40.499507904052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676669716835022 3.6751551628112793 38.42822265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.657710313796997 4.125502109527588 42.9127311706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6003680229187012 3.471060037612915 36.310970306396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6277048587799072 3.425104856491089 35.878753662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5947532653808594 3.8258309364318848 39.85306167602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.648925542831421 3.643683433532715 38.08576202392578
  batch 20 loss: 1.648925542831421, 3.643683433532715, 38.08576202392578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6139180660247803 3.991669178009033 41.530609130859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.58303964138031 3.5563228130340576 37.14626693725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5978037118911743 2.9197275638580322 30.79507827758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6168144941329956 2.664050340652466 28.25731658935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6542742252349854 3.9775781631469727 41.4300537109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5902749300003052 3.414794921875 35.738224029541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6142419576644897 3.386641263961792 35.480655670166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6045382022857666 3.213698625564575 33.74152374267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5419557094573975 3.273980140686035 34.28175354003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6400108337402344 3.860118865966797 40.2411994934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5430229902267456 2.9025373458862305 30.568395614624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6265790462493896 3.165233612060547 33.27891540527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.598597526550293 3.515533208847046 36.753929138183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.595859408378601 3.8109803199768066 39.70566177368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5541292428970337 3.790566921234131 39.459800720214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5717722177505493 3.6607987880706787 38.17975997924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5789374113082886 3.1682636737823486 33.261573791503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6547406911849976 2.9268674850463867 30.923416137695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654190182685852 3.6618361473083496 38.272552490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698633432388306 3.203801393508911 33.70787811279297
  batch 40 loss: 1.6698633432388306, 3.203801393508911, 33.70787811279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6292442083358765 3.0099661350250244 31.728904724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6141780614852905 3.564375638961792 37.2579345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6056872606277466 3.953031301498413 41.1359977722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6175181865692139 3.3699400424957275 35.31692123413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.59065580368042 3.4184744358062744 35.7754020690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6220314502716064 3.1669163703918457 33.291194915771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.658852458000183 2.823681116104126 29.895662307739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.610611915588379 3.2011771202087402 33.62238311767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6688942909240723 3.670353889465332 38.372432708740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6016104221343994 3.820209503173828 39.803707122802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6511623859405518 3.3008735179901123 34.65989685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6330093145370483 3.753657341003418 39.169586181640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6125590801239014 3.0869085788726807 32.48164367675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432030200958252 3.3156769275665283 34.79997253417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6103541851043701 3.2804131507873535 34.414485931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6673862934112549 3.6411821842193604 38.07920837402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6045747995376587 3.0892131328582764 32.496707916259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.591933250427246 3.4136109352111816 35.72804260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.60628080368042 3.147813320159912 33.084415435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6846078634262085 3.585174322128296 37.53635025024414
  batch 60 loss: 1.6846078634262085, 3.585174322128296, 37.53635025024414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5859946012496948 3.7990059852600098 39.576053619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6173250675201416 3.5822904109954834 37.44022750854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.60035240650177 3.725414991378784 38.8545036315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5924657583236694 2.984046459197998 31.432931900024414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5679649114608765 2.455712080001831 26.125085830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6000202894210815 3.249971628189087 34.09973907470703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6173661947250366 3.447094678878784 36.088314056396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.615070104598999 3.2647955417633057 34.26302719116211
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6127831935882568 3.3770668506622314 35.38344955444336
Total LOSS train 35.96098879300631 valid 34.958632469177246
CE LOSS train 1.6228018137124869 valid 0.4031957983970642
Contrastive LOSS train 3.4338186960953934 valid 0.8442667126655579
EPOCH 23:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6366060972213745 3.4042584896087646 35.6791877746582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644928216934204 2.9193551540374756 30.83847999572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6115025281906128 2.8782012462615967 30.39351463317871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6215211153030396 2.917050838470459 30.792030334472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6479214429855347 3.0770156383514404 32.4180793762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5998566150665283 3.172086000442505 33.320716857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6463595628738403 3.231123924255371 33.95759963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182337999343872 2.6155693531036377 27.773927688598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6117947101593018 2.8856475353240967 30.468271255493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543142795562744 3.196566104888916 33.61997604370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6113303899765015 3.3197948932647705 34.80928039550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6127524375915527 2.729262590408325 28.905378341674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6140363216400146 2.9998085498809814 31.61212158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6219676733016968 3.4544577598571777 36.16654586791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6762679815292358 3.2963175773620605 34.63944625854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6650424003601074 3.2076573371887207 33.741615295410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6099096536636353 3.1602256298065186 33.21216583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.63984215259552 3.0918338298797607 32.55818176269531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6164039373397827 3.1156392097473145 32.772796630859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6700375080108643 2.68686842918396 28.53872299194336
  batch 20 loss: 1.6700375080108643, 2.68686842918396, 28.53872299194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.64098060131073 2.54811429977417 27.12212371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6164475679397583 4.093628883361816 42.552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6353485584259033 3.5001254081726074 36.63660430908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6500288248062134 2.9897143840789795 31.54717254638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6747348308563232 3.7515456676483154 39.19019317626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6353871822357178 3.3117477893829346 34.752864837646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644777774810791 3.541605234146118 37.06083297729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6401489973068237 3.259221076965332 34.232357025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5935312509536743 3.8372297286987305 39.9658317565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6801220178604126 3.1872446537017822 33.55256652832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6021581888198853 3.709567070007324 38.69783020019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6641336679458618 3.9092588424682617 40.7567253112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.639176607131958 2.938490629196167 31.024084091186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.634899377822876 3.6143648624420166 37.77854919433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6087528467178345 3.266559600830078 34.274349212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.619938611984253 4.3474249839782715 45.09418869018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6163972616195679 3.890056610107422 40.516963958740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6707912683486938 4.081587314605713 42.486663818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670567512512207 3.8912153244018555 40.58272171020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6821932792663574 3.5163192749023438 36.84538650512695
  batch 40 loss: 1.6821932792663574, 3.5163192749023438, 36.84538650512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6433544158935547 3.298840045928955 34.631752014160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6328885555267334 3.214749336242676 33.78038024902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626476526260376 3.271413564682007 34.34061050415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6374738216400146 3.6138381958007812 37.775856018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.617844820022583 3.5422468185424805 37.040313720703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6405550241470337 2.909480094909668 30.735355377197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669115424156189 3.2047908306121826 33.71702575683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6366513967514038 3.1609911918640137 33.24656295776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6816978454589844 3.2474217414855957 34.155914306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6382231712341309 3.2464914321899414 34.10313415527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6706215143203735 3.5490972995758057 37.16159439086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.66441011428833 3.1061744689941406 32.72615432739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6447993516921997 2.53769588470459 27.021759033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6730595827102661 3.44358491897583 36.108909606933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643370270729065 3.0308446884155273 31.95181655883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6867165565490723 2.993014335632324 31.616859436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6433005332946777 3.696890354156494 38.61220169067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.625069260597229 3.5884997844696045 37.510066986083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6372679471969604 3.291478157043457 34.55204772949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6997820138931274 3.3604838848114014 35.304622650146484
  batch 60 loss: 1.6997820138931274, 3.3604838848114014, 35.304622650146484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6292799711227417 3.858332872390747 40.212608337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6599215269088745 4.211829662322998 43.77821731567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6448509693145752 3.5021355152130127 36.66620635986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6442152261734009 3.7224464416503906 38.86867904663086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.62696373462677 2.9359896183013916 30.986860275268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6593888998031616 3.5845320224761963 37.50471115112305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671041488647461 3.350037097930908 35.171409606933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6720030307769775 3.395636796951294 35.62837219238281
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6645487546920776 3.5559186935424805 37.22373580932617
Total LOSS train 34.8845275585468 valid 36.382057189941406
CE LOSS train 1.6410623788833618 valid 0.4161371886730194
Contrastive LOSS train 3.324346505678617 valid 0.8889796733856201
EPOCH 24:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6844151020050049 3.249584913253784 34.18026351928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6911019086837769 3.5764758586883545 37.45586013793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663254737854004 2.5060207843780518 26.723461151123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6801178455352783 3.0118227005004883 31.7983455657959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.697749376296997 3.4903059005737305 36.60081100463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6601452827453613 3.1234214305877686 32.89435958862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6811391115188599 3.3505711555480957 35.186851501464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6673768758773804 2.808289051055908 29.750267028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6582200527191162 2.5895040035247803 27.553260803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.689090609550476 3.4183921813964844 35.87301254272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6524832248687744 2.819223642349243 29.84471893310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6508578062057495 3.5367703437805176 37.018558502197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6584068536758423 3.8496923446655273 40.155330657958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663274884223938 3.4514832496643066 36.17810821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.706791877746582 3.27632737159729 34.47006607055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7045484781265259 3.9519076347351074 41.22362518310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6626580953598022 3.397611379623413 35.638771057128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6899632215499878 3.0450830459594727 32.14079284667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6683416366577148 3.4771435260772705 36.43977737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7099950313568115 2.6269118785858154 27.979114532470703
  batch 20 loss: 1.7099950313568115, 2.6269118785858154, 27.979114532470703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6871668100357056 3.1837453842163086 33.524620056152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6593557596206665 3.055387020111084 32.213226318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776654720306396 2.8741793632507324 30.41946029663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6910723447799683 3.1117498874664307 32.808570861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7066199779510498 3.429046154022217 35.9970817565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6706615686416626 3.7987940311431885 39.65860366821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6814148426055908 3.6156094074249268 37.83750915527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6744078397750854 2.9043080806732178 30.71748924255371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.622412085533142 2.7472071647644043 29.094484329223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6975899934768677 3.1345062255859375 33.04265213012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62779700756073 3.4000518321990967 35.62831497192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6845707893371582 3.0582423210144043 32.26699447631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6775174140930176 3.0351922512054443 32.029441833496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6619586944580078 3.174323320388794 33.405189514160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626404881477356 3.067749500274658 32.303897857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6442328691482544 2.972985029220581 31.37408447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6420754194259644 2.89229416847229 30.56501579284668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6886601448059082 3.2868266105651855 34.55692672729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6972352266311646 3.4194297790527344 35.89153289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.704332709312439 3.469266653060913 36.39699935913086
  batch 40 loss: 1.704332709312439, 3.469266653060913, 36.39699935913086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6616034507751465 3.339446783065796 35.056068420410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6533479690551758 3.6844406127929688 38.49775314331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6480555534362793 3.1032204627990723 32.680259704589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6586472988128662 3.5778043270111084 37.43668746948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6394165754318237 2.737553119659424 29.01494598388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6580640077590942 3.2565603256225586 34.223670959472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6828604936599731 3.4696388244628906 36.379249572753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445212364196777 3.0703256130218506 32.3477783203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6930452585220337 3.2406845092773438 34.099891662597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6474028825759888 3.1198973655700684 32.846378326416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6835718154907227 3.3250198364257812 34.93376922607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6755183935165405 3.3548638820648193 35.22415542602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6547555923461914 3.167269229888916 33.32744598388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.683085322380066 3.583656072616577 37.51964569091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6598517894744873 2.959911823272705 31.258968353271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7036808729171753 4.033138275146484 42.035064697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.658105492591858 3.6583926677703857 38.24203109741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6424623727798462 3.553086757659912 37.17333221435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6581692695617676 3.4694697856903076 36.352867126464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7091844081878662 3.6251180171966553 37.96036148071289
  batch 60 loss: 1.7091844081878662, 3.6251180171966553, 37.96036148071289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6404359340667725 4.3466572761535645 45.10700607299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6648938655853271 3.0991220474243164 32.65611267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641404151916504 3.115441083908081 32.795814514160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.634138822555542 3.4120147228240967 35.75428771972656
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.616918921470642 2.9463744163513184 31.080663681030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6477123498916626 3.4615108966827393 36.262821197509766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660554051399231 3.1778957843780518 33.439510345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6595826148986816 3.661393642425537 38.273521423339844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.655963659286499 3.6329705715179443 37.98566818237305
Total LOSS train 34.3514101762038 valid 36.49038028717041
CE LOSS train 1.6688650094545805 valid 0.41399091482162476
Contrastive LOSS train 3.2682545295128453 valid 0.9082426428794861
EPOCH 25:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6738582849502563 3.269810676574707 34.37196350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6828268766403198 3.723916530609131 38.921993255615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6601399183273315 2.935058832168579 31.010726928710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6671630144119263 3.1593017578125 33.26018142700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679307222366333 3.1933188438415527 33.61249542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6449954509735107 3.437486410140991 36.019859313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678247094154358 3.586643934249878 37.54468536376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6587342023849487 2.4386696815490723 26.045429229736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654012680053711 3.081055164337158 32.464561462402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6917420625686646 3.4485349655151367 36.177093505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659785270690918 3.6799376010894775 38.459163665771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6534019708633423 3.787635326385498 39.529754638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6526414155960083 3.1920907497406006 33.573551177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663146734237671 3.942143678665161 41.0845832824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7040131092071533 3.602134943008423 37.725364685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6997735500335693 3.5865719318389893 37.56549072265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.650125503540039 3.000232219696045 31.652448654174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6627339124679565 3.2618699073791504 34.28143310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.647053837776184 3.5804896354675293 37.45195007324219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694990873336792 3.4970407485961914 36.66539764404297
  batch 20 loss: 1.694990873336792, 3.4970407485961914, 36.66539764404297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664715051651001 3.012289524078369 31.787609100341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432088613510132 3.421597957611084 35.859188079833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6563435792922974 3.0259783267974854 31.916126251220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6660767793655396 3.2086689472198486 33.75276565551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6853889226913452 3.5645132064819336 37.33052444458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.648199200630188 3.4243834018707275 35.89203643798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693460941314697 2.950874090194702 31.17808723449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6680688858032227 3.4109253883361816 35.777320861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6204370260238647 3.037313222885132 31.99357032775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993670463562012 2.968090772628784 31.38027572631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6234122514724731 3.112637996673584 32.749794006347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6745859384536743 3.121613025665283 32.890716552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6677895784378052 3.447009325027466 36.137882232666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663877010345459 3.827361583709717 39.93749237060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.627963900566101 3.5846128463745117 37.47409439086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6339706182479858 3.709827423095703 38.73224639892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6402784585952759 3.1053357124328613 32.693634033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6913036108016968 2.8617448806762695 30.308752059936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6949154138565063 3.3889718055725098 35.584632873535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7031710147857666 2.867435932159424 30.37752914428711
  batch 40 loss: 1.7031710147857666, 2.867435932159424, 30.37752914428711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659904956817627 2.792781352996826 29.587718963623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6459720134735107 3.375108003616333 35.39705276489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6329436302185059 2.8278095722198486 29.911039352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.638940453529358 3.4285261631011963 35.92420196533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.615981101989746 2.9753897190093994 31.369876861572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642663598060608 2.602370262145996 27.666366577148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687983751296997 2.80642032623291 29.752187728881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466889381408691 4.002577781677246 41.67247009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7005378007888794 3.5417213439941406 37.11775207519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6695473194122314 3.391338348388672 35.58293151855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981961727142334 3.231638193130493 34.01457977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682761311531067 3.202876567840576 33.71152877807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6618911027908325 3.0261266231536865 31.923158645629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6956268548965454 4.003189563751221 41.72751998901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6704000234603882 3.9319217205047607 40.98961639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70260751247406 2.7599408626556396 29.302017211914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6526638269424438 2.758378267288208 29.236446380615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6363770961761475 3.3668088912963867 35.304466247558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6533081531524658 4.423470497131348 45.88801193237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7087364196777344 3.3453032970428467 35.16176986694336
  batch 60 loss: 1.7087364196777344, 3.3453032970428467, 35.16176986694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6320818662643433 3.336427927017212 34.996360778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6500191688537598 3.1524977684020996 33.17499923706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.632796049118042 3.4322638511657715 35.95543670654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624834418296814 3.5696961879730225 37.32179641723633
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5957077741622925 2.7713003158569336 29.3087100982666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6172997951507568 2.689006805419922 28.507368087768555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.632372498512268 2.9330599308013916 30.962970733642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6305656433105469 3.110823392868042 32.738800048828125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6293926239013672 3.2700600624084473 34.329994201660156
Total LOSS train 34.66416033231295 valid 31.634783267974854
CE LOSS train 1.6624659006412212 valid 0.4073481559753418
Contrastive LOSS train 3.3001694202423097 valid 0.8175150156021118
EPOCH 26:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6513615846633911 3.082484245300293 32.47620391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.655653476715088 3.4055140018463135 35.71079635620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.631622552871704 2.6620116233825684 28.251739501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6393715143203735 3.455265522003174 36.19202423095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6531561613082886 3.684089422225952 38.494049072265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.613425374031067 2.989567995071411 31.509105682373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6506584882736206 3.999034881591797 41.64100646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6267588138580322 3.3069007396698 34.69576644897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6196616888046265 3.9851293563842773 41.4709587097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6506932973861694 3.566824197769165 37.31893539428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6018002033233643 3.00016450881958 31.603445053100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6013661623001099 3.4633798599243164 36.235164642333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6051251888275146 3.472726583480835 36.33238983154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6091639995574951 4.017651557922363 41.78567886352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629700660705566 2.6836776733398438 28.499746322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6569674015045166 3.6151838302612305 37.808807373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5907588005065918 3.845025062561035 40.04100799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6269057989120483 2.954850196838379 31.17540740966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6010549068450928 3.135640859603882 32.957462310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6561007499694824 3.39504075050354 35.606510162353516
  batch 20 loss: 1.6561007499694824, 3.39504075050354, 35.606510162353516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6249483823776245 3.2326130867004395 33.95107650756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5997734069824219 2.9352872371673584 30.952646255493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.620352864265442 2.981295347213745 31.433307647705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6404908895492554 3.2711880207061768 34.35237121582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6680519580841064 3.2856414318084717 34.52446746826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62290358543396 3.645775318145752 38.08065414428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6384204626083374 3.4202349185943604 35.84077072143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6301881074905396 3.375089406967163 35.381080627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5811723470687866 3.7776103019714355 39.357276916503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6677920818328857 3.1541404724121094 33.209197998046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5925610065460205 3.142059326171875 33.013153076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6594387292861938 3.7247965335845947 38.907405853271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644046425819397 4.138134956359863 43.025394439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6456341743469238 3.6788477897644043 38.434112548828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6150485277175903 2.9012444019317627 30.627492904663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6301881074905396 3.295541286468506 34.585601806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6417272090911865 3.0699117183685303 32.340843200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6943703889846802 3.3844716548919678 35.539085388183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7000830173492432 3.181454658508301 33.51462936401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7102023363113403 3.01491117477417 31.859315872192383
  batch 40 loss: 1.7102023363113403, 3.01491117477417, 31.859315872192383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6800733804702759 3.4358291625976562 36.03836441040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6727666854858398 2.930565118789673 30.978416442871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676289677619934 3.136852979660034 33.04481887817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6878292560577393 3.187307596206665 33.56090545654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6801323890686035 3.868504047393799 40.36517333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6916146278381348 4.035131931304932 42.04293441772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7200407981872559 3.981297731399536 41.533016204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6921031475067139 3.212216377258301 33.814266204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7154672145843506 3.8921549320220947 40.63701629638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.705704689025879 2.679715871810913 28.502864837646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7266509532928467 2.9918434619903564 31.645084381103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.718949317932129 3.162017583847046 33.33912658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6986634731292725 3.1584548950195312 33.28321075439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7147971391677856 3.451801300048828 36.232810974121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6961482763290405 4.347383975982666 45.169986724853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7138890027999878 3.9029500484466553 40.74338912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6873029470443726 3.620816469192505 37.89546585083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6735870838165283 4.038214683532715 42.05573654174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6742782592773438 3.9910812377929688 41.58509063720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7192786931991577 4.162312030792236 43.34239959716797
  batch 60 loss: 1.7192786931991577, 4.162312030792236, 43.34239959716797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669180989265442 3.4681427478790283 36.350608825683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6837003231048584 3.45576810836792 36.24138259887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6688565015792847 3.4202284812927246 35.87114334106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6585266590118408 3.3784632682800293 35.44316101074219
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6458637714385986 2.717576503753662 28.821630477905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6669706106185913 2.718035936355591 28.84733009338379
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679409146308899 2.762040853500366 29.29981803894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6775941848754883 2.594684362411499 27.624439239501953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6675074100494385 3.0089352130889893 31.756858825683594
Total LOSS train 35.804216825045074 valid 29.38211154937744
CE LOSS train 1.6569794691525972 valid 0.4168768525123596
Contrastive LOSS train 3.4147237300872804 valid 0.7522338032722473
EPOCH 27:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6863014698028564 2.7528610229492188 29.21491241455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.689591884613037 3.3616209030151367 35.30580139160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6657747030258179 3.116637945175171 32.8321533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6700966358184814 3.2336628437042236 34.0067253112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6789524555206299 2.7737345695495605 29.41629981994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6531243324279785 3.3401451110839844 35.0545768737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682569146156311 3.861398935317993 40.29655838012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6614106893539429 2.9170944690704346 30.832355499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6548980474472046 2.8174450397491455 29.829347610473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682565450668335 2.70029354095459 28.685501098632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6552793979644775 3.419313669204712 35.848419189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6520963907241821 3.2770020961761475 34.422115325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.651291012763977 2.8529489040374756 30.1807804107666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6581735610961914 3.12027907371521 32.860965728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7080081701278687 3.6441829204559326 38.149837493896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6983230113983154 3.3265700340270996 34.96402359008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6517279148101807 3.530463218688965 36.956363677978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672027587890625 3.7486422061920166 39.158451080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6551467180252075 3.3641669750213623 35.296817779541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6975905895233154 3.3425076007843018 35.12266540527344
  batch 20 loss: 1.6975905895233154, 3.3425076007843018, 35.12266540527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671304702758789 3.2167959213256836 33.839263916015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6462504863739014 4.048124313354492 42.12749481201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6602717638015747 3.682810068130493 38.488372802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665636420249939 3.6007208824157715 37.672847747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6882821321487427 4.497753620147705 46.66581726074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6439852714538574 4.1986212730407715 43.63019943237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6471084356307983 3.6631011962890625 38.27812194824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6479895114898682 3.190599203109741 33.55398178100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6018180847167969 3.1400246620178223 33.0020637512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6724203824996948 3.6567962169647217 38.24038314819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.609315037727356 3.62841796875 37.89349365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6665962934494019 3.706981658935547 38.736412048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6510072946548462 3.466172456741333 36.3127326965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6499651670455933 3.4363129138946533 36.013092041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6198358535766602 3.5337600708007812 36.957435607910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6355348825454712 3.7139503955841064 38.77503967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6341943740844727 2.7278060913085938 28.912254333496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6854053735733032 2.366342306137085 25.34882926940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6907483339309692 3.9939393997192383 41.63014221191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6916050910949707 3.2264485359191895 33.95608901977539
  batch 40 loss: 1.6916050910949707, 3.2264485359191895, 33.95608901977539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6531609296798706 3.1237854957580566 32.891014099121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6399307250976562 2.9013421535491943 30.653352737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641133189201355 3.003046989440918 31.671602249145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6527774333953857 3.0590500831604004 32.24327850341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6376157999038696 2.5826072692871094 27.463687896728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6612571477890015 2.9017484188079834 30.678741455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6911698579788208 2.9250142574310303 30.941312789916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6553977727890015 3.471062183380127 36.36602020263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.709255337715149 3.109649419784546 32.80575180053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6699941158294678 2.5979530811309814 27.649524688720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702864170074463 3.325727939605713 34.96014404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698825716972351 3.4626080989837646 36.32490539550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6745212078094482 3.6209938526153564 37.88446044921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6986620426177979 2.7927255630493164 29.625917434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6753400564193726 3.707519292831421 38.75053024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7148643732070923 2.9580583572387695 31.295448303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6775779724121094 3.107895851135254 32.75653839111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6649140119552612 3.353200674057007 35.19691848754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672406792640686 3.206516742706299 33.737571716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.723964810371399 3.523219585418701 36.95616149902344
  batch 60 loss: 1.723964810371399, 3.523219585418701, 36.95616149902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6640985012054443 4.227200031280518 43.93609619140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.683408498764038 3.2999284267425537 34.68268966674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664029598236084 3.6355390548706055 38.0194206237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6609669923782349 3.6262640953063965 37.92361068725586
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6399143934249878 3.011265754699707 31.75257110595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6674829721450806 3.0778164863586426 32.445648193359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6773853302001953 3.3255927562713623 34.933311462402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.675192952156067 3.102931022644043 32.70450210571289
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6799250841140747 3.0169103145599365 31.849029541015625
Total LOSS train 34.85593860332782 valid 32.98312282562256
CE LOSS train 1.666650392458989 valid 0.4199812710285187
Contrastive LOSS train 3.3189288139343263 valid 0.7542275786399841
EPOCH 28:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6932852268218994 3.1613106727600098 33.306392669677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7091253995895386 3.2817513942718506 34.52663803100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6804594993591309 3.028186321258545 31.962324142456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6834466457366943 3.018064022064209 31.86408805847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025107145309448 2.853116750717163 30.233678817749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6653642654418945 3.27201509475708 34.38551330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6919454336166382 3.9077634811401367 40.76958084106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6699304580688477 3.3471550941467285 35.1414794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6625791788101196 3.7148706912994385 38.81128692626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6923357248306274 3.7643651962280273 39.33599090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6526737213134766 3.4620254039764404 36.272926330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6557084321975708 3.538630247116089 37.04201126098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6561088562011719 3.274200201034546 34.398109436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6585516929626465 3.454533100128174 36.203880310058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7035146951675415 3.4398036003112793 36.1015510559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981624364852905 3.2301504611968994 33.99966812133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6405425071716309 3.4692461490631104 36.333003997802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6596245765686035 3.2843117713928223 34.502742767333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.63642418384552 3.393223524093628 35.568660736083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6847102642059326 3.480034589767456 36.48505783081055
  batch 20 loss: 1.6847102642059326, 3.480034589767456, 36.48505783081055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6547871828079224 3.1567251682281494 33.22203826904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6388994455337524 3.227445363998413 33.913352966308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6552923917770386 3.3855297565460205 35.510589599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6721051931381226 3.388245105743408 35.5545539855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6953397989273071 3.9664645195007324 41.3599853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6583905220031738 3.254636287689209 34.20475387573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6642241477966309 3.9265060424804688 40.929283142089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670096516609192 3.106937885284424 32.73947525024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6246353387832642 3.4841020107269287 36.46565246582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.693358063697815 3.271270513534546 34.406063079833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6331692934036255 3.4569151401519775 36.20232391357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676712989807129 3.6540350914001465 38.217063903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649491310119629 3.5788016319274902 37.43750762939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6511998176574707 3.5217278003692627 36.86847686767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.61893892288208 3.0174267292022705 31.79320526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6382811069488525 3.437992572784424 36.01820755004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6417850255966187 3.6063308715820312 37.70509338378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6871216297149658 2.8109405040740967 29.796527862548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6990702152252197 3.297703266143799 34.67610168457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7027848958969116 3.5726423263549805 37.4292106628418
  batch 40 loss: 1.7027848958969116, 3.5726423263549805, 37.4292106628418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6666566133499146 2.971632242202759 31.382978439331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654268741607666 2.784120798110962 29.49547576904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646893858909607 3.0173232555389404 31.820127487182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6541321277618408 3.2547638416290283 34.2017707824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6371512413024902 3.2307169437408447 33.94432067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6627148389816284 2.7621471881866455 29.2841854095459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691853404045105 3.5604538917541504 37.296390533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646606683731079 3.4353525638580322 36.0001335144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7013723850250244 3.3088490962982178 34.78986358642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494446992874146 3.1791999340057373 33.441444396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6877638101577759 3.4170522689819336 35.8582878112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6827107667922974 3.5918962955474854 37.60167694091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6564462184906006 3.0701169967651367 32.35761642456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.683050513267517 3.1724562644958496 33.407615661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6499792337417603 3.037144899368286 32.021427154541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6970865726470947 3.3016457557678223 34.71354293823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6547737121582031 3.711170196533203 38.766475677490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6306822299957275 3.7373287677764893 39.003971099853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6349759101867676 4.052087306976318 42.15584945678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6932728290557861 3.5898277759552 37.591548919677734
  batch 60 loss: 1.6932728290557861, 3.5898277759552, 37.591548919677734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6190367937088013 3.338463306427002 35.00366973876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6477608680725098 3.496738910675049 36.615150451660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633517861366272 4.089548110961914 42.52899932861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6375969648361206 3.6367485523223877 38.00508117675781
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.614566445350647 3.2773001194000244 34.387569427490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6381007432937622 2.888030529022217 30.518407821655273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6516132354736328 3.012467384338379 31.776287078857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.647192358970642 2.813899278640747 29.786184310913086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6486910581588745 2.9594104290008545 31.242795944213867
Total LOSS train 35.436449608436 valid 30.830918788909912
CE LOSS train 1.663953893001263 valid 0.41217276453971863
Contrastive LOSS train 3.3772495636573203 valid 0.7398526072502136
EPOCH 29:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6661572456359863 3.0361030101776123 32.02718734741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6767919063568115 3.406191825866699 35.738712310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6525884866714478 3.598137140274048 37.63396072387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6610774993896484 3.6909689903259277 38.570770263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6823608875274658 3.699902296066284 38.6813850402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6402146816253662 3.2370829582214355 34.011043548583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6745024919509888 3.2410285472869873 34.08478927612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646065592765808 2.92870831489563 30.933147430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644900918006897 3.192643642425537 33.57133865356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6804383993148804 3.5317111015319824 36.99755096435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6324925422668457 2.8144149780273438 29.776641845703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6348717212677002 3.5824525356292725 37.45939636230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.630985140800476 3.6598095893859863 38.22908020019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6289860010147095 3.2389729022979736 34.018714904785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.685248851776123 3.5932905673980713 37.61815643310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684617042541504 3.5431692600250244 37.116310119628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6263068914413452 3.2296457290649414 33.92276382446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6560964584350586 3.6641249656677246 38.29734802246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6250126361846924 3.2501580715179443 34.126590728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6806526184082031 3.5017330646514893 36.69798278808594
  batch 20 loss: 1.6806526184082031, 3.5017330646514893, 36.69798278808594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6502360105514526 3.521792411804199 36.868160247802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6208629608154297 3.2130753993988037 33.751617431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.638596534729004 2.9715521335601807 31.35411834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6561139822006226 3.372178316116333 35.37789535522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6825982332229614 2.9472286701202393 31.154884338378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6398619413375854 3.212291955947876 33.76278305053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652682900428772 3.3085899353027344 34.738582611083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6461182832717896 2.741753101348877 29.063648223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6020991802215576 3.0468125343322754 32.07022476196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6809742450714111 3.3267693519592285 34.948665618896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6085803508758545 3.8388144969940186 39.996726989746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6649686098098755 3.3001205921173096 34.666175842285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6547473669052124 2.870649814605713 30.361244201660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6542096138000488 3.302258014678955 34.676788330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6183428764343262 3.254117012023926 34.15951156616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.634779453277588 3.6384708881378174 38.01948928833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642898440361023 3.284698247909546 34.4898796081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6955158710479736 3.1492228507995605 33.187744140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6959717273712158 2.9880216121673584 31.576189041137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055586576461792 3.3747572898864746 35.45313262939453
  batch 40 loss: 1.7055586576461792, 3.3747572898864746, 35.45313262939453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6732664108276367 2.9990358352661133 31.663623809814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6734110116958618 2.9537901878356934 31.211313247680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665856957435608 3.7201006412506104 38.86686325073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6706551313400269 3.0047338008880615 31.717992782592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6634336709976196 2.9016337394714355 30.679771423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6761972904205322 3.6974337100982666 38.650535583496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7199972867965698 3.5446364879608154 37.16636276245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7000144720077515 3.6137757301330566 37.837772369384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7321641445159912 3.4778714179992676 36.5108757019043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.701568841934204 3.629207134246826 37.9936408996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7262033224105835 2.870856761932373 30.434772491455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7326706647872925 4.094301223754883 42.675682067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7063424587249756 3.548588275909424 37.192222595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.724885106086731 3.1576833724975586 33.301719665527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7099553346633911 3.2700202465057373 34.410160064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.730919361114502 2.9532809257507324 31.263729095458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.693360686302185 3.606813907623291 37.761497497558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6838182210922241 3.6370303630828857 38.05412292480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6920442581176758 4.016702651977539 41.85906982421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7383309602737427 3.050974130630493 32.24807357788086
  batch 60 loss: 1.7383309602737427, 3.050974130630493, 32.24807357788086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6785929203033447 3.1277921199798584 32.95651626586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6974081993103027 3.218425750732422 33.88166427612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6742327213287354 3.1571786403656006 33.24602127075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6768735647201538 3.094329595565796 32.62017059326172
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.654433250427246 2.7605433464050293 29.259868621826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.677303433418274 3.4274020195007324 35.951324462890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6901606321334839 3.4143474102020264 35.83363342285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6876213550567627 2.9979376792907715 31.66699981689453
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6805347204208374 2.5979325771331787 27.659860610961914
Total LOSS train 34.81006730886606 valid 32.77795457839966
CE LOSS train 1.6700418692368728 valid 0.42013368010520935
Contrastive LOSS train 3.3140025248894323 valid 0.6494831442832947
EPOCH 30:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6971604824066162 3.263427257537842 34.3314323425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7075389623641968 3.9320321083068848 41.02785873413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6877448558807373 3.7315187454223633 39.0029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6942309141159058 3.2098591327667236 33.79281997680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7031186819076538 3.6971235275268555 38.674354553222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6769318580627441 3.0771725177764893 32.44865798950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993780136108398 4.17930793762207 43.49245834350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6879252195358276 3.129593849182129 32.983863830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6847401857376099 3.0590274333953857 32.27501678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7125617265701294 3.181532621383667 33.527889251708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6790363788604736 3.235891580581665 34.0379524230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6878328323364258 3.702533006668091 38.71316146850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6930952072143555 3.603614091873169 37.7292366027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70197331905365 3.587808132171631 37.580055236816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7405359745025635 3.193509340286255 33.675628662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.731460452079773 3.2575390338897705 34.30685043334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.69273841381073 3.701178789138794 38.704524993896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.728702187538147 3.2399580478668213 34.1282844543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7063391208648682 3.3113644123077393 34.819984436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.75640869140625 2.6532094478607178 28.288503646850586
  batch 20 loss: 1.75640869140625, 2.6532094478607178, 28.288503646850586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7382453680038452 2.858668327331543 30.324928283691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.743065357208252 3.5496978759765625 37.24004364013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7552989721298218 2.835925340652466 30.114551544189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7569894790649414 2.761817693710327 29.375167846679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772110104560852 4.096651554107666 42.738624572753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7737345695495605 3.6003756523132324 37.77749252319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7619813680648804 3.0208916664123535 31.970897674560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7706663608551025 3.0934641361236572 32.70530700683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7388380765914917 4.0515971183776855 42.25481033325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7610019445419312 3.8328449726104736 40.08945083618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688394546508789 3.3221592903137207 34.90998840332031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7167127132415771 3.1406667232513428 33.12337875366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702164888381958 3.839282274246216 40.09498596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6899913549423218 3.4309823513031006 35.999813079833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6464043855667114 3.310204267501831 34.74844741821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6512444019317627 3.6662003993988037 38.31324768066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445391178131104 3.8099310398101807 39.74385070800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692138910293579 3.2274885177612305 33.96702575683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68930184841156 3.458528995513916 36.27458953857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6970431804656982 4.153714656829834 43.23419189453125
  batch 40 loss: 1.6970431804656982, 4.153714656829834, 43.23419189453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6405029296875 3.2989182472229004 34.62968444824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6232527494430542 3.048513889312744 32.10839080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6049630641937256 3.601454734802246 37.619510650634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623390793800354 3.362586498260498 35.2492561340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5916985273361206 3.2598063945770264 34.189762115478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6279211044311523 3.0242714881896973 31.870635986328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6665304899215698 3.0865285396575928 32.53181457519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6042968034744263 3.5071051120758057 36.675350189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6749848127365112 3.742365837097168 39.0986442565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6139371395111084 4.393311500549316 45.54705047607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6512378454208374 3.3037781715393066 34.68901824951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6463474035263062 3.5823285579681396 37.46963119506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6154917478561401 3.4505648612976074 36.12113952636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6514893770217896 3.6271626949310303 37.92311477661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.605201244354248 3.833306074142456 39.938262939453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6688833236694336 3.0146989822387695 31.815872192382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.607477068901062 3.7903225421905518 39.51070022583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.594131350517273 3.061643123626709 32.21056365966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6084955930709839 3.464003086090088 36.2485237121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686124324798584 2.7578160762786865 29.264286041259766
  batch 60 loss: 1.686124324798584, 2.7578160762786865, 29.264286041259766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6031867265701294 3.7249913215637207 38.85309982299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6361150741577148 3.0094082355499268 31.73019790649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6077203750610352 2.9865946769714355 31.47366714477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6019047498703003 3.433756113052368 35.93946838378906
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5747156143188477 3.2507104873657227 34.081817626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614012360572815 2.9955222606658936 31.56923484802246
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62818443775177 3.31421160697937 34.770301818847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6257189512252808 2.9798471927642822 31.424190521240234
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.624993920326233 3.1068079471588135 32.69307327270508
Total LOSS train 35.7743341592642 valid 32.61420011520386
CE LOSS train 1.6782972409174992 valid 0.4062484800815582
Contrastive LOSS train 3.409603709440965 valid 0.7767019867897034
EPOCH 31:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6482285261154175 3.12849760055542 32.933204650878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6583603620529175 3.082385540008545 32.482215881347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6273863315582275 3.1198439598083496 32.825828552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6353487968444824 3.3483147621154785 35.11849594116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6535974740982056 3.511337995529175 36.7669792175293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6225144863128662 3.682779550552368 38.45030975341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6568933725357056 3.9079039096832275 40.73593521118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6306548118591309 3.454213857650757 36.17279052734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6189416646957397 3.9117958545684814 40.736900329589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6634682416915894 3.177304744720459 33.43651580810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6041090488433838 3.6380867958068848 37.98497772216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6157526969909668 3.8294029235839844 39.90978240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6122007369995117 3.5138661861419678 36.75086212158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6199710369110107 3.9716293811798096 41.336265563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807600259780884 3.318730354309082 34.86806106567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6717138290405273 3.6876895427703857 38.54861068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6160063743591309 3.7976090908050537 39.59209442138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.637261986732483 3.4884073734283447 36.52133560180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.615140438079834 3.451444387435913 36.129581451416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676741123199463 3.4032602310180664 35.70934295654297
  batch 20 loss: 1.676741123199463, 3.4032602310180664, 35.70934295654297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6515158414840698 3.426220655441284 35.91372299194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6401726007461548 3.730212688446045 38.942298889160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654428482055664 3.5059640407562256 36.71406555175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671887755393982 3.2369091510772705 34.040977478027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6935406923294067 3.1420252323150635 33.113792419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663562297821045 3.310704231262207 34.77060317993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.67252516746521 3.1812989711761475 33.48551559448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6844459772109985 3.0462403297424316 32.146846771240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645202875137329 3.087634325027466 32.52154541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.718766450881958 3.776923179626465 39.487998962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6446373462677002 3.4556069374084473 36.200706481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6950149536132812 3.515850305557251 36.853519439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6865794658660889 2.8764450550079346 30.45102882385254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6940284967422485 3.553128719329834 37.22531509399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659684181213379 3.0902369022369385 32.56205368041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6681212186813354 3.4679784774780273 36.34790802001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6738656759262085 3.8095571994781494 39.769439697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7128632068634033 3.2171413898468018 33.88427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7149622440338135 3.5798521041870117 37.513484954833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7231581211090088 3.6586503982543945 38.309661865234375
  batch 40 loss: 1.7231581211090088, 3.6586503982543945, 38.309661865234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6865788698196411 3.1645710468292236 33.33229064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.67633855342865 3.1168413162231445 32.84475326538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6667076349258423 3.056121587753296 32.22792434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664853811264038 4.0541582107543945 42.2064323425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6533372402191162 3.071662664413452 32.369964599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669743299484253 3.168654441833496 33.35628890991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6923325061798096 3.7510263919830322 39.202598571777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6514843702316284 2.9054720401763916 30.70620346069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7007877826690674 2.742675304412842 29.12754249572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6642382144927979 3.5803885459899902 37.46812438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679958462715149 3.896129846572876 40.641258239746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673301339149475 3.463087558746338 36.304176330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543797254562378 2.9658899307250977 31.313278198242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6840596199035645 3.6106832027435303 37.790889739990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6475940942764282 4.321568012237549 44.86327362060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6891289949417114 3.4368786811828613 36.05791473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643261194229126 4.017911434173584 41.8223762512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6322407722473145 3.8758933544158936 40.39117431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6371432542800903 3.6666018962860107 38.30316162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6900769472122192 3.4999241828918457 36.6893196105957
  batch 60 loss: 1.6900769472122192, 3.4999241828918457, 36.6893196105957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6234736442565918 4.091909408569336 42.54256820678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6502654552459717 3.2925868034362793 34.576133728027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6349526643753052 3.0038328170776367 31.673280715942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643053650856018 3.276575803756714 34.4088134765625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6129616498947144 2.983299970626831 31.445960998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642124056816101 3.001997947692871 31.6621036529541
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654128909111023 3.1822285652160645 33.47641372680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.650956630706787 2.908092975616455 30.73188591003418
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6531058549880981 2.9324350357055664 30.97745704650879
Total LOSS train 36.137393159132735 valid 31.711965084075928
CE LOSS train 1.6593272025768573 valid 0.41327646374702454
Contrastive LOSS train 3.4478065967559814 valid 0.7331087589263916
EPOCH 32:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.666812777519226 2.925031900405884 30.917131423950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6820201873779297 3.7360002994537354 39.042022705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6602065563201904 3.5466604232788086 37.12681198120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6607857942581177 3.2858734130859375 34.5195198059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6754390001296997 3.4256367683410645 35.93180465698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641919732093811 2.7603437900543213 29.245357513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6783349514007568 3.8519375324249268 40.19770812988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6627552509307861 2.7418739795684814 29.08149528503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6390745639801025 3.2446062564849854 34.085140228271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6783185005187988 3.1736693382263184 33.41501235961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629239797592163 3.197575330734253 33.6049919128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6335140466690063 4.0967302322387695 42.60081481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.628292202949524 3.8030073642730713 39.65836715698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6489847898483276 3.500793695449829 36.65692138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691627025604248 3.831360340118408 40.00522994995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6863079071044922 3.944878339767456 41.135093688964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.625168800354004 3.822765350341797 39.852821350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6570965051651 3.926002264022827 40.917118072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6329978704452515 3.815139055252075 39.78438949584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692761778831482 2.93354868888855 31.028249740600586
  batch 20 loss: 1.692761778831482, 2.93354868888855, 31.028249740600586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693693399429321 2.875471830368042 30.424087524414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6508572101593018 3.291008472442627 34.56093978881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6709955930709839 3.049222230911255 32.16321563720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6845941543579102 3.190349817276001 33.58809280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7001036405563354 3.762836456298828 39.328468322753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6624222993850708 3.635117530822754 38.01359558105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6654324531555176 3.565234661102295 37.317779541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669427752494812 3.664376735687256 38.313194274902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6320947408676147 3.935311794281006 40.98521423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7209656238555908 3.731393575668335 39.0349006652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6142557859420776 3.717045783996582 38.78470993041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6856350898742676 4.040170192718506 42.087337493896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6699262857437134 3.6193723678588867 37.863651275634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639530658721924 3.450711488723755 36.17106628417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6084892749786377 3.723224639892578 38.840736389160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6193227767944336 4.082150459289551 42.440826416015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6063979864120483 3.488142967224121 36.487831115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6806049346923828 3.7309303283691406 38.989906311035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70697820186615 4.085821628570557 42.56519317626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7230279445648193 3.730717897415161 39.03020477294922
  batch 40 loss: 1.7230279445648193, 3.730717897415161, 39.03020477294922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6515166759490967 3.353234052658081 35.18385696411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6312912702560425 3.314854383468628 34.77983474731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623578429222107 3.1632676124572754 33.25625228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6418092250823975 3.929955005645752 40.94135665893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6157786846160889 3.8984978199005127 40.60075759887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6263303756713867 3.393568754196167 35.56201934814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679785132408142 3.618272304534912 37.862510681152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6051195859909058 3.2932863235473633 34.53797912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.714239239692688 3.7228922843933105 38.94316482543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6359879970550537 3.592630624771118 37.562294006347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6718462705612183 3.7433841228485107 39.10568618774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6686867475509644 4.010338306427002 41.77206802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642114281654358 3.050379991531372 32.145912170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6789271831512451 3.1715147495269775 33.394073486328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6224339008331299 3.5500283241271973 37.12271499633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981819868087769 3.467526912689209 36.373451232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6396819353103638 3.243849754333496 34.078182220458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6182268857955933 3.935960531234741 40.97782897949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626006007194519 3.775651454925537 39.38252258300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6963083744049072 3.538815498352051 37.0844612121582
  batch 60 loss: 1.6963083744049072, 3.538815498352051, 37.0844612121582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.622872233390808 3.548632860183716 37.10919952392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466435194015503 3.3990399837493896 35.63704299926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.620295524597168 3.7939200401306152 39.55949783325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6024954319000244 3.7820703983306885 39.42320251464844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5783867835998535 4.345461845397949 45.03300857543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6102840900421143 3.554589033126831 37.15617370605469
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6273424625396729 2.8814008235931396 30.44135093688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6240949630737305 3.3693017959594727 35.31711196899414
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6324682235717773 2.9378409385681152 31.010879516601562
Total LOSS train 37.28039741516113 valid 33.48137903213501
CE LOSS train 1.6543854750119722 valid 0.40811705589294434
Contrastive LOSS train 3.562601217856774 valid 0.7344602346420288
EPOCH 33:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6500797271728516 3.3363564014434814 35.013641357421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672570824623108 3.2094342708587646 33.766910552978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6258732080459595 3.6265244483947754 37.891117095947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6188223361968994 2.9038102626800537 30.656925201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6442636251449585 3.560335636138916 37.24761962890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.588573694229126 3.2335078716278076 33.92365264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623892903327942 3.502089500427246 36.64479064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.628940224647522 3.6064422130584717 37.693363189697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5940903425216675 3.6668827533721924 38.262916564941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6357817649841309 3.164362668991089 33.2794075012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5575335025787354 3.6442034244537354 37.999568939208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5812174081802368 3.752579689025879 39.10701370239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5717132091522217 3.4903483390808105 36.475196838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5855870246887207 3.7861013412475586 39.44660186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6458431482315063 3.283245086669922 34.478294372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6394890546798706 3.388805866241455 35.52754592895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5553781986236572 3.7083919048309326 38.63929748535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5861924886703491 3.227071523666382 33.85690689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.587154746055603 3.903031826019287 40.61747360229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896806001663208 4.245152473449707 44.24120330810547
  batch 20 loss: 1.7896806001663208, 4.245152473449707, 44.24120330810547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6893774271011353 3.9690134525299072 41.379512786865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5825632810592651 4.058134078979492 42.163902282714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5817850828170776 4.224453449249268 43.826316833496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6033718585968018 3.528085947036743 36.88423156738281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6476789712905884 3.7135884761810303 38.78356170654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5871835947036743 3.3138339519500732 34.72552490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5978580713272095 3.837519407272339 39.973052978515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5965325832366943 3.4893667697906494 36.49020004272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5243111848831177 3.499462604522705 36.51893615722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6716980934143066 3.4619784355163574 36.29148483276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5349503755569458 3.2871510982513428 34.40645980834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6662776470184326 3.1403067111968994 33.06934356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.63958740234375 3.2598929405212402 34.23851776123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.602285385131836 3.8959360122680664 40.5616455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.585772156715393 3.506760597229004 36.653377532958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5796815156936646 3.386070966720581 35.440391540527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.610155701637268 2.949526071548462 31.10541534423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6805651187896729 3.6365699768066406 38.0462646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.680984377861023 3.2252793312072754 33.93377685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6849175691604614 3.376964807510376 35.454566955566406
  batch 40 loss: 1.6849175691604614, 3.376964807510376, 35.454566955566406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6508980989456177 3.1334762573242188 32.985660552978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6497710943222046 3.5660500526428223 37.310272216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6463731527328491 4.102251052856445 42.66888427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6478861570358276 3.8999650478363037 40.64753341674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5820821523666382 2.742380380630493 29.00588607788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6385642290115356 2.84830379486084 30.12160301208496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6899853944778442 3.2237370014190674 33.9273567199707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5806502103805542 3.25993013381958 34.17995071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7098913192749023 3.9361681938171387 41.07157516479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5803329944610596 3.3474957942962646 35.05529022216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6676156520843506 3.7960479259490967 39.62809371948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6474584341049194 3.0520553588867188 32.16801071166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6024256944656372 3.124187707901001 32.844303131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6762056350708008 3.850929021835327 40.18549346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5784835815429688 3.6999387741088867 38.57787322998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6882681846618652 3.63607120513916 38.04897689819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5853699445724487 3.427671194076538 35.86207962036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5423730611801147 3.5045673847198486 36.58804702758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5828160047531128 3.5976312160491943 37.55912780761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6926097869873047 3.4455087184906006 36.14769744873047
  batch 60 loss: 1.6926097869873047, 3.4455087184906006, 36.14769744873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5288329124450684 3.5366668701171875 36.89550018310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.577804684638977 4.108631610870361 42.664119720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5161807537078857 3.5843210220336914 37.35939025878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5086368322372437 3.9730257987976074 41.238895416259766
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4463797807693481 3.2131712436676025 33.578094482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5100563764572144 3.175553798675537 33.265594482421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5466331243515015 3.3639421463012695 35.18605422973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5326613187789917 3.0246665477752686 31.779327392578125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5391191244125366 3.0323753356933594 31.862873077392578
Total LOSS train 36.785163791363054 valid 33.02346229553223
CE LOSS train 1.614586325792166 valid 0.38477978110313416
Contrastive LOSS train 3.517057774617122 valid 0.7580938339233398
EPOCH 34:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5702894926071167 3.213231325149536 33.70260238647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.595354676246643 3.5685007572174072 37.280364990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5385547876358032 3.2177884578704834 33.71643829345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5465984344482422 3.731940507888794 38.866004943847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5737289190292358 3.5200281143188477 36.774009704589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4950982332229614 3.370145320892334 35.19655227661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5629080533981323 3.236370086669922 33.92660903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5241178274154663 2.770143747329712 29.225555419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5142472982406616 3.107395648956299 32.58820343017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5891345739364624 3.88777494430542 40.46688461303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5012160539627075 3.263294219970703 34.134159088134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.504044771194458 3.561584949493408 37.11989212036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.483304500579834 3.398338556289673 35.46669006347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5229233503341675 3.8135528564453125 39.658451080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6175323724746704 3.523359537124634 36.85112762451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.616572380065918 4.159026622772217 43.20684051513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5200992822647095 3.237943410873413 33.899532318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5914041996002197 3.055344343185425 32.14484786987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.534501314163208 3.7703287601470947 39.237789154052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646548867225647 3.163940191268921 33.28594970703125
  batch 20 loss: 1.646548867225647, 3.163940191268921, 33.28594970703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5673680305480957 3.147388219833374 33.04125213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.512353539466858 3.6900134086608887 38.4124870300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5277060270309448 3.549370527267456 37.02141189575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5564029216766357 3.260993003845215 34.16633605957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606487512588501 3.529219627380371 36.898685455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5361109972000122 4.077179431915283 42.30790328979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5404961109161377 3.5725302696228027 37.26580047607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5297740697860718 3.1785237789154053 33.31501388549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4430296421051025 3.841564178466797 39.858673095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.599096417427063 3.3205137252807617 34.804237365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4432878494262695 3.59739089012146 37.41719436645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5693267583847046 3.982539653778076 41.39472579956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5042150020599365 3.073948383331299 32.24369812011719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5158226490020752 4.060254096984863 42.11836242675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4247716665267944 2.91853404045105 30.6101131439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.474904179573059 3.9229369163513184 40.70427322387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4583377838134766 2.8406717777252197 29.865055084228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.586853265762329 3.077791690826416 32.364768981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6206300258636475 3.3012773990631104 34.63340377807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6279069185256958 3.2919440269470215 34.54734802246094
  batch 40 loss: 1.6279069185256958, 3.2919440269470215, 34.54734802246094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5559982061386108 3.996314287185669 41.519142150878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5280834436416626 3.7435829639434814 38.96391296386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4735221862792969 3.9795076847076416 41.26860046386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5028257369995117 3.2600255012512207 34.10308074951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4726662635803223 2.5162453651428223 26.63511848449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5306123495101929 3.212113857269287 33.65175247192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.601025104522705 3.09867525100708 32.58777618408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5063366889953613 3.6535141468048096 38.04147720336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6366472244262695 3.8897881507873535 40.53452682495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.51227867603302 3.3102622032165527 34.61490249633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5678642988204956 3.5374016761779785 36.94187927246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5697871446609497 3.782759428024292 39.39738082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5237820148468018 3.6882941722869873 38.40672302246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6064262390136719 4.055446624755859 42.160892486572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.53229558467865 4.034852027893066 41.88081359863281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6342799663543701 3.69045090675354 38.53879165649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5234016180038452 3.313002824783325 34.6534309387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5036625862121582 3.486248731613159 36.36614990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.525664210319519 3.7365565299987793 38.891231536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6478677988052368 3.605384349822998 37.70171356201172
  batch 60 loss: 1.6478677988052368, 3.605384349822998, 37.70171356201172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5151423215866089 3.409074306488037 35.60588455200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5604430437088013 3.378082752227783 35.341270446777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5263372659683228 3.8539624214172363 40.06595993041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5129271745681763 4.412027359008789 45.633201599121094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.466262698173523 3.02763295173645 31.742591857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5279767513275146 3.258527994155884 34.113258361816406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5585964918136597 3.5373475551605225 36.932071685791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.54863703250885 2.8751838207244873 30.30047607421875
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.56012761592865 3.2485415935516357 34.0455436706543
Total LOSS train 36.5382685147799 valid 33.84783744812012
CE LOSS train 1.5419877015627348 valid 0.3900319039821625
Contrastive LOSS train 3.4996280596806453 valid 0.8121353983879089
EPOCH 35:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.583338737487793 3.084738254547119 32.430721282958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6156708002090454 3.5139527320861816 36.75519561767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.563839316368103 3.163973331451416 33.20357131958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5788363218307495 3.138481616973877 32.96364974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6038644313812256 3.658618688583374 38.19004821777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5355942249298096 3.1984708309173584 33.520301818847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.605484962463379 3.4971344470977783 36.57682800292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5601340532302856 3.5825307369232178 37.385440826416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5717682838439941 3.251262903213501 34.08440017700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6247320175170898 2.788586378097534 29.510597229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5377917289733887 3.0348963737487793 31.886756896972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5715184211730957 3.276074171066284 34.33226013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5574291944503784 2.9990477561950684 31.54790687561035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.582863688468933 3.3993401527404785 35.576263427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6511849164962769 3.945591449737549 41.10709762573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6808832883834839 3.614284038543701 37.82372283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.584119439125061 2.9755733013153076 31.339853286743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62848699092865 3.2266721725463867 33.89521026611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5810434818267822 3.374920606613159 35.33024978637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6834008693695068 3.636296272277832 38.04636001586914
  batch 20 loss: 1.6834008693695068, 3.636296272277832, 38.04636001586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623852014541626 3.2019453048706055 33.643306732177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5968079566955566 3.634787082672119 37.944679260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6076228618621826 3.4575729370117188 36.183353424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6230804920196533 3.39874267578125 35.61050796508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.653483510017395 3.464951992034912 36.30300521850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5956807136535645 4.229870319366455 43.89438247680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5988129377365112 3.8705873489379883 40.304683685302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5786935091018677 3.635464906692505 37.93334197998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.492112636566162 3.507385015487671 36.56595993041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6300853490829468 3.918407678604126 40.81416320800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4991014003753662 3.6908018589019775 38.40711975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6104921102523804 3.4005253314971924 35.615745544433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.579266905784607 2.8528971672058105 30.108240127563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6262933015823364 3.425076484680176 35.87705612182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5382155179977417 3.3205459117889404 34.743675231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5605289936065674 3.322474956512451 34.7852783203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.582036018371582 3.5741164684295654 37.32320022583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6486878395080566 3.0811779499053955 32.46046829223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661912202835083 3.5163774490356445 36.82568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.674708366394043 3.879288673400879 40.467594146728516
  batch 40 loss: 1.674708366394043, 3.879288673400879, 40.467594146728516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5930322408676147 3.2015366554260254 33.6083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5884006023406982 3.477787733078003 36.36627960205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5834251642227173 3.9320175647735596 40.90359878540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5877602100372314 3.356985569000244 35.157615661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5552477836608887 3.463841676712036 36.19366455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5914734601974487 3.5400984287261963 36.99245834350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6343529224395752 2.919480562210083 30.829158782958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5613101720809937 3.3834869861602783 35.39617919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6395398378372192 3.3618223667144775 35.25776672363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5635532140731812 3.5996475219726562 37.560028076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6095062494277954 3.1234357357025146 32.84386444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6189346313476562 3.1309914588928223 32.92884826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5832492113113403 2.49599552154541 26.54320526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6320531368255615 3.205461025238037 33.68666458129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.569547176361084 3.4894955158233643 36.464500427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6604502201080322 3.0357115268707275 32.0175666809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.577539086341858 3.871680498123169 40.294342041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5496000051498413 3.9583256244659424 41.13285446166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5763487815856934 4.0131940841674805 41.708290100097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629284620285034 3.252612829208374 34.189056396484375
  batch 60 loss: 1.6629284620285034, 3.252612829208374, 34.189056396484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5366318225860596 3.0563931465148926 32.100563049316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5807912349700928 3.1051785945892334 32.63257598876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5443702936172485 3.163372039794922 33.1780891418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5447858572006226 3.3157830238342285 34.702613830566406
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.510646939277649 2.74040150642395 28.914661407470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5640674829483032 3.2230794429779053 33.79486083984375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5853173732757568 3.2762527465820312 34.347843170166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.57949960231781 3.5140178203582764 36.71967697143555
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5856921672821045 3.3323395252227783 34.909088134765625
Total LOSS train 35.42954961336576 valid 34.942867279052734
CE LOSS train 1.592906746497521 valid 0.3964230418205261
Contrastive LOSS train 3.3836643218994142 valid 0.8330848813056946
EPOCH 36:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6049529314041138 3.423882246017456 35.84377670288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6279343366622925 3.44970703125 36.125003814697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5864120721817017 3.82729434967041 39.859352111816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606704831123352 3.7424540519714355 39.031246185302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6341931819915771 4.1069769859313965 42.70396423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5799424648284912 4.189203262329102 43.47197341918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.63825261592865 3.5955920219421387 37.594173431396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5982544422149658 3.612963914871216 37.7278938293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5816338062286377 3.823843240737915 39.820068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6183788776397705 3.8191683292388916 39.810062408447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5546830892562866 3.884854793548584 40.40323257446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5503628253936768 3.854311943054199 40.093482971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5533255338668823 4.044885158538818 42.00217819213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.560420036315918 3.7611186504364014 39.171607971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6477761268615723 3.670239210128784 38.35017013549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629145622253418 3.8197715282440186 39.82686233520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5408620834350586 3.8686013221740723 40.22687530517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6053824424743652 3.152980327606201 33.13518524169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5722657442092896 3.5041613578796387 36.6138801574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6497244834899902 3.301419496536255 34.663917541503906
  batch 20 loss: 1.6497244834899902, 3.301419496536255, 34.663917541503906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6118099689483643 3.2022411823272705 33.634220123291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5600868463516235 3.3877878189086914 35.43796157836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5872790813446045 3.060774326324463 32.19502258300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6165721416473389 3.0501837730407715 32.118412017822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445086002349854 3.3102011680603027 34.74652099609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5953665971755981 3.4418280124664307 36.013648986816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6010748147964478 3.5028138160705566 36.62921142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5863182544708252 3.4880969524383545 36.467288970947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.516964316368103 3.6916396617889404 38.4333610534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6470839977264404 3.675384521484375 38.40092849731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5175061225891113 3.9120023250579834 40.63752746582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6216862201690674 3.796501874923706 39.58670425415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.582040548324585 3.3669726848602295 35.251766204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5887372493743896 3.8203399181365967 39.792137145996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.523607611656189 3.612697124481201 37.65058135986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5415767431259155 3.630965232849121 37.85123062133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5254011154174805 2.933150053024292 30.856903076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6255255937576294 3.107642650604248 32.70195388793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6401621103286743 3.279372215270996 34.43388748168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6369690895080566 3.9451684951782227 41.088653564453125
  batch 40 loss: 1.6369690895080566, 3.9451684951782227, 41.088653564453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5834195613861084 3.5311906337738037 36.89532470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5687415599822998 3.495636463165283 36.52510452270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5351927280426025 3.217498540878296 33.71017837524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5660938024520874 3.6704089641571045 38.27018356323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5202465057373047 3.4446496963500977 35.96674346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5597283840179443 3.728804349899292 38.84777069091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6161147356033325 3.4980766773223877 36.59688186645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5389914512634277 3.4369399547576904 35.908390045166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.653234601020813 3.336286783218384 35.01610565185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5431222915649414 3.728492021560669 38.828041076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5787004232406616 4.212050914764404 43.69921112060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5756593942642212 3.5669000148773193 37.244659423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5305051803588867 3.71690034866333 38.69950866699219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5846445560455322 3.737813711166382 38.9627799987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4960458278656006 3.8265833854675293 39.761878967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.61017906665802 3.3562769889831543 35.172950744628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4982272386550903 3.351680278778076 35.01502990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4764715433120728 3.344776153564453 34.924232482910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5055488348007202 4.035647869110107 41.862030029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6221948862075806 3.381564140319824 35.43783950805664
  batch 60 loss: 1.6221948862075806, 3.381564140319824, 35.43783950805664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4609001874923706 4.156875133514404 43.0296516418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.522185206413269 3.0285723209381104 31.807907104492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4802067279815674 3.3378775119781494 34.85898208618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4786579608917236 3.3149561882019043 34.62821960449219
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.4517396688461304 3.707657814025879 38.528316497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4932334423065186 3.0636696815490723 32.12992858886719
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5265274047851562 2.9058237075805664 30.58476448059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5120234489440918 3.1420130729675293 32.93215560913086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.5234438180923462 2.732429027557373 28.847734451293945
Total LOSS train 37.39385769183819 valid 31.123645782470703
CE LOSS train 1.5733482599258424 valid 0.38086095452308655
Contrastive LOSS train 3.582050921366765 valid 0.6831072568893433
EPOCH 37:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5547007322311401 3.0133814811706543 31.68851661682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.58140230178833 3.201488971710205 33.596290588378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5401434898376465 3.6283693313598633 37.82383346557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5565288066864014 3.4923033714294434 36.47956466674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.603061556816101 4.26321268081665 44.23518753051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5130105018615723 2.799970865249634 29.512718200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5935890674591064 3.7403388023376465 38.996978759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5522750616073608 3.6342031955718994 37.894309997558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5285612344741821 3.1042683124542236 32.57124328613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6133840084075928 2.887336254119873 30.48674774169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5382956266403198 3.5105631351470947 36.64392852783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5418150424957275 3.835118055343628 39.89299774169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.52069091796875 3.2679250240325928 34.1999397277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5355522632598877 3.3969666957855225 35.505218505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6312025785446167 3.5431485176086426 37.062686920166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6271454095840454 3.5181171894073486 36.80831527709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5225709676742554 3.6655995845794678 38.178565979003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5611039400100708 4.024865627288818 41.80976104736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5171109437942505 4.024601936340332 41.76313018798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6228939294815063 3.2674033641815186 34.29692840576172
  batch 20 loss: 1.6228939294815063, 3.2674033641815186, 34.29692840576172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5653603076934814 3.236133098602295 33.926692962646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.522929310798645 3.22662091255188 33.78913879394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5582629442214966 2.9548792839050293 31.107057571411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.577962040901184 4.331236362457275 44.890323638916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6124024391174316 3.844589948654175 40.05830383300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5596284866333008 3.456146001815796 36.12108612060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5719552040100098 3.6661338806152344 38.23329544067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5653948783874512 3.5655386447906494 37.22078323364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4956493377685547 3.615877866744995 37.65442657470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.622609257698059 3.943673610687256 41.05934524536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4963258504867554 3.1508030891418457 33.004356384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.597653865814209 3.847764253616333 40.075294494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5764669179916382 3.494483232498169 36.521297454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5772958993911743 3.4428188800811768 36.00548553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5260682106018066 3.520273447036743 36.72880554199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5562763214111328 3.750037670135498 39.05665588378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5537776947021484 3.1328344345092773 32.88212203979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6308013200759888 2.7489140033721924 29.11994171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6376605033874512 3.048034191131592 32.118003845214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6562387943267822 2.845215082168579 30.108388900756836
  batch 40 loss: 1.6562387943267822, 2.845215082168579, 30.108388900756836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5964676141738892 2.967928409576416 31.2757511138916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5796446800231934 3.493403196334839 36.513675689697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.567605972290039 3.0215065479278564 31.782670974731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.591395616531372 3.7373857498168945 38.96525192260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5706844329833984 3.339561939239502 34.96630096435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.612586498260498 3.978358268737793 41.39617156982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6496610641479492 3.279208183288574 34.44174575805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5916221141815186 3.2144575119018555 33.73619842529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6754226684570312 3.753288984298706 39.20831298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6068800687789917 3.2564666271209717 34.171546936035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6459299325942993 3.0104551315307617 31.75048065185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6381800174713135 3.063711404800415 32.27529525756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.617252230644226 3.04483962059021 32.06564712524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494641304016113 3.4726274013519287 36.375736236572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6104819774627686 3.7387847900390625 38.998329162597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6791678667068481 3.0516486167907715 32.195655822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6115013360977173 3.233894109725952 33.950439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5937117338180542 3.5867276191711426 37.46098709106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6029682159423828 3.171374559402466 33.31671142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6802911758422852 3.3165228366851807 34.84552001953125
  batch 60 loss: 1.6802911758422852, 3.3165228366851807, 34.84552001953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6029386520385742 3.689051389694214 38.49345397949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6379555463790894 3.190319538116455 33.5411491394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6140246391296387 3.7010698318481445 38.62472152709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6166127920150757 2.8571648597717285 30.188261032104492
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5961872339248657 2.359457492828369 25.19076156616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6253187656402588 3.200807809829712 33.633399963378906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645193099975586 3.14569091796875 33.10210418701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6350096464157104 3.477665662765503 36.41166687011719
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6389433145523071 2.88498854637146 30.488828659057617
Total LOSS train 35.6132068340595 valid 33.40899991989136
CE LOSS train 1.586990710405203 valid 0.4097358286380768
Contrastive LOSS train 3.4026216140160193 valid 0.721247136592865
EPOCH 38:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6554286479949951 3.7949986457824707 39.60541534423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664844274520874 3.2719907760620117 34.3847541809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6384985446929932 3.018033504486084 31.81883430480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6429682970046997 3.8869333267211914 40.512298583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6585111618041992 3.3959052562713623 35.6175651550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624810814857483 3.517451763153076 36.7993278503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6619844436645508 3.7246687412261963 38.90867233276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6311352252960205 3.1333167552948 32.96430206298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.620137095451355 2.899719476699829 30.61733055114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6590543985366821 3.426954746246338 35.9286003112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6060209274291992 2.724536180496216 28.851383209228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6242563724517822 3.262420654296875 34.24846267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.612652063369751 3.706404209136963 38.676692962646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6121000051498413 3.8325133323669434 39.937232971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6710774898529053 3.2091586589813232 33.762664794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693477630615234 3.334156036376953 35.01091003417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6066974401474 3.519124746322632 36.797943115234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6395334005355835 3.429065704345703 35.93019104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5979036092758179 3.6427366733551025 38.025272369384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6712626218795776 3.0511090755462646 32.18235397338867
  batch 20 loss: 1.6712626218795776, 3.0511090755462646, 32.18235397338867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6270134449005127 3.4910285472869873 36.53730010986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5759272575378418 3.3600120544433594 35.176048278808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6061598062515259 3.0956003665924072 32.562164306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6332249641418457 3.2045412063598633 33.67863464355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6679967641830444 3.4369451999664307 36.03744888305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6212087869644165 3.6363563537597656 37.984771728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6349596977233887 4.005522727966309 41.690189361572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6243270635604858 3.551995277404785 37.14427947998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.577232003211975 3.25270414352417 34.10427474975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6669050455093384 3.3619635105133057 35.28654098510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.58238685131073 3.7550501823425293 39.13288879394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.651558518409729 3.780789852142334 39.45945739746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6194287538528442 3.239950656890869 34.01893615722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6212176084518433 3.8619842529296875 40.241058349609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5808498859405518 3.443904399871826 36.019893646240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5998841524124146 3.472719669342041 36.32707977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.594342589378357 3.6675522327423096 38.26986312866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661881446838379 3.3681793212890625 35.34367370605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6673061847686768 3.390606164932251 35.573368072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6782044172286987 3.68792986869812 38.55750274658203
  batch 40 loss: 1.6782044172286987, 3.68792986869812, 38.55750274658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629475712776184 3.3463375568389893 35.09284973144531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6214014291763306 3.269195795059204 34.31336212158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6064475774765015 3.3249027729034424 34.85547637939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.615393042564392 3.386766195297241 35.483055114746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5919052362442017 3.0807909965515137 32.39981460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6219711303710938 2.7829737663269043 29.451709747314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6605702638626099 3.27282452583313 34.388816833496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6026231050491333 2.544405698776245 27.046680450439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6822075843811035 3.5019044876098633 36.70125198364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6136386394500732 3.362718105316162 35.240821838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.64780592918396 3.3275551795959473 34.92335510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.638864517211914 3.5907952785491943 37.54681396484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6192283630371094 2.8342621326446533 29.961849212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6542608737945557 3.739138603210449 39.045650482177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6097207069396973 3.5861456394195557 37.4711799621582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.680269479751587 3.0180976390838623 31.86124610900879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614821434020996 3.208570957183838 33.700531005859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5963881015777588 4.160006999969482 43.19646072387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.602541208267212 3.259340524673462 34.19594955444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6722077131271362 3.580629587173462 37.4785041809082
  batch 60 loss: 1.6722077131271362, 3.580629587173462, 37.4785041809082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5860117673873901 3.6954338550567627 38.540348052978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6185158491134644 3.245826482772827 34.076778411865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6106871366500854 4.444633483886719 46.05702209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6001979112625122 2.9104955196380615 30.705154418945312
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5797390937805176 2.6742162704467773 28.321901321411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6188194751739502 3.532864570617676 36.94746398925781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6317322254180908 3.627990961074829 37.911643981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6272194385528564 3.311724901199341 34.744468688964844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6351375579833984 3.4435322284698486 36.070457458496094
Total LOSS train 35.6274179311899 valid 36.418508529663086
CE LOSS train 1.6282635945540207 valid 0.4087843894958496
Contrastive LOSS train 3.399915420092069 valid 0.8608830571174622
EPOCH 39:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6496676206588745 3.1843762397766113 33.49342727661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6666369438171387 3.51987361907959 36.86537551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.638566017150879 3.100135087966919 32.639915466308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6429963111877441 3.0884904861450195 32.52790069580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660874605178833 3.2872636318206787 34.53350830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6215507984161377 2.920539140701294 30.826942443847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629294157028198 3.3467063903808594 35.1299934387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6427505016326904 3.5315725803375244 36.95847702026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6337692737579346 3.4478938579559326 36.112709045410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6674363613128662 3.34016752243042 35.06911087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6295539140701294 3.0138041973114014 31.767597198486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633190631866455 3.038735866546631 32.02054977416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6339255571365356 3.2680928707122803 34.31485366821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6490178108215332 3.005798816680908 31.70700454711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6911123991012573 3.5775771141052246 37.46688461303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6825010776519775 3.603858470916748 37.72108840942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6340755224227905 3.5465328693389893 37.099403381347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6587940454483032 3.8880183696746826 40.538978576660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644858717918396 3.6407461166381836 38.05232238769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6942850351333618 3.0512094497680664 32.20637893676758
  batch 20 loss: 1.6942850351333618, 3.0512094497680664, 32.20637893676758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.680165410041809 3.060338258743286 32.283546447753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6577476263046265 2.8609681129455566 30.267427444458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679557204246521 3.139702558517456 33.07658386230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6944496631622314 3.1517107486724854 33.21155548095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.719260811805725 3.3840363025665283 35.55962371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6988914012908936 3.4913251399993896 36.61214065551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7076513767242432 3.0246686935424805 31.95433807373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981346607208252 3.1558337211608887 33.256473541259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6549352407455444 3.806520938873291 39.72014236450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7303688526153564 3.3158910274505615 34.889278411865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6678903102874756 3.1819427013397217 33.4873161315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7117897272109985 3.1747286319732666 33.459075927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694348931312561 3.072810649871826 32.422454833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6927417516708374 3.0433616638183594 32.12635803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6444629430770874 3.7657530307769775 39.30199432373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6748048067092896 3.5217530727386475 36.892333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6747822761535645 3.4269344806671143 35.94412612915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7166508436203003 2.9063560962677 30.780210494995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.726296067237854 3.1252150535583496 32.97844696044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7391912937164307 3.192943811416626 33.66862869262695
  batch 40 loss: 1.7391912937164307, 3.192943811416626, 33.66862869262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7077100276947021 3.3247485160827637 34.955196380615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6939005851745605 2.801865577697754 29.712556838989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6861753463745117 2.650315284729004 28.189327239990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6943063735961914 3.6795055866241455 38.48936080932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6910784244537354 3.152493476867676 33.21601486206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7108078002929688 2.90915584564209 30.802366256713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7304635047912598 2.715010166168213 28.880563735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6976912021636963 3.3308732509613037 35.00642395019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.743605375289917 3.7391693592071533 39.13529968261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7037198543548584 3.522536277770996 36.92908477783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7296732664108276 3.764463186264038 39.37430191040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7286677360534668 3.742233991622925 39.15100860595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7273112535476685 3.2145140171051025 33.87245178222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7618998289108276 3.5358870029449463 37.12076950073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7379933595657349 4.005693435668945 41.79492950439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7728875875473022 3.421149969100952 35.9843864440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.739453911781311 3.9798996448516846 41.538448333740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7373870611190796 3.7769224643707275 39.506614685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.728548526763916 3.43808650970459 36.10941696166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.786527156829834 3.890270233154297 40.68922805786133
  batch 60 loss: 1.786527156829834, 3.890270233154297, 40.68922805786133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7338056564331055 3.6789486408233643 38.523292541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7565218210220337 3.271145820617676 34.467979431152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.757472038269043 3.4617743492126465 36.37521743774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908275127410889 3.7130579948425293 38.921409606933594
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7706329822540283 3.210301399230957 33.8736457824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7342567443847656 3.2646450996398926 34.380706787109375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.748824119567871 3.266631841659546 34.41514205932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.739619493484497 3.4231371879577637 35.97099304199219
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7409440279006958 2.8962905406951904 30.70384979248047
Total LOSS train 35.100981110792894 valid 33.86767292022705
CE LOSS train 1.6957181838842539 valid 0.43523600697517395
Contrastive LOSS train 3.34052629837623 valid 0.7240726351737976
EPOCH 40:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7460002899169922 2.8816421031951904 30.562421798706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.736920952796936 3.555774211883545 37.29466247558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7028824090957642 3.4326694011688232 36.02957534790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6800687313079834 3.275362730026245 34.43369674682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6760483980178833 2.8803741931915283 30.47978973388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6223160028457642 2.8884940147399902 30.50725746154785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6487417221069336 3.24112606048584 34.06000518798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6128207445144653 3.1912333965301514 33.52515411376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5920495986938477 2.788123607635498 29.473285675048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6296547651290894 3.0500357151031494 32.13001251220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5543850660324097 3.3552322387695312 35.10670852661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5472352504730225 3.2523348331451416 34.07058334350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.511994481086731 3.954352855682373 41.05552291870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5291439294815063 4.027638912200928 41.80553436279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6129320859909058 3.4867136478424072 36.48006820678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.60202956199646 3.7955172061920166 39.55720138549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4974302053451538 4.09572696685791 42.45469665527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.541326880455017 3.9117014408111572 40.65834426879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4924192428588867 3.2189762592315674 33.68218231201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.594124436378479 3.52840518951416 36.878173828125
  batch 20 loss: 1.594124436378479, 3.52840518951416, 36.878173828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5316929817199707 3.689171314239502 38.423404693603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4873559474945068 3.757338285446167 39.06073760986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.518180251121521 3.5030910968780518 36.54909133911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5480002164840698 4.214703559875488 43.69503402709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.598858118057251 4.206547737121582 43.66433334350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5341724157333374 3.853533983230591 40.06951141357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5519514083862305 3.7342405319213867 38.89435958862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.532397985458374 3.414708137512207 35.67947769165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4540212154388428 3.4763362407684326 36.217384338378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6029384136199951 4.20906400680542 43.693580627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4494638442993164 3.07000732421875 32.1495361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5625479221343994 3.3377299308776855 34.939849853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5323359966278076 3.602820873260498 37.560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5261589288711548 3.7038605213165283 38.564762115478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4728872776031494 3.8608903884887695 40.081790924072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4972330331802368 4.23641300201416 43.86136245727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.4917702674865723 3.0604450702667236 32.096221923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5958294868469238 2.9619035720825195 31.21486473083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.622725486755371 3.524508237838745 36.8678092956543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6436635255813599 3.665949821472168 38.303165435791016
  batch 40 loss: 1.6436635255813599, 3.665949821472168, 38.303165435791016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5663025379180908 3.5712902545928955 37.279205322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5495038032531738 3.972106695175171 41.27056884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.517573356628418 3.4066896438598633 35.584468841552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5429880619049072 3.5504214763641357 37.047203063964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5078257322311401 3.332911729812622 34.83694076538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.564688801765442 3.3319251537323 34.883941650390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.616210699081421 3.4489779472351074 36.10599136352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5461972951889038 3.349560260772705 35.04179763793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466193199157715 3.4098081588745117 35.74470138549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5629305839538574 2.2905423641204834 24.468355178833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6166120767593384 3.479491710662842 36.411529541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6098859310150146 3.9206011295318604 40.81589889526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5847910642623901 4.309804439544678 44.68283462524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6222018003463745 3.5757791996002197 37.3799934387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5708987712860107 3.175530195236206 33.326202392578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6574362516403198 2.965864658355713 31.316082000732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5800832509994507 3.647799015045166 38.05807113647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5757335424423218 4.08722448348999 42.44797897338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5980689525604248 3.361128568649292 35.209354400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6773626804351807 3.1501355171203613 33.17871856689453
  batch 60 loss: 1.6773626804351807, 3.1501355171203613, 33.17871856689453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5815428495407104 3.3149096965789795 34.73064041137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6188219785690308 3.0053813457489014 31.672636032104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5965161323547363 3.08341646194458 32.43067932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5964031219482422 3.680434226989746 38.40074920654297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5605387687683105 3.1352016925811768 32.91255569458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.604383111000061 3.517892360687256 36.78330612182617
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621701717376709 3.393069267272949 35.55239486694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6157910823822021 3.857996702194214 40.19575881958008
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6227861642837524 2.9542577266693115 31.165363311767578
Total LOSS train 36.41644307650053 valid 35.9242057800293
CE LOSS train 1.5777299514183631 valid 0.4056965410709381
Contrastive LOSS train 3.4838713022378776 valid 0.7385644316673279
EPOCH 41:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.639112114906311 3.4851882457733154 36.49099349975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6569392681121826 3.7869937419891357 39.526878356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.616451382637024 3.7861132621765137 39.47758483886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6192553043365479 3.4233596324920654 35.85285186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6415314674377441 2.8587417602539062 30.22894859313965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6070538759231567 2.4294674396514893 25.9017276763916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654676914215088 2.793884515762329 29.593521118164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633728265762329 2.987992525100708 31.513652801513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6266758441925049 2.5704190731048584 27.330867767333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776789426803589 2.9956421852111816 31.63409996032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.625551700592041 3.0429022312164307 32.05457305908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6272727251052856 3.490905284881592 36.53632736206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6263560056686401 2.9998323917388916 31.624679565429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6341378688812256 3.619544506072998 37.82958221435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6766473054885864 3.929126501083374 40.96791076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661784291267395 3.03381609916687 31.99994659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6113488674163818 3.455739736557007 36.16874313354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6455312967300415 3.6542739868164062 38.188270568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.609661340713501 3.5362510681152344 36.972171783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6696840524673462 3.5960593223571777 37.630279541015625
  batch 20 loss: 1.6696840524673462, 3.5960593223571777, 37.630279541015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641509771347046 2.5360500812530518 27.002010345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6183099746704102 3.6710708141326904 38.329017639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6286869049072266 3.244675397872925 34.075439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6478508710861206 3.5126569271087646 36.774417877197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6754518747329712 3.868494987487793 40.36040496826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6375322341918945 4.095978736877441 42.59731674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6476891040802002 3.632636070251465 37.97405242919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6430743932724 3.0118305683135986 31.76137924194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5885226726531982 3.6232664585113525 37.821189880371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6692488193511963 3.789214611053467 39.561397552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.583713173866272 3.192809581756592 33.511810302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6568466424942017 3.7114601135253906 38.771446228027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6259117126464844 3.271376848220825 34.33967971801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6251533031463623 3.467411994934082 36.29927062988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5857845544815063 3.2673423290252686 34.25920867919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6048595905303955 2.824263095855713 29.847490310668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6039267778396606 2.633476734161377 27.93869400024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6652824878692627 2.654200315475464 28.207286834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6726106405258179 3.3851444721221924 35.52405548095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6810842752456665 3.347729206085205 35.15837478637695
  batch 40 loss: 1.6810842752456665, 3.347729206085205, 35.15837478637695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6406290531158447 3.2774572372436523 34.41520309448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6301640272140503 3.206275224685669 33.69291687011719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6107268333435059 3.637075901031494 37.981483459472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.619689702987671 3.86075758934021 40.227264404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.600111961364746 3.2859396934509277 34.459510803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6300431489944458 3.0265328884124756 31.89537239074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6694751977920532 3.5994575023651123 37.6640510559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6183527708053589 3.195552349090576 33.573875427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6904644966125488 3.309018611907959 34.7806510925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6237596273422241 2.8196768760681152 29.82052993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6599059104919434 3.6292309761047363 37.952213287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6536331176757812 3.356680154800415 35.220436096191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6357970237731934 3.2094805240631104 33.7306022644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6677591800689697 3.296638250350952 34.63414001464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6285784244537354 3.4339263439178467 35.96784210205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6912627220153809 3.370170831680298 35.39297103881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6400388479232788 3.5206236839294434 36.846275329589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6243512630462646 3.841458320617676 40.03893280029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6288479566574097 3.4426474571228027 36.05532455444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6839735507965088 3.1846721172332764 33.530696868896484
  batch 60 loss: 1.6839735507965088, 3.1846721172332764, 33.530696868896484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6319791078567505 3.4436380863189697 36.068363189697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6425389051437378 3.477935314178467 36.42189407348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6226824522018433 3.1182825565338135 32.80550765991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626615285873413 3.037224769592285 31.998863220214844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.610384464263916 2.7441787719726562 29.05217170715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6475142240524292 3.0717952251434326 32.3654670715332
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6641027927398682 3.270887613296509 34.372982025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6570839881896973 2.918604850769043 30.84313201904297
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.656935214996338 3.202974796295166 33.686683654785156
Total LOSS train 34.79791764479417 valid 32.81706619262695
CE LOSS train 1.6376291330044086 valid 0.4142338037490845
Contrastive LOSS train 3.3160288443932164 valid 0.8007436990737915
EPOCH 42:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6703625917434692 3.1680448055267334 33.35081100463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6818516254425049 3.7506728172302246 39.18857955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543971300125122 3.381622552871704 35.47062301635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6605443954467773 3.034163475036621 32.00217819213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673789620399475 2.991532802581787 31.58911895751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6464173793792725 3.55665922164917 37.213008880615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686698317527771 3.2375009059906006 34.06170654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663442611694336 2.8611040115356445 30.27448272705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.653451919555664 2.757941722869873 29.23287010192871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6826906204223633 3.2865078449249268 34.547767639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6532429456710815 3.264421224594116 34.297454833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.656479001045227 3.651273250579834 38.169212341308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6458699703216553 3.8340697288513184 39.986568450927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6480340957641602 3.9379525184631348 41.027557373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6889363527297974 3.0978024005889893 32.666961669921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6871845722198486 3.8912885189056396 40.600067138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6337940692901611 3.4985573291778564 36.61936569213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6611578464508057 3.1806068420410156 33.467227935791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629490852355957 3.148672580718994 33.116214752197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6829264163970947 3.1589865684509277 33.27279281616211
  batch 20 loss: 1.6829264163970947, 3.1589865684509277, 33.27279281616211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6573193073272705 2.89308500289917 30.588171005249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6307939291000366 3.690046548843384 38.5312614440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6434714794158936 2.565918207168579 27.30265235900879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6632201671600342 3.3513131141662598 35.17634963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6885700225830078 3.936873197555542 41.05730438232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6475729942321777 3.3722105026245117 35.36967849731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6584570407867432 4.670771598815918 48.36617660522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652502417564392 2.944096088409424 31.093461990356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.608121633529663 3.2977914810180664 34.58603286743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679901361465454 3.155111789703369 33.23101806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6143070459365845 3.789919137954712 39.51350021362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6633970737457275 3.2201571464538574 33.86497116088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6469666957855225 3.23813796043396 34.028343200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6461478471755981 3.116281270980835 32.8089599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.61002516746521 3.164050579071045 33.25053024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626993179321289 3.3673079013824463 35.300071716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621431589126587 3.762869119644165 39.250125885009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6732227802276611 3.11210036277771 32.794227600097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6780502796173096 2.6588714122772217 28.266765594482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6860562562942505 2.623208522796631 27.918142318725586
  batch 40 loss: 1.6860562562942505, 2.623208522796631, 27.918142318725586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6476677656173706 4.1657795906066895 43.30546188354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6362030506134033 3.5237321853637695 36.8735237121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6296521425247192 2.8519928455352783 30.149580001831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6417896747589111 3.0785229206085205 32.42701721191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6192920207977295 3.7612485885620117 39.231781005859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652206540107727 3.1852219104766846 33.504425048828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6804176568984985 3.079695224761963 32.47736740112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6318587064743042 3.7907588481903076 39.53944778442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694198727607727 3.9149222373962402 40.843421936035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6348133087158203 3.1380858421325684 33.01567077636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6696531772613525 3.2041449546813965 33.71110534667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6661884784698486 3.6436400413513184 38.10258865356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6475683450698853 3.706912040710449 38.71669006347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6732122898101807 2.908661127090454 30.759822845458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6356815099716187 3.390247106552124 35.538150787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6934796571731567 3.5551841259002686 37.245323181152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.639585256576538 3.0765464305877686 32.40504837036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.631450891494751 3.7229321002960205 38.86077117919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466305255889893 3.8600471019744873 40.247100830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7137677669525146 3.642138719558716 38.135154724121094
  batch 60 loss: 1.7137677669525146, 3.642138719558716, 38.135154724121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6447449922561646 4.139779090881348 43.04253387451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6676160097122192 3.773250102996826 39.40011978149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6511350870132446 3.309048891067505 34.74162292480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6474940776824951 3.309809446334839 34.74559020996094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.639395833015442 2.994687080383301 31.586267471313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6618549823760986 3.4639806747436523 36.30166244506836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6775424480438232 3.485460042953491 36.532142639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6699519157409668 3.126563787460327 32.93558883666992
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.674696922302246 3.526324987411499 36.93794631958008
Total LOSS train 35.400890790499176 valid 35.67683506011963
CE LOSS train 1.6552768322137685 valid 0.4186742305755615
Contrastive LOSS train 3.37456139417795 valid 0.8815812468528748
EPOCH 43:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68575119972229 3.399014711380005 35.675899505615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993937492370605 3.4380414485931396 36.07980728149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664453148841858 3.198697805404663 33.651432037353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776548624038696 3.7617857456207275 39.29551315307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695692539215088 3.424903392791748 35.94472885131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6690834760665894 3.659045457839966 38.25953674316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702052354812622 4.184218883514404 43.54424285888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6823846101760864 3.571357011795044 37.39595413208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6754512786865234 3.062751531600952 32.3029670715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7060176134109497 3.6288108825683594 37.99412536621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6598860025405884 3.249605417251587 34.155941009521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6595975160598755 3.180187702178955 33.46147537231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6539075374603271 3.6778693199157715 38.43260192871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6584887504577637 3.7896921634674072 39.55541229248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698899269104004 2.7023134231567383 28.722034454345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6963313817977905 3.049745798110962 32.193790435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.646679162979126 3.029395818710327 31.940637588500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6733804941177368 2.8894312381744385 30.567691802978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6486119031906128 4.20245885848999 43.67320251464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6960704326629639 3.7605247497558594 39.30131912231445
  batch 20 loss: 1.6960704326629639, 3.7605247497558594, 39.30131912231445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6720199584960938 3.3882954120635986 35.55497360229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6494860649108887 3.2360026836395264 34.00951385498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6580920219421387 2.5814883708953857 27.472976684570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6686183214187622 2.6520092487335205 28.188711166381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6928116083145142 3.3691112995147705 35.3839225769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6562260389328003 3.2548742294311523 34.20497131347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6612788438796997 2.996325969696045 31.624540328979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6636403799057007 2.8648629188537598 30.31226921081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6225260496139526 3.402539014816284 35.64791488647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6904418468475342 3.3969078063964844 35.65951919555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6220659017562866 3.564990520477295 37.27197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678263545036316 3.517819881439209 36.85646438598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6578184366226196 3.0813653469085693 32.471473693847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6619011163711548 2.9770326614379883 31.432228088378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.628291368484497 3.0685224533081055 32.313514709472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6506626605987549 3.6952366828918457 38.60302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652160882949829 3.2772035598754883 34.4241943359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7036594152450562 3.3764798641204834 35.46845626831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7069905996322632 2.5431954860687256 27.138946533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7156153917312622 2.9709198474884033 31.424814224243164
  batch 40 loss: 1.7156153917312622, 2.9709198474884033, 31.424814224243164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6836912631988525 3.2785956859588623 34.46965026855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6718800067901611 3.1380460262298584 33.05234146118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6569855213165283 3.746277332305908 39.11975860595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6650867462158203 3.735159158706665 39.01667785644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6460363864898682 3.6343605518341064 37.98964309692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6634734869003296 3.5098061561584473 36.76153564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6933554410934448 2.8013229370117188 29.706584930419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649356722831726 3.7023468017578125 38.67282485961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7062053680419922 3.1990647315979004 33.69685363769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645472764968872 3.1190378665924072 32.83584976196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6659432649612427 3.795912981033325 39.62507247924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.66554856300354 3.794485330581665 39.61040496826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.652167797088623 3.9203414916992188 40.85558319091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6767827272415161 3.4097774028778076 35.774559020996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6409413814544678 3.3380982875823975 35.02192306518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6898998022079468 3.680365562438965 38.49355697631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6433748006820679 3.2970213890075684 34.613590240478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6370991468429565 3.859820604324341 40.23530578613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643896222114563 3.5247535705566406 36.89143371582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696432113647461 3.434250593185425 36.0389404296875
  batch 60 loss: 1.696432113647461, 3.434250593185425, 36.0389404296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6328117847442627 3.589002847671509 37.52284240722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6511486768722534 3.302615165710449 34.677303314208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6325438022613525 3.620445489883423 37.83700180053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6307181119918823 3.329913377761841 34.92985153198242
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6094669103622437 2.7150917053222656 28.76038360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6413307189941406 3.1789753437042236 33.43108367919922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6567150354385376 3.0972111225128174 32.62882614135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6544216871261597 2.9774649143218994 31.42906951904297
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6532292366027832 2.870678424835205 30.36001205444336
Total LOSS train 35.28957217289851 valid 31.962247848510742
CE LOSS train 1.6663488699839666 valid 0.4133073091506958
Contrastive LOSS train 3.362322271787203 valid 0.7176696062088013
EPOCH 44:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6678345203399658 3.013943910598755 31.807273864746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6798433065414429 3.4811227321624756 36.49106979370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6553797721862793 3.313994884490967 34.79533004760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6609222888946533 3.1156389713287354 32.81731033325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6757323741912842 3.004668712615967 31.72241973876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6390163898468018 2.8660202026367188 30.299219131469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6744924783706665 3.4006834030151367 35.68132781982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6522376537322998 2.609957695007324 27.751813888549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445006132125854 2.4959778785705566 26.604278564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6823314428329468 2.7914888858795166 29.597219467163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6353791952133179 3.9988694190979004 41.62407302856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6398224830627441 3.7094857692718506 38.73468017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.632582426071167 3.7616870403289795 39.249454498291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6355668306350708 3.636859655380249 38.0041618347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6854329109191895 3.0880582332611084 32.566017150878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6817841529846191 2.9647250175476074 31.32903480529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6209112405776978 3.3296055793762207 34.91696548461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6466790437698364 3.1409738063812256 33.056419372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6189578771591187 3.7500245571136475 39.11920166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6750297546386719 3.6729493141174316 38.40452194213867
  batch 20 loss: 1.6750297546386719, 3.6729493141174316, 38.40452194213867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6431866884231567 2.495398998260498 26.597177505493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6117725372314453 3.1211018562316895 32.822792053222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6288044452667236 3.1587891578674316 33.21669387817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642665147781372 3.1857481002807617 33.500144958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.666502594947815 3.8734545707702637 40.40105056762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624043345451355 3.170947313308716 33.33351516723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.643121600151062 3.309619903564453 34.73931884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6399095058441162 3.236514091491699 34.00505065917969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5850210189819336 2.7983436584472656 29.568458557128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6676403284072876 3.332224130630493 34.98988342285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5956636667251587 3.5279810428619385 36.8754768371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6651010513305664 3.2420670986175537 34.08576965332031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6375709772109985 2.694798707962036 28.58555793762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6417934894561768 2.8855206966400146 30.49700164794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.601609230041504 2.9348037242889404 30.94964599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6283241510391235 3.613680601119995 37.76512908935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6202000379562378 3.4779088497161865 36.399288177490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6801172494888306 3.121182680130005 32.891944885253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6889489889144897 3.6249160766601562 37.9381103515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6994693279266357 3.548628807067871 37.185760498046875
  batch 40 loss: 1.6994693279266357, 3.548628807067871, 37.185760498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659602403640747 2.9257378578186035 30.916980743408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649200201034546 3.068833827972412 32.33753967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6391957998275757 2.951045036315918 31.149646759033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6508265733718872 3.2715823650360107 34.36664962768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626142144203186 3.0038208961486816 31.664350509643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6625971794128418 3.6200671195983887 37.8632698059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6908507347106934 3.987114191055298 41.56199264526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6366885900497437 4.477336406707764 46.41005325317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7029458284378052 3.8997857570648193 40.700801849365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633962631225586 3.6971333026885986 38.60529327392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6683193445205688 3.2495992183685303 34.164310455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664522409439087 4.145570278167725 43.1202278137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6450341939926147 3.731912136077881 38.96415710449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6722123622894287 3.5409092903137207 37.081302642822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6295193433761597 4.159353733062744 43.22305679321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6891241073608398 3.781748056411743 39.50660705566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6241286993026733 3.541620969772339 37.040340423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6014922857284546 3.6805641651153564 38.407135009765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6145085096359253 2.983818531036377 31.45269203186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682032585144043 3.0443804264068604 32.12583541870117
  batch 60 loss: 1.682032585144043, 3.0443804264068604, 32.12583541870117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.59386146068573 3.0674939155578613 32.268798828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6194170713424683 3.053999900817871 32.15941619873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6024279594421387 4.167021751403809 43.272647857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.586586356163025 3.58345365524292 37.42112350463867
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5598928928375244 3.3535103797912598 35.09499740600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.596161127090454 3.8861501216888428 40.45766067504883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6132029294967651 2.8068060874938965 29.681264877319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606645941734314 3.0426743030548096 32.033390045166016
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6140598058700562 2.7353274822235107 28.967334747314453
Total LOSS train 35.104596768892726 valid 32.78491258621216
CE LOSS train 1.6449383662297175 valid 0.40351495146751404
Contrastive LOSS train 3.345965829262367 valid 0.6838318705558777
EPOCH 45:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6296930313110352 3.0738492012023926 32.36818313598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6467136144638062 3.689972162246704 38.54643630981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6160376071929932 2.9365386962890625 30.98142433166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6239089965820312 3.271692991256714 34.34083938598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6412168741226196 3.225876808166504 33.89998245239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.597420334815979 3.5286526679992676 36.88394546508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645377516746521 3.6763124465942383 38.40850067138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6168731451034546 3.3484628200531006 35.10150146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.605771780014038 3.157475709915161 33.18052673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6416025161743164 3.2007663249969482 33.64926528930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5937635898590088 3.130683422088623 32.90060043334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5949863195419312 3.5842137336730957 37.4371223449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5947662591934204 3.2473671436309814 34.06843566894531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6065462827682495 3.1759274005889893 33.36581802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6630229949951172 3.1935064792633057 33.598087310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6628583669662476 3.760073661804199 39.26359558105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5988327264785767 3.5485997200012207 37.0848274230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629137635231018 3.386469602584839 35.49383544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5999826192855835 3.2036292552948 33.63627624511719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.667797327041626 2.8621652126312256 30.28944969177246
  batch 20 loss: 1.667797327041626, 2.8621652126312256, 30.28944969177246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633885383605957 2.4689583778381348 26.323467254638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606170415878296 3.9576218128204346 41.18238830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621700406074524 2.8304007053375244 29.92570686340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.639555811882019 3.54199481010437 37.059505462646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.668491005897522 3.3850998878479004 35.51948928833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6342744827270508 2.5205485820770264 26.839759826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6501147747039795 3.4353744983673096 36.00386047363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6472028493881226 3.427400827407837 35.92121124267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5970215797424316 3.2224316596984863 33.82133865356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776353120803833 3.0857033729553223 32.53466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6022897958755493 2.9806368350982666 31.40865707397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6667081117630005 3.868076801300049 40.347476959228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6480767726898193 2.7017064094543457 28.66514015197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6506391763687134 3.7272286415100098 38.92292404174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6125218868255615 2.7816758155822754 29.429279327392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.631256341934204 3.2526848316192627 34.158103942871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6312891244888306 3.140976905822754 33.04105758666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6835891008377075 2.876563549041748 30.44922637939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6961082220077515 3.056356191635132 32.25967025756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7059394121170044 2.8816275596618652 30.522216796875
  batch 40 loss: 1.7059394121170044, 2.8816275596618652, 30.522216796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670309066772461 3.6334292888641357 38.004600524902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.65741765499115 2.829235076904297 29.94976806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6520625352859497 3.4378302097320557 36.030364990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639575958251953 3.0590758323669434 32.25471496582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6370348930358887 3.5875566005706787 37.51259994506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6643098592758179 3.3485045433044434 35.149356842041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692399024963379 2.8514835834503174 30.207233428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6557828187942505 3.148850917816162 33.14429473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7089800834655762 3.8232574462890625 39.94155502319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6615580320358276 3.0731935501098633 32.39349365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6896311044692993 3.50594425201416 36.74907302856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6905303001403809 3.147307872772217 33.16360855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6771540641784668 3.028726100921631 31.96441650390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6995962858200073 2.8393468856811523 30.09306526184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671428918838501 3.4043972492218018 35.71540069580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.720030426979065 3.6092615127563477 37.812644958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6781792640686035 3.403395652770996 35.71213912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6658432483673096 3.488232135772705 36.54816436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6723511219024658 3.3467278480529785 35.13962936401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7184456586837769 2.869892120361328 30.41736602783203
  batch 60 loss: 1.7184456586837769, 2.869892120361328, 30.41736602783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6630847454071045 3.63788104057312 38.04189682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6813989877700806 3.8549294471740723 40.23069381713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.661087155342102 3.299530506134033 34.65639114379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6536201238632202 3.2120745182037354 33.77436828613281
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6363564729690552 2.1337454319000244 22.97381019592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6644917726516724 2.7592947483062744 29.25743865966797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6765211820602417 2.7727017402648926 29.40353775024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.671627402305603 2.504565954208374 26.717287063598633
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.673721194267273 2.5284478664398193 26.958200454711914
Total LOSS train 34.09868387075571 valid 28.084115982055664
CE LOSS train 1.649589676123399 valid 0.41843029856681824
Contrastive LOSS train 3.244909433218149 valid 0.6321119666099548
Saved best model. Old loss 28.233078002929688 and new best loss 28.084115982055664
EPOCH 46:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.685516357421875 2.717057943344116 28.856096267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6967202425003052 3.3919601440429688 35.6163215637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6779212951660156 2.7618792057037354 29.29671287536621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6854435205459595 3.0753121376037598 32.43856430053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6995667219161987 3.5253753662109375 36.95331954956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6742709875106812 2.676133155822754 28.43560218811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7020081281661987 2.7830450534820557 29.53245735168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6848844289779663 3.194206714630127 33.626949310302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776669025421143 3.5242114067077637 36.91978073120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7018485069274902 2.861515998840332 30.31700897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669386625289917 3.111271381378174 32.782100677490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673723578453064 2.895892858505249 30.632652282714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6736363172531128 2.877988576889038 30.453521728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6828383207321167 2.985731601715088 31.54015350341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7189910411834717 3.316502094268799 34.884010314941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7181123495101929 3.819988250732422 39.91799545288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6797523498535156 2.9263875484466553 30.943628311157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6971595287322998 2.826580762863159 29.962966918945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6759005784988403 2.9442431926727295 31.11833381652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7137256860733032 3.21783447265625 33.89207077026367
  batch 20 loss: 1.7137256860733032, 3.21783447265625, 33.89207077026367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6950803995132446 3.0568506717681885 32.263587951660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678091287612915 3.3521504402160645 35.1995964050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6901485919952393 3.0649819374084473 32.3399658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6976323127746582 2.7254371643066406 28.952003479003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7190651893615723 3.3682026863098145 35.401092529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6975982189178467 3.183528423309326 33.53288269042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.708336353302002 3.6632654666900635 38.34099197387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702534794807434 2.33504056930542 25.052942276000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6767431497573853 2.970482587814331 31.381568908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7270562648773193 3.2531189918518066 34.258243560791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807312965393066 2.8689239025115967 30.369970321655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7165604829788208 2.8104238510131836 29.820798873901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7062181234359741 3.112337827682495 32.82959747314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.703154444694519 3.550663471221924 37.20978927612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6750940084457397 3.397857189178467 35.65366744995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682701587677002 3.425950050354004 35.94219970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6836657524108887 3.0110366344451904 31.79403305053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7205144166946411 2.5652780532836914 27.373294830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.720883846282959 3.0962674617767334 32.68355941772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7280964851379395 4.002042293548584 41.74851989746094
  batch 40 loss: 1.7280964851379395, 4.002042293548584, 41.74851989746094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7084383964538574 3.349853277206421 35.20697021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70068359375 3.1323928833007812 33.02461242675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692063331604004 3.639348268508911 38.08554458618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7022522687911987 3.5732946395874023 37.43519973754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6932460069656372 2.950512647628784 31.198373794555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7112480401992798 3.033742666244507 32.048675537109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7293756008148193 3.2410972118377686 34.14034652709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.697587490081787 3.401498556137085 35.71257019042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7377378940582275 3.3967444896698 35.70518493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6974694728851318 3.2042202949523926 33.73966979980469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.716724157333374 3.286846876144409 34.5851936340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7191895246505737 3.0458805561065674 32.17799377441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.706577181816101 3.6072778701782227 37.779354095458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.721838355064392 3.250782012939453 34.22966003417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6990567445755005 3.2192323207855225 33.891380310058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7349554300308228 3.4243671894073486 35.9786262512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6976864337921143 3.1174776554107666 32.87246322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6910823583602905 3.5948426723480225 37.63950729370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6968199014663696 3.3003997802734375 34.7008171081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7341082096099854 3.2795019149780273 34.52912902832031
  batch 60 loss: 1.7341082096099854, 3.2795019149780273, 34.52912902832031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691301941871643 3.0776963233947754 32.468265533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.708229422569275 3.3455934524536133 35.164161682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6994352340698242 3.3119518756866455 34.81895446777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694868564605713 2.8499679565429688 30.194547653198242
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6784214973449707 2.353060007095337 25.209020614624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6938921213150024 3.263418436050415 34.32807922363281
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.704566478729248 3.2016046047210693 33.720611572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7009814977645874 3.195876121520996 33.65974426269531
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.700406551361084 3.1422617435455322 33.123023986816406
Total LOSS train 33.27391961904672 valid 33.70786476135254
CE LOSS train 1.6993750388805682 valid 0.425101637840271
Contrastive LOSS train 3.1574544759897085 valid 0.7855654358863831
EPOCH 47:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710990309715271 3.362004280090332 35.331031799316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7154767513275146 3.172450304031372 33.439979553222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6991093158721924 3.3024301528930664 34.72340774536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7033449411392212 3.193857431411743 33.64191818237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710844874382019 2.8640732765197754 30.35157585144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6857974529266357 3.130147695541382 32.987274169921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.71109139919281 3.742459535598755 39.135684967041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6996208429336548 3.9445745944976807 41.14536666870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6886075735092163 3.5487582683563232 37.17618942260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.714744210243225 3.0191917419433594 31.906661987304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6854876279830933 3.595858097076416 37.644065856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686557650566101 2.7605035305023193 29.291593551635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.691380262374878 2.8323302268981934 30.01468276977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6967500448226929 3.196627140045166 33.663021087646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7310386896133423 2.918111562728882 30.912155151367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7335026264190674 3.529057264328003 37.02407455444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.694854736328125 3.2517669200897217 34.2125244140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7125952243804932 3.1260054111480713 32.97264862060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6887048482894897 2.8571317195892334 30.26002311706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7315219640731812 3.0938034057617188 32.6695556640625
  batch 20 loss: 1.7315219640731812, 3.0938034057617188, 32.6695556640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025436162948608 2.4334845542907715 26.037389755249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676576852798462 3.797908067703247 39.65565872192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6819007396697998 2.9811813831329346 31.49371337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6853368282318115 3.1420540809631348 33.10587692260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025877237319946 3.55525279045105 37.2551155090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6690788269042969 3.5018415451049805 36.687496185302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6767547130584717 3.3376972675323486 35.05372619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673498511314392 3.4478797912597656 36.15229797363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6293970346450806 2.748100757598877 29.110403060913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6973352432250977 3.5357794761657715 37.05513000488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6304837465286255 3.540580987930298 37.03629684448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6820913553237915 3.484091281890869 36.52300262451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6637049913406372 3.864313840866089 40.30684280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6663895845413208 3.458012342453003 36.24651336669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6276893615722656 3.6149001121520996 37.77669143676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6448540687561035 3.4111669063568115 35.75652313232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6560896635055542 3.679720401763916 38.45329284667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6996142864227295 3.838824987411499 40.08786392211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710693597793579 3.7343101501464844 39.053794860839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7260892391204834 3.512612819671631 36.85221862792969
  batch 40 loss: 1.7260892391204834, 3.512612819671631, 36.85221862792969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6978704929351807 2.7556049823760986 29.25391960144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7005566358566284 2.827028512954712 29.970840454101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6987793445587158 3.4628446102142334 36.32722473144531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7107901573181152 3.3574883937835693 35.28567123413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698826551437378 2.9011714458465576 30.710540771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7313189506530762 3.6182658672332764 37.913978576660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7507473230361938 3.154379367828369 33.29454040527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.71755051612854 3.4361846446990967 36.07939910888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7637901306152344 3.417780876159668 35.94160079956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7262182235717773 2.9015543460845947 30.74176025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7444907426834106 3.207810640335083 33.82259750366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.740706205368042 3.6436657905578613 38.177364349365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7276381254196167 3.652033805847168 38.24797821044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7376477718353271 3.458209753036499 36.31974411010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7089052200317383 3.451199769973755 36.22090148925781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7441954612731934 3.155399799346924 33.29819107055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.698015570640564 3.361328363418579 35.311302185058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6832369565963745 3.464695692062378 36.33019256591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6879788637161255 3.5242135524749756 36.93011474609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7351596355438232 3.598879814147949 37.723960876464844
  batch 60 loss: 1.7351596355438232, 3.598879814147949, 37.723960876464844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.690590500831604 3.195220708847046 33.642799377441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7149083614349365 3.1289687156677246 33.00459671020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6931723356246948 3.0791168212890625 32.48434066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70289945602417 3.559922933578491 37.302127838134766
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7025539875030518 3.52532696723938 36.9558219909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7026745080947876 3.7446110248565674 39.14878463745117
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7194111347198486 3.6341681480407715 38.061092376708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7139919996261597 3.566995143890381 37.38394546508789
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7023777961730957 3.5890495777130127 37.592872619628906
Total LOSS train 34.91530450674204 valid 38.04667377471924
CE LOSS train 1.7000504438693707 valid 0.4255944490432739
Contrastive LOSS train 3.3215254196753867 valid 0.8972623944282532
EPOCH 48:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.722941517829895 3.6998651027679443 38.72159194946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7325221300125122 3.5750341415405273 37.48286437988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7198901176452637 3.1237106323242188 32.95699691772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7204341888427734 3.463540554046631 36.35584259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7282274961471558 3.0418648719787598 32.146873474121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7119139432907104 3.775867462158203 39.47058868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7385503053665161 3.4817352294921875 36.555904388427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7063493728637695 3.1931040287017822 33.63739013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.709740161895752 3.467715263366699 36.38689422607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7271101474761963 3.6413767337799072 38.1408805847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7033910751342773 3.6466403007507324 38.169795989990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7096847295761108 3.129610538482666 33.00579071044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7123125791549683 3.809015989303589 39.80247116088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7103289365768433 3.2281341552734375 33.991668701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7358229160308838 3.2638003826141357 34.37382888793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7302520275115967 3.3695597648620605 35.42584991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6838918924331665 3.163060426712036 33.31449508666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.700984239578247 3.4361720085144043 36.062705993652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6734139919281006 3.0717110633850098 32.39052200317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7158969640731812 3.305492877960205 34.77082443237305
  batch 20 loss: 1.7158969640731812, 3.305492877960205, 34.77082443237305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6839265823364258 3.3632898330688477 35.31682205200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654806137084961 3.2004477977752686 33.65928649902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6685115098953247 3.5680134296417236 37.3486442565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6645426750183105 3.0597548484802246 32.26209259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687876582145691 3.6078760623931885 37.766639709472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6519066095352173 2.938589572906494 31.03780174255371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.663883924484253 3.851527690887451 40.179161071777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6564513444900513 3.4140918254852295 35.79737091064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6046591997146606 3.170862913131714 33.313289642333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6851102113723755 2.982646942138672 31.511579513549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6059590578079224 3.1866683959960938 33.4726448059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6818301677703857 3.529752016067505 36.97935104370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6539900302886963 2.9170825481414795 30.82481575012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6608378887176514 3.863409996032715 40.29494094848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621997356414795 3.7639224529266357 39.26122283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.632613182067871 3.540015697479248 37.032772064208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6276743412017822 3.361161708831787 35.23929214477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687407374382019 3.132920980453491 33.01661682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6966800689697266 3.123183488845825 32.92851257324219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7054108381271362 3.336613178253174 35.07154083251953
  batch 40 loss: 1.7054108381271362, 3.336613178253174, 35.07154083251953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6506190299987793 2.6773698329925537 28.424318313598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6262109279632568 2.9002230167388916 30.628440856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6110882759094238 3.7435660362243652 39.046749114990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6306819915771484 3.6253843307495117 37.88452911376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5974807739257812 3.031608819961548 31.9135684967041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6330342292785645 2.9594526290893555 31.22756004333496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6655001640319824 2.6901822090148926 28.56732177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.612917423248291 3.2794766426086426 34.407684326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68185293674469 3.649771213531494 38.1795654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.613629698753357 3.302004098892212 34.633670806884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6520392894744873 3.8901679515838623 40.55371856689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6541557312011719 3.5690431594848633 37.34458541870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6299399137496948 3.8613955974578857 40.243896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6586929559707642 3.4472129344940186 36.130821228027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.605440378189087 3.5180413722991943 36.78585433959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6758816242218018 3.1629180908203125 33.30506134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6033066511154175 4.080089092254639 42.404197692871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5789239406585693 3.5584235191345215 37.16315841674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5921249389648438 3.8900747299194336 40.49287414550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6726641654968262 3.399787187576294 35.670536041259766
  batch 60 loss: 1.6726641654968262, 3.399787187576294, 35.670536041259766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5774893760681152 4.04469633102417 42.024452209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6085165739059448 3.33837890625 34.992305755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5935920476913452 3.5006868839263916 36.6004638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5800607204437256 3.2273812294006348 33.8538703918457
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.557482123374939 3.163062334060669 33.188106536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5992655754089355 3.807816505432129 39.67742919921875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6150785684585571 3.2029335498809814 33.644412994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6098219156265259 3.522357225418091 36.83339309692383
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6189491748809814 3.4274375438690186 35.89332580566406
Total LOSS train 35.55608444213867 valid 36.51214027404785
CE LOSS train 1.6623547645715566 valid 0.40473729372024536
Contrastive LOSS train 3.389372939329881 valid 0.8568593859672546
EPOCH 49:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6350946426391602 3.7666757106781006 39.301849365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6525667905807495 3.5648791790008545 37.30135726928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.616765022277832 3.021898031234741 31.83574676513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.627083420753479 3.3972742557525635 35.59982681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6485716104507446 3.480356454849243 36.4521369934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614291787147522 3.3457913398742676 35.07220458984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.658224105834961 3.5196115970611572 36.854339599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6338269710540771 3.478760004043579 36.42142868041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623634696006775 3.3982152938842773 35.60578918457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6640955209732056 3.477365016937256 36.437747955322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6233628988265991 3.2121851444244385 33.745216369628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6345776319503784 3.5835249423980713 37.46982955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6303313970565796 3.350634813308716 35.136680603027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6340843439102173 3.1889548301696777 33.52363204956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6904550790786743 3.3231911659240723 34.922367095947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6898303031921387 3.719573736190796 38.88556671142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.640445590019226 3.356060743331909 35.201053619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664478063583374 3.190039873123169 33.564876556396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6467936038970947 3.090895414352417 32.555747985839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7021924257278442 2.9140098094940186 30.8422908782959
  batch 20 loss: 1.7021924257278442, 2.9140098094940186, 30.8422908782959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676810622215271 2.51912784576416 26.86808967590332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6549873352050781 3.3135077953338623 34.79006576538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6725529432296753 2.8676037788391113 30.348588943481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.674139142036438 3.0538129806518555 32.2122688293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6941821575164795 3.6557273864746094 38.25145721435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660699725151062 3.3483307361602783 35.144004821777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6700572967529297 3.8673899173736572 40.343955993652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6665433645248413 3.7708141803741455 39.37468338012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6215455532073975 3.773752450942993 39.35906982421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.687338948249817 3.070497989654541 32.39231872558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614600658416748 3.1178131103515625 32.79273223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6752334833145142 3.5291104316711426 36.96633529663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6542402505874634 2.325772523880005 24.911964416503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6580462455749512 3.023409605026245 31.89214324951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6280211210250854 3.442107915878296 36.04909896850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642930269241333 2.8219757080078125 29.862688064575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432087421417236 3.5337610244750977 36.98081588745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6921547651290894 4.5557756423950195 47.249908447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6969236135482788 4.019833564758301 41.89525604248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7040536403656006 3.4437758922576904 36.14181137084961
  batch 40 loss: 1.7040536403656006, 3.4437758922576904, 36.14181137084961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6683789491653442 2.829928159713745 29.967660903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6577003002166748 2.817108392715454 29.82878303527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6428011655807495 3.1941370964050293 33.584171295166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543296575546265 3.6488454341888428 38.142784118652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6356197595596313 3.155277729034424 33.18839645385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6629953384399414 3.2886345386505127 34.549339294433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6919455528259277 2.731764316558838 29.00958824157715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654611587524414 3.4588980674743652 36.24359130859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7142199277877808 3.2601206302642822 34.315425872802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6614224910736084 3.1392009258270264 33.05343246459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6835691928863525 3.4437003135681152 36.120574951171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6878666877746582 3.628063678741455 37.968502044677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6675236225128174 2.948413848876953 31.151662826538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688867449760437 3.313133478164673 34.8202018737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6571996212005615 3.097759246826172 32.63479232788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7188754081726074 3.4076263904571533 35.79513931274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6744182109832764 3.255735158920288 34.23176956176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6586804389953613 3.888348340988159 40.54216384887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6696631908416748 2.9268691539764404 30.9383544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7174007892608643 2.768099546432495 29.39839744567871
  batch 60 loss: 1.7174007892608643, 2.768099546432495, 29.39839744567871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.657758116722107 4.189388751983643 43.55164337158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684456467628479 3.743213415145874 39.1165885925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6778782606124878 3.135410785675049 33.031986236572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6720144748687744 2.832627058029175 29.9982852935791
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.662302017211914 2.9731435775756836 31.39373779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6789005994796753 3.907405138015747 40.752952575683594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6912204027175903 3.6641342639923096 38.33256149291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686423420906067 3.125807762145996 32.94449996948242
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6918038129806519 3.8439791202545166 40.131595611572266
Total LOSS train 34.817444904033955 valid 38.04040241241455
CE LOSS train 1.6621765301777767 valid 0.42295095324516296
Contrastive LOSS train 3.3155268595768854 valid 0.9609947800636292
EPOCH 50:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7042210102081299 3.771676778793335 39.42098617553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7146828174591064 3.2544140815734863 34.25882339477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7016984224319458 3.7145750522613525 38.847450256347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7001655101776123 2.875527858734131 30.4554443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7162114381790161 3.264101982116699 34.357234954833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6922129392623901 3.082260847091675 32.51482009887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7156031131744385 3.8121256828308105 39.83686065673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6970326900482178 3.4915971755981445 36.61300277709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692206859588623 3.6871066093444824 38.56327438354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7188483476638794 2.903463125228882 30.753480911254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6934298276901245 3.25500750541687 34.24350357055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7004203796386719 3.3040542602539062 34.740962982177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6829650402069092 3.2327210903167725 34.01017379760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692811369895935 3.881014823913574 40.502960205078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7166712284088135 3.46496319770813 36.366302490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7207649946212769 3.8542377948760986 40.26314163208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6754392385482788 3.3403542041778564 35.0789794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6967498064041138 3.2939229011535645 34.63597869873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6751600503921509 3.2686805725097656 34.36196517944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7140121459960938 3.397242307662964 35.68643569946289
  batch 20 loss: 1.7140121459960938, 3.397242307662964, 35.68643569946289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6931908130645752 3.4022023677825928 35.715213775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.668334722518921 2.801912546157837 29.68745994567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6764155626296997 2.6631598472595215 28.308015823364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6945995092391968 3.096975088119507 32.66435241699219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7058489322662354 3.3872151374816895 35.577999114990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6717791557312012 3.2333548069000244 34.00532913208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6830848455429077 3.482262134552002 36.505706787109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6685187816619873 3.4948408603668213 36.61692810058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6251200437545776 2.9940991401672363 31.566110610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6882401704788208 3.1121606826782227 32.80984878540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6191033124923706 3.6482129096984863 38.10123062133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6745336055755615 3.114345073699951 32.81798553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6543642282485962 3.1927597522735596 33.58196258544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6497617959976196 3.7501606941223145 39.1513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6179777383804321 3.3413949012756348 35.031925201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6399040222167969 3.714146137237549 38.78136444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6461174488067627 2.7624082565307617 29.270200729370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6964597702026367 2.834946393966675 30.04592514038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7024255990982056 2.7581264972686768 29.283689498901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.713880181312561 3.2216949462890625 33.93082809448242
  batch 40 loss: 1.713880181312561, 3.2216949462890625, 33.93082809448242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672348976135254 3.398920774459839 35.661556243896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6697319746017456 4.266462802886963 44.33435821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6472426652908325 4.052734851837158 42.174591064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654175043106079 3.4225103855133057 35.87928009033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6374088525772095 3.554903984069824 37.18645095825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6733567714691162 3.7693073749542236 39.36642837524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6993176937103271 3.674701452255249 38.44633102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432355642318726 4.180028438568115 43.443519592285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7136644124984741 3.917057991027832 40.88424301147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.641067624092102 3.175628185272217 33.3973503112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6605212688446045 3.3227505683898926 34.88802719116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6660672426223755 2.848381757736206 30.149885177612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6482065916061401 3.690446615219116 38.552669525146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.675658941268921 2.9669530391693115 31.345190048217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6282864809036255 3.6599276065826416 38.22756576538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692671298980713 3.234550952911377 34.03818130493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6252596378326416 3.519692897796631 36.82218933105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6141730546951294 3.090348720550537 32.517662048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6246355772018433 3.723996877670288 38.864601135253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6875486373901367 3.6889402866363525 38.57695388793945
  batch 60 loss: 1.6875486373901367, 3.6889402866363525, 38.57695388793945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614126205444336 3.1577680110931396 33.19180679321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6372590065002441 3.34303617477417 35.067623138427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621358871459961 3.7128677368164062 38.750038146972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6066639423370361 3.4162659645080566 35.76932144165039
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5829335451126099 2.6280601024627686 27.863534927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6183066368103027 2.9492027759552 31.110334396362305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.631219744682312 2.9599087238311768 31.23030662536621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6265040636062622 2.621068000793457 27.83718490600586
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6391515731811523 2.801896333694458 29.65811538696289
Total LOSS train 35.45176283029409 valid 29.958985328674316
CE LOSS train 1.671505959217365 valid 0.4097878932952881
Contrastive LOSS train 3.378025685823881 valid 0.7004740834236145
EPOCH 51:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.654601812362671 2.957892894744873 31.233531951904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6715571880340576 2.9388222694396973 31.059778213500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6375428438186646 3.018270254135132 31.82024574279785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445200443267822 2.8915536403656006 30.560056686401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6622569561004639 3.220869302749634 33.87095260620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6282380819320679 3.2452235221862793 34.080474853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6704597473144531 3.59867787361145 37.6572380065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6460583209991455 3.2608141899108887 34.25419998168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6344468593597412 2.675272226333618 28.387168884277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6691696643829346 2.7152302265167236 28.82147216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.621718406677246 3.101313591003418 32.63485336303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62505304813385 3.3415846824645996 35.04090118408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6199774742126465 3.366804838180542 35.28802490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6230429410934448 3.2652337551116943 34.2753791809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6777397394180298 3.8000335693359375 39.67807388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6731846332550049 3.618520498275757 37.8583869934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6086452007293701 3.1799402236938477 33.40804672241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6431915760040283 3.101680040359497 32.65999221801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6052076816558838 3.1600372791290283 33.20558166503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6710656881332397 3.1449015140533447 33.120079040527344
  batch 20 loss: 1.6710656881332397, 3.1449015140533447, 33.120079040527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.633980393409729 2.2510833740234375 24.144813537597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6060229539871216 3.6010549068450928 37.61656951904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.625167727470398 3.645759105682373 38.082759857177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642029881477356 3.09751558303833 32.617183685302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6689766645431519 3.126121997833252 32.93019485473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.626651406288147 3.235015630722046 33.976806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642466425895691 2.95408034324646 31.183269500732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6348756551742554 3.5671067237854004 37.30594253540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5770577192306519 2.880418539047241 30.381242752075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6631466150283813 2.9317338466644287 30.980485916137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5745474100112915 2.645928382873535 28.033830642700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649009108543396 3.131808280944824 32.96709060668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6226534843444824 3.0364620685577393 31.987274169921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6222503185272217 3.2130300998687744 33.7525520324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5797926187515259 3.013514995574951 31.714942932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5993621349334717 2.1241743564605713 22.84110450744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5958354473114014 2.5585222244262695 27.18105697631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6661893129348755 2.86457896232605 30.311979293823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6709434986114502 2.8806447982788086 30.477392196655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6815265417099 2.494719982147217 26.628726959228516
  batch 40 loss: 1.6815265417099, 2.494719982147217, 26.628726959228516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6353483200073242 3.066951274871826 32.30486297607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6241731643676758 3.5748836994171143 37.373008728027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6045116186141968 2.95173716545105 31.121883392333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6223030090332031 3.476531744003296 36.38761901855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5980839729309082 3.3344943523406982 34.94302749633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6305984258651733 3.8283960819244385 39.9145622253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6701927185058594 3.4266395568847656 35.936588287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6109710931777954 3.412187099456787 35.73284149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6859205961227417 3.8635408878326416 40.32133102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6190485954284668 3.2010445594787598 33.629493713378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6498851776123047 3.680241107940674 38.452293395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649356484413147 3.838304281234741 40.03239822387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6288599967956543 3.693850517272949 38.56736755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6591166257858276 3.7506678104400635 39.165794372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.608144998550415 3.5407941341400146 37.01608657836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6818256378173828 3.4921844005584717 36.603668212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6166764497756958 4.238973617553711 44.006412506103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5990848541259766 4.243895530700684 44.03804016113281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6147438287734985 3.468167781829834 36.29642105102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.693886637687683 3.1272904872894287 32.966793060302734
  batch 60 loss: 1.693886637687683, 3.1272904872894287, 32.966793060302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6064352989196777 3.75103759765625 39.1168098449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6294362545013428 3.4434056282043457 36.06349182128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.609024167060852 3.476135015487671 36.3703727722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5936561822891235 2.810753107070923 29.701187133789062
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5701167583465576 2.7396762371063232 28.96687889099121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6088178157806396 3.271545171737671 34.32426834106445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6211706399917603 3.4718453884124756 36.339622497558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6152777671813965 2.7225091457366943 28.840370178222656
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6289124488830566 2.879971742630005 30.42862892150879
Total LOSS train 33.9855214045598 valid 32.48322248458862
CE LOSS train 1.633562524502094 valid 0.40722811222076416
Contrastive LOSS train 3.235195911847628 valid 0.7199929356575012
EPOCH 52:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644534945487976 2.712587833404541 28.77041244506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6611539125442505 3.6350932121276855 38.012088775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6246153116226196 2.5625088214874268 27.24970245361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6299666166305542 3.286522150039673 34.49518966674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.647553563117981 3.742056369781494 39.068115234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6093323230743408 4.105792045593262 42.66725540161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6480045318603516 3.679887533187866 38.446876525878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6220011711120605 3.34975528717041 35.11955261230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6123595237731934 3.7755913734436035 39.36827087402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6486696004867554 4.032546043395996 41.9741325378418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6018891334533691 3.0731842517852783 32.33373260498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6092160940170288 3.4760422706604004 36.36963653564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5988514423370361 3.8792898654937744 40.39175033569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.602800965309143 3.615302085876465 37.75582504272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6603739261627197 3.6505701541900635 38.16607666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6597355604171753 3.9988818168640137 41.648555755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5916084051132202 3.835041046142578 39.942020416259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624428153038025 3.4722633361816406 36.34706115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5902615785598755 3.660161018371582 38.191871643066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6510714292526245 3.361891031265259 35.269981384277344
  batch 20 loss: 1.6510714292526245, 3.361891031265259, 35.269981384277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6167508363723755 3.933384656906128 40.950599670410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5848920345306396 3.631911039352417 37.90400314331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6059465408325195 3.438865900039673 35.994606018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6258561611175537 4.013044834136963 41.75630187988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6521461009979248 3.788820743560791 39.54035186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6083383560180664 3.762164831161499 39.229984283447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6249855756759644 3.8956868648529053 40.581851959228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614567518234253 3.7951853275299072 39.56642150878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.558647632598877 3.9108529090881348 40.66717529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6479840278625488 3.5842955112457275 37.49094009399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5600132942199707 3.6973676681518555 38.53369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6339609622955322 3.6057398319244385 37.69136047363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6047264337539673 3.8195505142211914 39.800228118896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6049537658691406 3.882495880126953 40.42991256713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5620969533920288 3.1902995109558105 33.465091705322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5802313089370728 3.553520917892456 37.115440368652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5757288932800293 4.046011924743652 42.035850524902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.647685170173645 3.5225324630737305 36.87301254272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.65363347530365 3.4788262844085693 36.44189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6630594730377197 3.9148685932159424 40.811744689941406
  batch 40 loss: 1.6630594730377197, 3.9148685932159424, 40.811744689941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6193801164627075 3.408679723739624 35.7061767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6045403480529785 4.183900833129883 43.44355010986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5904086828231812 3.3271803855895996 34.862213134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614203929901123 3.5213747024536133 36.82794952392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5828334093093872 3.3390109539031982 34.97294235229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6122981309890747 4.273050308227539 44.34280014038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6510932445526123 3.5156147480010986 36.8072395324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5945132970809937 3.95790433883667 41.17355728149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6713850498199463 3.6587870121002197 38.25925827026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6060267686843872 3.7292144298553467 38.898170471191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6367416381835938 3.8857955932617188 40.49469757080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6369260549545288 3.890462636947632 40.54154968261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6172916889190674 3.9940500259399414 41.55778884887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6502572298049927 3.4093215465545654 35.743473052978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.597782850265503 3.7018394470214844 38.61617660522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6726462841033936 4.080286026000977 42.47550582885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6081105470657349 4.09360408782959 42.54415512084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5911790132522583 3.503436326980591 36.62554168701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.606049656867981 3.5974888801574707 37.580936431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6765612363815308 3.215927839279175 33.835838317871094
  batch 60 loss: 1.6765612363815308, 3.215927839279175, 33.835838317871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5941358804702759 3.3850717544555664 35.44485092163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6199082136154175 3.8398280143737793 40.0181884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6030972003936768 3.9074783325195312 40.677879333496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.592448115348816 3.3488059043884277 35.080509185791016
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5688035488128662 3.2065534591674805 33.63433837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6106371879577637 3.4993603229522705 36.60424041748047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6217918395996094 3.7016537189483643 38.638328552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6156280040740967 3.401531934738159 35.63094711303711
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6308846473693848 2.776671886444092 29.39760398864746
Total LOSS train 38.13282858041617 valid 35.06778001785278
CE LOSS train 1.6182039205844585 valid 0.4077211618423462
Contrastive LOSS train 3.6514624779041 valid 0.694167971611023
EPOCH 53:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6451433897018433 3.3604531288146973 35.249671936035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6621373891830444 4.102456569671631 42.686702728271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6256635189056396 3.2625343799591064 34.251007080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.631245732307434 3.9086403846740723 40.717647552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6470991373062134 3.9343948364257812 40.99104690551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6109062433242798 3.498126268386841 36.592166900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6463048458099365 3.4814250469207764 36.46055603027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6223745346069336 3.4497101306915283 36.119476318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6115086078643799 3.95253324508667 41.1368408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.64451265335083 3.877772092819214 40.42223358154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6028274297714233 3.8095943927764893 39.69877243041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6076053380966187 3.5000457763671875 36.608062744140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6032127141952515 3.4745707511901855 36.34892272949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6050560474395752 3.6030752658843994 37.63581085205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6593047380447388 3.7903976440429688 39.56328201293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6589865684509277 4.136809349060059 43.02708053588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5988343954086304 3.019103527069092 31.789871215820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6282951831817627 3.4914329051971436 36.542625427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5995376110076904 3.1452786922454834 33.05232620239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6561623811721802 3.2974088191986084 34.6302490234375
  batch 20 loss: 1.6561623811721802, 3.2974088191986084, 34.6302490234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6234160661697388 3.0825700759887695 32.44911575317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5953196287155151 3.594773769378662 37.54305648803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6139744520187378 3.9412150382995605 41.026126861572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6375149488449097 3.1632845401763916 33.27035903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639498472213745 3.3280906677246094 34.944854736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6200597286224365 3.713752031326294 38.7575798034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6398323774337769 4.285716533660889 44.49699783325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6299023628234863 3.489516496658325 36.52506637573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5793821811676025 3.1714913845062256 33.29429626464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6607671976089478 3.882052183151245 40.48128890991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.580498218536377 3.8367316722869873 39.94781494140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6470327377319336 3.3334856033325195 34.98188781738281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.629913091659546 3.2371034622192383 34.000946044921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6225547790527344 2.8719394207000732 30.341949462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5828975439071655 3.107416868209839 32.657066345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5993984937667847 2.6807467937469482 28.4068660736084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.5970174074172974 2.5619521141052246 27.21653938293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6615644693374634 3.229870080947876 33.96026611328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6685312986373901 3.4944007396698 36.6125373840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6825975179672241 3.4262278079986572 35.94487762451172
  batch 40 loss: 1.6825975179672241, 3.4262278079986572, 35.94487762451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6427257061004639 3.623307943344116 37.87580490112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.62818443775177 4.432505130767822 45.9532356262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6133537292480469 3.2379531860351562 33.99288558959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6326388120651245 3.135709285736084 32.98973083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6081602573394775 3.22432541847229 33.851417541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6408532857894897 3.797623872756958 39.61709213256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6702865362167358 3.941347599029541 41.083763122558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6176750659942627 3.507514715194702 36.69282150268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6887801885604858 3.1565492153167725 33.2542724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6324098110198975 2.5637664794921875 27.27007484436035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6586768627166748 3.0193445682525635 31.852121353149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6564335823059082 2.5105040073394775 26.761472702026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6350237131118774 3.1620779037475586 33.255802154541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6618038415908813 3.1175687313079834 32.83749008178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6198097467422485 3.844430685043335 40.06411361694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686301589012146 3.637598752975464 38.06229019165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.624726414680481 3.2618658542633057 34.243385314941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6139657497406006 3.5617191791534424 37.23115539550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6274429559707642 2.945303440093994 31.080476760864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6906633377075195 3.3048837184906006 34.739498138427734
  batch 60 loss: 1.6906633377075195, 3.3048837184906006, 34.739498138427734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.614637017250061 3.46224045753479 36.23704147338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6421587467193604 3.07483172416687 32.39047622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623518466949463 2.945246934890747 31.075986862182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6113618612289429 3.483224630355835 36.443607330322266
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.5949403047561646 3.33500075340271 34.9449462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6286574602127075 3.567694902420044 37.305606842041016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6369742155075073 3.1036040782928467 32.67301559448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.630971908569336 3.423285961151123 35.86383056640625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.647233247756958 3.1253652572631836 32.90088653564453
Total LOSS train 35.91053551893968 valid 34.685834884643555
CE LOSS train 1.6313749973590557 valid 0.4118083119392395
Contrastive LOSS train 3.4279160719651443 valid 0.7813413143157959
EPOCH 54:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.658218502998352 3.333617925643921 34.9943962097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6722217798233032 3.8720457553863525 40.39268112182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6428394317626953 2.7413299083709717 29.05613899230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6484681367874146 3.138115882873535 33.02962875366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665759801864624 3.677272081375122 38.438480377197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.635022521018982 3.325873851776123 34.893760681152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6648496389389038 3.8331074714660645 39.99592208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6445804834365845 3.351026773452759 35.154850006103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.634290099143982 3.951996088027954 41.15425109863281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6652029752731323 3.8820199966430664 40.48540115356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6263129711151123 3.9632108211517334 41.258419036865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6301252841949463 3.9601387977600098 41.23151397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6248704195022583 3.1164424419403076 32.7892951965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6298844814300537 3.39359712600708 35.565853118896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678102731704712 3.979950189590454 41.47760772705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.676422357559204 3.4330384731292725 36.006805419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.623736023902893 3.5067484378814697 36.69122314453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6490623950958252 3.2674622535705566 34.32368469238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6241466999053955 3.401137113571167 35.63551712036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6752246618270874 2.6202163696289062 27.87738800048828
  batch 20 loss: 1.6752246618270874, 2.6202163696289062, 27.87738800048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6443825960159302 2.9178857803344727 30.823240280151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.622714638710022 3.4239513874053955 35.86222839355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6383275985717773 2.546520709991455 27.103534698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6516454219818115 3.4680306911468506 36.33195114135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6761417388916016 3.7575531005859375 39.251670837402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.64193594455719 3.5690155029296875 37.33209228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.655429720878601 3.6603240966796875 38.258670806884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6464923620224 3.235018491744995 33.99667739868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6007694005966187 3.2702341079711914 34.30310821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6726953983306885 3.127126455307007 32.94396209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.603421926498413 3.5841333866119385 37.44475555419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6635372638702393 3.434497833251953 36.008514404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.645725965499878 3.322662353515625 34.87234878540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6458735466003418 3.186619997024536 33.5120735168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6127173900604248 3.147531032562256 33.08802795410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6287575960159302 2.935255289077759 30.98130989074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6237692832946777 3.5496397018432617 37.12016677856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.681434154510498 3.2700183391571045 34.38161849975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686018466949463 3.5208497047424316 36.89451599121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6935086250305176 3.0277066230773926 31.97057342529297
  batch 40 loss: 1.6935086250305176, 3.0277066230773926, 31.97057342529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.656417727470398 2.767638921737671 29.332807540893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6465139389038086 3.4884912967681885 36.531429290771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.635740041732788 2.8879008293151855 30.51474952697754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6509939432144165 3.360424041748047 35.25523376464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.630231261253357 3.35583233833313 35.18855285644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6573601961135864 3.2321279048919678 33.9786376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6858012676239014 3.307429790496826 34.760101318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642375111579895 3.6522679328918457 38.16505432128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.70149564743042 3.523751974105835 36.93901443481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6491183042526245 3.1829750537872314 33.47886657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6719114780426025 3.8227593898773193 39.899505615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.670911431312561 3.6338436603546143 38.00934600830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.65631103515625 3.2798104286193848 34.45441436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6787915229797363 3.147705316543579 33.155845642089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6412400007247925 3.05133056640625 32.154544830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.697387456893921 2.9367005825042725 31.064393997192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.648803472518921 3.3033196926116943 34.68199920654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6368430852890015 3.2363100051879883 33.999942779541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649065613746643 3.7488644123077393 39.13771057128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7035846710205078 2.8038699626922607 29.742284774780273
  batch 60 loss: 1.7035846710205078, 2.8038699626922607, 29.742284774780273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6391563415527344 3.6072838306427 37.71199417114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660895824432373 3.5225982666015625 36.886878967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.644802212715149 3.775017023086548 39.39497375488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6374101638793945 3.4197769165039062 35.83517837524414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6193640232086182 3.6698482036590576 38.317848205566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6474539041519165 3.564361333847046 37.29106521606445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6553244590759277 3.538463830947876 37.03996276855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6508533954620361 3.7050211429595947 38.70106506347656
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6608213186264038 3.6094491481781006 37.75531005859375
Total LOSS train 35.40807949946477 valid 37.69685077667236
CE LOSS train 1.651033326295706 valid 0.41520532965660095
Contrastive LOSS train 3.3757046259366548 valid 0.9023622870445251
EPOCH 55:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.673784852027893 3.443936586380005 36.11315155029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6862074136734009 3.5612783432006836 37.29899215698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6580026149749756 3.612548351287842 37.783485412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6638621091842651 3.403298854827881 35.69684982299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6805123090744019 2.997723340988159 31.657745361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6520254611968994 3.3721578121185303 35.37360382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6822465658187866 3.363776206970215 35.320011138916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6637026071548462 3.3716821670532227 35.380523681640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6554962396621704 2.8236708641052246 29.8922061920166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6824191808700562 3.8596279621124268 40.2786979675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.649878978729248 3.6386120319366455 38.0359992980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.650883674621582 3.6274478435516357 37.92536163330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6471132040023804 3.974135637283325 41.38846969604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6519700288772583 4.176087856292725 43.41284942626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6920100450515747 3.3842222690582275 35.53423309326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6893788576126099 3.124016761779785 32.92954635620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.642040729522705 3.178980588912964 33.431846618652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6667072772979736 3.077608823776245 32.44279479980469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6423009634017944 3.0251758098602295 31.894060134887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6855717897415161 3.145932674407959 33.144901275634766
  batch 20 loss: 1.6855717897415161, 3.145932674407959, 33.144901275634766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.662811279296875 3.6565451622009277 38.22826385498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.640969157218933 3.5201804637908936 36.8427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6550509929656982 2.8911235332489014 30.566286087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6702662706375122 2.943074941635132 31.101016998291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6887378692626953 3.3370258808135986 35.058998107910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6579653024673462 3.4045498371124268 35.70346450805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6693775653839111 3.355393409729004 35.22330856323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6647894382476807 2.806797981262207 29.732769012451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6250163316726685 2.7869632244110107 29.49464988708496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684967041015625 3.1227002143859863 32.91196823120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6255227327346802 3.549940347671509 37.12492752075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6777580976486206 3.6704189777374268 38.3819465637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660654902458191 3.255079746246338 34.21145248413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6593886613845825 3.1281983852386475 32.94137191772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6294679641723633 3.2279043197631836 33.908512115478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6446834802627563 3.102363348007202 32.66831588745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6432249546051025 3.0682895183563232 32.32612228393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6920421123504639 3.618042469024658 37.872467041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696278691291809 3.016786813735962 31.864147186279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025587558746338 3.0902602672576904 32.60516357421875
  batch 40 loss: 1.7025587558746338, 3.0902602672576904, 32.60516357421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698163747787476 3.3049545288085938 34.7193603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664067029953003 2.748267889022827 29.146745681762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6518434286117554 3.8021085262298584 39.67292785644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.666940689086914 3.487354278564453 36.54048156738281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6472173929214478 3.369147777557373 35.33869552612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6689988374710083 3.9430644512176514 41.09964370727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.695197582244873 3.3220818042755127 34.916015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6571638584136963 3.7741265296936035 39.39842987060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7073293924331665 3.2251060009002686 33.95838928222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.66509211063385 3.0140106678009033 31.805198669433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6877535581588745 3.141441822052002 33.102169036865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68593168258667 3.2572076320648193 34.25800704956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.672400951385498 2.690774440765381 28.58014678955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6927986145019531 3.3104445934295654 34.797245025634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.660940408706665 4.092126369476318 42.58220672607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7096518278121948 3.399378538131714 35.70343780517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6665836572647095 2.621798276901245 27.884567260742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6576660871505737 3.3044607639312744 34.702274322509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6659270524978638 3.001819610595703 31.684123992919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7138389348983765 2.958874225616455 31.302579879760742
  batch 60 loss: 1.7138389348983765, 2.958874225616455, 31.302579879760742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6579153537750244 3.3717105388641357 35.375022888183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6781874895095825 3.152869462966919 33.20688247680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.664400339126587 3.308063268661499 34.745033264160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.655667781829834 3.6880581378936768 38.53624725341797
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6410000324249268 2.7748918533325195 29.38991928100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6674076318740845 2.821380615234375 29.881214141845703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.674692988395691 3.2469356060028076 34.14405059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6720302104949951 3.2496519088745117 34.16855239868164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.68050217628479 3.0708844661712646 32.389347076416016
Total LOSS train 34.709984705998345 valid 32.64579105377197
CE LOSS train 1.6672611841788658 valid 0.4201255440711975
Contrastive LOSS train 3.304272332558265 valid 0.7677211165428162
EPOCH 56:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6901201009750366 2.8706374168395996 30.396495819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7004272937774658 3.7029457092285156 38.72988510131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6768643856048584 3.4014134407043457 35.69099807739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6802932024002075 3.14780330657959 33.1583251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6926871538162231 3.505671977996826 36.74940872192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6689238548278809 3.629197120666504 37.96089172363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6927560567855835 3.7776196002960205 39.46895217895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6780370473861694 3.0395781993865967 32.07381820678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6711639165878296 3.628376007080078 37.954925537109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6952590942382812 3.3979458808898926 35.67471694946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6668719053268433 2.9843950271606445 31.510822296142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698989868164062 3.5478591918945312 37.14849090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6666799783706665 3.2588300704956055 34.254981994628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6694953441619873 3.289154052734375 34.56103515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7050367593765259 2.9527533054351807 31.232568740844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7032045125961304 3.0051846504211426 31.755050659179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6630237102508545 3.336919069290161 35.0322151184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6843229532241821 2.654751777648926 28.231840133666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6614665985107422 3.423635244369507 35.89781951904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7016677856445312 2.8594062328338623 30.295730590820312
  batch 20 loss: 1.7016677856445312, 2.8594062328338623, 30.295730590820312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6802473068237305 3.2541868686676025 34.22211837768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6611722707748413 4.033724308013916 41.9984130859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6730605363845825 3.6479573249816895 38.15263366699219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6861448287963867 3.025928497314453 31.945430755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7037094831466675 3.5707907676696777 37.411617279052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6761466264724731 2.7954633235931396 29.630781173706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6845570802688599 3.157320499420166 33.25776290893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6793620586395264 3.3488361835479736 35.167724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6439982652664185 3.426694631576538 35.91094207763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6999356746673584 3.7427101135253906 39.127037048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6479229927062988 3.145839214324951 33.10631561279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.693664312362671 3.4309117794036865 36.00278091430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6772154569625854 4.193233966827393 43.609554290771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6763043403625488 3.5873043537139893 37.549346923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6497812271118164 3.5419836044311523 37.069618225097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6613906621932983 3.489266872406006 36.55406188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.659822940826416 3.8314530849456787 39.9743537902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7027719020843506 3.530949592590332 37.012264251708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.707166314125061 2.7072763442993164 28.779930114746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7131457328796387 3.4596524238586426 36.309669494628906
  batch 40 loss: 1.7131457328796387, 3.4596524238586426, 36.309669494628906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6858116388320923 2.900524616241455 30.691057205200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6772575378417969 3.0248959064483643 31.92621612548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.668217658996582 3.306614875793457 34.7343635559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807996034622192 3.108558177947998 32.76638412475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6639494895935059 2.8095710277557373 29.759660720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6820745468139648 3.367034912109375 35.35242462158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7055177688598633 3.3766708374023438 35.472225189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6714125871658325 3.462825298309326 36.29966735839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7187148332595825 3.3789896965026855 35.50861358642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6768125295639038 3.0620927810668945 32.2977409362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696006417274475 3.912414073944092 40.82014846801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6944948434829712 3.3804712295532227 35.49920654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6810375452041626 3.533719539642334 37.01823425292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7000031471252441 3.0654191970825195 32.35419464111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6705182790756226 3.304399013519287 34.714508056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7142019271850586 3.2157299518585205 33.87150192260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6743966341018677 2.9448540210723877 31.122936248779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.665689468383789 3.31146240234375 34.780311584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.675810694694519 3.0377798080444336 32.05360794067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7201473712921143 3.315098762512207 34.871131896972656
  batch 60 loss: 1.7201473712921143, 3.315098762512207, 34.871131896972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6702508926391602 3.7782864570617676 39.4531135559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6883124113082886 3.3773083686828613 35.461395263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6747177839279175 3.5966637134552 37.641353607177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6671440601348877 3.292874813079834 34.59589385986328
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6534128189086914 2.5807557106018066 27.460968017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.67805814743042 3.3763720989227295 35.44178009033203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.68556547164917 3.0937750339508057 32.623313903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6832940578460693 2.664848804473877 28.3317813873291
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6888238191604614 2.7073004245758057 28.76182746887207
Total LOSS train 34.87843366769644 valid 31.28967571258545
CE LOSS train 1.6814220483486468 valid 0.42220595479011536
Contrastive LOSS train 3.3197011727553147 valid 0.6768251061439514
EPOCH 57:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.69801926612854 3.0784554481506348 32.482574462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7078462839126587 3.087829113006592 32.58613967895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6872564554214478 3.3787996768951416 35.47525405883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6900920867919922 3.055372714996338 32.24382019042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7013404369354248 3.811676025390625 39.81809997558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6787439584732056 2.8867619037628174 30.546361923217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7020620107650757 3.9739151000976562 41.44121170043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6874825954437256 3.144556760787964 33.13304901123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6794865131378174 3.273256778717041 34.412052154541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7029387950897217 3.153642177581787 33.23936080932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6747026443481445 3.5590388774871826 37.26509094238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.67746102809906 3.3052096366882324 34.729557037353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.675127625465393 3.2308084964752197 33.98321533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.679133415222168 3.0772178173065186 32.45131301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7108263969421387 3.49703049659729 36.68113327026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.707558035850525 3.5027384757995605 36.73494338989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6698917150497437 3.0597546100616455 32.26743698120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6884068250656128 2.793452739715576 29.622934341430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.668684959411621 3.540257692337036 37.07126235961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7072697877883911 2.9393470287323 31.100740432739258
  batch 20 loss: 1.7072697877883911, 2.9393470287323, 31.100740432739258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6862246990203857 3.697758197784424 38.6638069152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.66865074634552 3.626056671142578 37.92921829223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6807777881622314 2.7687265872955322 29.368043899536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6907460689544678 3.6083412170410156 37.7741584777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7069118022918701 3.8512048721313477 40.21895980834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6817457675933838 4.036877632141113 42.05052185058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6903496980667114 4.157410621643066 43.26445388793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6839364767074585 4.044035911560059 42.124298095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6490072011947632 3.981750965118408 41.466514587402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7028392553329468 3.727710485458374 38.979942321777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6530311107635498 2.726496934890747 28.917999267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6974289417266846 3.7203567028045654 38.900997161865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6773078441619873 3.3285391330718994 34.96269989013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.678401231765747 3.158911943435669 33.267520904541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6567821502685547 2.8068811893463135 29.72559356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6669185161590576 3.5350420475006104 37.01734161376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6661169528961182 3.681832790374756 38.48444747924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7077833414077759 3.2579591274261475 34.28737258911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7118923664093018 3.347808837890625 35.189979553222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7163652181625366 3.019430637359619 31.91067123413086
  batch 40 loss: 1.7163652181625366, 3.019430637359619, 31.91067123413086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.690058946609497 3.409414768218994 35.78420639038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.682104229927063 3.924673557281494 40.92884063720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6722965240478516 3.2952880859375 34.62517547607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6842900514602661 3.8946821689605713 40.6311149597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6692612171173096 3.2713100910186768 34.382362365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6884610652923584 3.076972007751465 32.45817947387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7101480960845947 3.1521265506744385 33.231414794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6776772737503052 3.991265296936035 41.590328216552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.718988299369812 3.7123801708221436 38.84278869628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6828113794326782 3.6206023693084717 37.88883590698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7011425495147705 3.6869492530822754 38.57063293457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7003676891326904 3.3767480850219727 35.46784591674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6895561218261719 3.5732743740081787 37.422298431396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7072056531906128 3.8524370193481445 40.23157501220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.680781364440918 3.854233741760254 40.22311782836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7209943532943726 3.7527410984039307 39.24840545654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684762954711914 3.5369420051574707 37.05418395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6769704818725586 3.7430925369262695 39.10789489746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.686234474182129 3.533205509185791 37.018287658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.722758173942566 3.530891180038452 37.03166961669922
  batch 60 loss: 1.722758173942566, 3.530891180038452, 37.03166961669922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6783275604248047 3.358090877532959 35.259239196777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6961477994918823 3.5558431148529053 37.25457763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6848970651626587 3.5402870178222656 37.0877685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6759474277496338 3.4631495475769043 36.30744552612305
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.662536382675171 2.3471739292144775 25.134275436401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6839851140975952 3.630309581756592 37.987083435058594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6881239414215088 3.4552760124206543 36.24088668823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6880022287368774 4.17177677154541 43.40576934814453
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.6937373876571655 3.402526617050171 35.71900177001953
Total LOSS train 36.07034703768217 valid 38.33818531036377
CE LOSS train 1.6878811561144316 valid 0.4234343469142914
Contrastive LOSS train 3.438246591274555 valid 0.8506316542625427
EPOCH 58:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.703250527381897 3.8950204849243164 40.6534538269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7134162187576294 3.8446595668792725 40.160011291503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6930299997329712 4.011898517608643 41.812015533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6966105699539185 3.9134409427642822 40.83102035522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.704898476600647 4.43726110458374 46.077510833740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6850485801696777 3.9284863471984863 40.96990966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7060980796813965 4.062251091003418 42.328609466552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6951892375946045 3.4308953285217285 36.00414276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688712239265442 2.758820056915283 29.276912689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.709731936454773 2.5853796005249023 27.563528060913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.684959053993225 2.8459532260894775 30.14449119567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6873669624328613 2.801791191101074 29.705278396606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6847535371780396 3.1663599014282227 33.34835433959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6877104043960571 3.1435892581939697 33.12360382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7181334495544434 3.546252727508545 37.180660247802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7174559831619263 3.7626593112945557 39.344051361083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6842142343521118 3.1347591876983643 33.03180694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025583982467651 3.4607696533203125 36.31025314331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6817858219146729 3.8090949058532715 39.772735595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.716567873954773 3.662552833557129 38.34209442138672
  batch 20 loss: 1.716567873954773, 3.662552833557129, 38.34209442138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6982507705688477 3.294860601425171 34.646854400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6836837530136108 3.2954647541046143 34.63833236694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.693336009979248 3.2893261909484863 34.58659744262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.702727198600769 2.5812501907348633 27.515228271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7175670862197876 3.0025343894958496 31.74291229248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6951618194580078 3.2182273864746094 33.87743377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7032488584518433 3.196951150894165 33.672760009765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6995458602905273 3.330156087875366 35.00110626220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6699186563491821 3.1592860221862793 33.262779235839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7162424325942993 2.4826977252960205 26.5432186126709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.669571042060852 3.286590337753296 34.53547286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7075146436691284 3.288613796234131 34.59365463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6959929466247559 3.5832221508026123 37.52821350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6948152780532837 3.189537525177002 33.59019088745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6718989610671997 3.424107551574707 35.91297149658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6789495944976807 3.3249142169952393 34.92809295654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6785169839859009 3.310682773590088 34.785343170166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7151707410812378 3.4473319053649902 36.18849182128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7177180051803589 3.461259603500366 36.3303108215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7244638204574585 3.47776460647583 36.50210952758789
  batch 40 loss: 1.7244638204574585, 3.47776460647583, 36.50210952758789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7006360292434692 3.634817600250244 38.04881286621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6949925422668457 3.557281494140625 37.26780700683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6853018999099731 3.8655622005462646 40.34092330932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.696285367012024 3.9771413803100586 41.46770095825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6837608814239502 3.634859800338745 38.0323600769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7023152112960815 3.6684775352478027 38.38709259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7212965488433838 3.2693753242492676 34.4150505065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6927610635757446 3.555202007293701 37.244781494140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7306956052780151 3.3121590614318848 34.8522834777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6986258029937744 3.4779107570648193 36.47773361206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7141547203063965 3.5223228931427 36.937381744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7134984731674194 2.859168767929077 30.305187225341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.703216552734375 3.5950653553009033 37.65386962890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7181729078292847 3.083824634552002 32.556419372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6940189599990845 2.917052745819092 30.864547729492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7302345037460327 3.0304999351501465 32.03523635864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6976583003997803 3.31547212600708 34.852378845214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6896156072616577 3.2694997787475586 34.38461685180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6974809169769287 3.4635748863220215 36.333229064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.732283353805542 3.2262072563171387 33.99435806274414
  batch 60 loss: 1.732283353805542, 3.2262072563171387, 33.99435806274414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.692078948020935 3.599933385848999 37.691410064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7056916952133179 3.091198444366455 32.61767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6951714754104614 3.357421636581421 35.269386291503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.688321590423584 2.8109920024871826 29.798242568969727
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6779580116271973 2.6715333461761475 28.393291473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6975398063659668 2.8572962284088135 30.2705020904541
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025423049926758 3.3623039722442627 35.32558059692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7009080648422241 2.734584093093872 29.046749114990234
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7068216800689697 2.795193672180176 29.65875816345215
Total LOSS train 35.332066081120416 valid 31.075397491455078
CE LOSS train 1.6992617387038012 valid 0.42670542001724243
Contrastive LOSS train 3.363280439376831 valid 0.698798418045044
EPOCH 59:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7146679162979126 2.6160736083984375 27.875404357910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7228502035140991 3.022484064102173 31.947690963745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7052878141403198 2.841434955596924 30.11963653564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7085694074630737 3.4065821170806885 35.774391174316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7181344032287598 3.4904823303222656 36.62295913696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7003976106643677 3.041569232940674 32.1160888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7175766229629517 2.991814136505127 31.635717391967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7071609497070312 2.175523519515991 23.4623966217041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7000921964645386 3.581981897354126 37.51991271972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7203574180603027 2.7730672359466553 29.451030731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6966108083724976 3.6399729251861572 38.09634017944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6988340616226196 3.484304666519165 36.5418815612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6957025527954102 3.4639999866485596 36.33570098876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.699018955230713 3.4806008338928223 36.505027770996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7255395650863647 2.900937080383301 30.73491096496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.725525975227356 3.0720467567443848 32.44599151611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6945627927780151 3.4945409297943115 36.63996887207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710456371307373 3.3263309001922607 34.9737663269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6935012340545654 3.9768307209014893 41.46180725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7246061563491821 2.9118621349334717 30.84322738647461
  batch 20 loss: 1.7246061563491821, 2.9118621349334717, 30.84322738647461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7081639766693115 2.704709053039551 28.7552547454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6941215991973877 3.5618958473205566 37.313079833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7025457620620728 3.6597487926483154 38.30003356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7111891508102417 3.0068652629852295 31.779842376708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7242788076400757 3.1416385173797607 33.140663146972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7043346166610718 2.958005666732788 31.284391403198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7114830017089844 3.0086119174957275 31.7976016998291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7063323259353638 3.5322346687316895 37.02867889404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6808524131774902 2.851656913757324 30.19742202758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7214555740356445 3.038213014602661 32.10358428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6823773384094238 2.836503267288208 30.047409057617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7174880504608154 3.3670384883880615 35.38787078857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7045434713363647 3.1189627647399902 32.89417266845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7041276693344116 3.7692837715148926 39.39696502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6863727569580078 3.3258209228515625 34.944580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6945338249206543 3.333341121673584 35.02794647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6934109926223755 2.867694139480591 30.370351791381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7251983880996704 3.153905153274536 33.26424789428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7270430326461792 3.480616331100464 36.533206939697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7327417135238647 3.638796091079712 38.120704650878906
  batch 40 loss: 1.7327417135238647, 3.638796091079712, 38.120704650878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.710992455482483 3.408719062805176 35.798179626464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7045543193817139 4.010106086730957 41.80561447143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6968022584915161 3.518608570098877 36.88288879394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7056891918182373 3.0865371227264404 32.57106018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6933655738830566 3.430373430252075 35.997100830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7085449695587158 2.927664279937744 30.985187530517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7249101400375366 2.6716840267181396 28.44175148010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7012863159179688 3.0366551876068115 32.067840576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7367607355117798 3.1126952171325684 32.863712310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7059383392333984 3.1783792972564697 33.48973083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7210595607757568 3.6965835094451904 38.686893463134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7204793691635132 3.5785751342773438 37.506229400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7107903957366943 3.5999629497528076 37.710418701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7243363857269287 3.067016363143921 32.394500732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7022767066955566 4.195282936096191 43.65510559082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7353087663650513 3.3492836952209473 35.228145599365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7056859731674194 3.3260347843170166 34.966033935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6989561319351196 2.900015115737915 30.699106216430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7053405046463013 3.4376437664031982 36.08177947998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7363903522491455 2.9461309909820557 31.19770050048828
  batch 60 loss: 1.7363903522491455, 2.9461309909820557, 31.19770050048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6995376348495483 2.7769463062286377 29.4689998626709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7131390571594238 3.1613943576812744 33.327083587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.704172134399414 3.5066096782684326 36.77027130126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6986814737319946 3.1282012462615967 32.98069381713867
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.6897841691970825 1.9922739267349243 21.612524032592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7074671983718872 2.9583587646484375 31.29105567932129
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.711256980895996 3.37937331199646 35.50498962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7103110551834106 4.016694068908691 41.87725067138672
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7149348258972168 3.185701847076416 33.57195281982422
Total LOSS train 33.87662171583909 valid 35.561312198638916
CE LOSS train 1.7087204676408034 valid 0.4287337064743042
Contrastive LOSS train 3.2167901350901675 valid 0.796425461769104
EPOCH 60:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.72184419631958 2.9531877040863037 31.253721237182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7299553155899048 4.0733256340026855 42.46321105957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7143070697784424 3.2958617210388184 34.67292404174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7171027660369873 2.85984206199646 30.315523147583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7242259979248047 3.612006425857544 37.84429168701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.709025502204895 3.463517665863037 36.34420394897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7242980003356934 3.9077515602111816 40.80181121826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.715517520904541 3.9625158309936523 41.34067916870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7102718353271484 3.0992648601531982 32.702919006347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7278025150299072 3.588301181793213 37.61081314086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7083585262298584 3.3852591514587402 35.560951232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7096126079559326 3.676525115966797 38.4748649597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7078349590301514 3.8856213092803955 40.564048767089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7102254629135132 3.4399991035461426 36.11021423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733342170715332 3.0682458877563477 32.415802001953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7326328754425049 3.5327069759368896 37.05970001220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7065366506576538 3.611966371536255 37.82619857788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7202554941177368 3.769188642501831 39.41214370727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.706148386001587 3.578305721282959 37.48920822143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7323980331420898 3.220099687576294 33.93339538574219
  batch 20 loss: 1.7323980331420898, 3.220099687576294, 33.93339538574219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7188080549240112 2.804304599761963 29.761852264404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7071595191955566 3.3653368949890137 35.360530853271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.714495062828064 3.2301557064056396 34.01605224609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7219089269638062 3.503187894821167 36.753787994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733910083770752 3.280658483505249 34.54049301147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7158058881759644 3.9455950260162354 41.171756744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7234141826629639 3.9396421909332275 41.11983871459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7193348407745361 4.020856857299805 41.92790222167969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6981303691864014 3.1615562438964844 33.31369400024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7316452264785767 3.758326530456543 39.314910888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.6975703239440918 3.4406192302703857 36.103763580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7265976667404175 3.575260877609253 37.47920608520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7159643173217773 3.8510825634002686 40.22679138183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.716704249382019 3.753117561340332 39.24787902832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7013612985610962 3.4544568061828613 36.24592971801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7087624073028564 2.989654541015625 31.605308532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7074310779571533 3.599355459213257 37.700984954833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733520746231079 3.439422845840454 36.127750396728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7355972528457642 3.821838617324829 39.953983306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7376317977905273 3.2459003925323486 34.19663619995117
  batch 40 loss: 1.7376317977905273, 3.2459003925323486, 34.19663619995117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7242182493209839 3.1825671195983887 33.549888610839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7208518981933594 3.210099220275879 33.821842193603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7142311334609985 3.293912649154663 34.65335464477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7228320837020874 3.7257637977600098 38.98046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7133992910385132 3.5433437824249268 37.14683532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7251631021499634 3.7297463417053223 39.02262496948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7389631271362305 3.375556468963623 35.494529724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7193149328231812 3.410813093185425 35.82744598388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.74713134765625 3.5171613693237305 36.91874694824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7232145071029663 3.4011595249176025 35.73480987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733646035194397 3.3526344299316406 35.25999069213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7338786125183105 3.2962634563446045 34.69651412963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.727138638496399 2.9476277828216553 31.20341682434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7371758222579956 3.611215114593506 37.849327087402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7202892303466797 3.718390941619873 38.904197692871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.746016025543213 3.2319424152374268 34.0654411315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.723681092262268 3.5974984169006348 37.698665618896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7195765972137451 3.9364843368530273 41.08442306518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.725675344467163 4.272850036621094 44.45417404174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7484869956970215 3.770134449005127 39.4498291015625
  batch 60 loss: 1.7484869956970215, 3.770134449005127, 39.4498291015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7221883535385132 4.108894348144531 42.81113052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.731405258178711 3.7078135013580322 38.809539794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7262762784957886 3.532562017440796 37.05189514160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7207067012786865 3.2681567668914795 34.40227508544922
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7157727479934692 3.056063652038574 32.27640914916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.729000210762024 3.213843822479248 33.86743927001953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7317442893981934 3.212432861328125 33.85607147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7304385900497437 3.4337027072906494 36.067466735839844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7354379892349243 2.766465663909912 29.400094985961914
Total LOSS train 36.792822353656476 valid 33.297768115997314
CE LOSS train 1.721672039765578 valid 0.4338594973087311
Contrastive LOSS train 3.507115030288696 valid 0.691616415977478
EPOCH 61:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7396072149276733 3.114834785461426 32.88795471191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.744903564453125 4.3157453536987305 44.90235900878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733147144317627 3.160374164581299 33.33688735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.73569655418396 3.147148847579956 33.207183837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7413501739501953 3.6513726711273193 38.25507354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7308576107025146 3.402618646621704 35.75704574584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7419341802597046 3.2077300548553467 33.819236755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.736018180847168 2.6743040084838867 28.47905731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7341010570526123 3.812995433807373 39.86405563354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7445613145828247 3.765706777572632 39.40162658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7319648265838623 3.559159994125366 37.32356262207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.733237862586975 3.4833984375 36.567222595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7326768636703491 3.7188632488250732 38.92131042480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7333449125289917 3.313899517059326 34.87234115600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.749061942100525 3.4466958045959473 36.21601867675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7494046688079834 3.695265293121338 38.702056884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7341277599334717 3.0917859077453613 32.65198516845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.744756817817688 3.4981460571289062 36.726219177246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7376525402069092 2.905151844024658 30.789169311523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7529889345169067 2.964350700378418 31.396495819091797
  batch 20 loss: 1.7529889345169067, 2.964350700378418, 31.396495819091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7459336519241333 3.0936169624328613 32.68210220336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.739416241645813 3.017954111099243 31.918956756591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7431997060775757 3.169806718826294 33.44126510620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.748853325843811 3.1867258548736572 33.616111755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7545115947723389 3.34641695022583 35.21868133544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7452260255813599 3.3702986240386963 35.44821548461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7507556676864624 3.5921759605407715 37.672515869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.748258352279663 3.323383092880249 34.982086181640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7388825416564941 3.1838786602020264 33.57767105102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7570933103561401 2.7090137004852295 28.847230911254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7420117855072021 3.398205518722534 35.72406768798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7560150623321533 2.4694149494171143 26.450164794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7515921592712402 2.6463429927825928 28.215023040771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.751876950263977 3.5716753005981445 37.4686279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7450934648513794 3.7980034351348877 39.725128173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.749044418334961 3.7814815044403076 39.56385803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7467025518417358 3.4920263290405273 36.666969299316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7598426342010498 3.2453973293304443 34.21381378173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7613070011138916 2.885885000228882 30.62015724182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.763158917427063 3.3476667404174805 35.239830017089844
  batch 40 loss: 1.763158917427063, 3.3476667404174805, 35.239830017089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.755637288093567 2.9726662635803223 31.4822998046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7535492181777954 3.2290375232696533 34.043922424316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7505766153335571 3.3420071601867676 35.17064666748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7576340436935425 3.4384443759918213 36.1420783996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.752223014831543 3.9491848945617676 41.24407196044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7571874856948853 3.6509366035461426 38.26655197143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7638311386108398 2.840846538543701 30.172298431396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7546659708023071 3.1217880249023438 32.9725456237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7694462537765503 3.5441622734069824 37.2110710144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7579225301742554 3.670832395553589 38.46624755859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7618839740753174 2.9769129753112793 31.531015396118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7646607160568237 3.5338830947875977 37.10348892211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7647424936294556 2.6919548511505127 28.684289932250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7699944972991943 3.4774880409240723 36.54487228393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7653344869613647 3.0946664810180664 32.711997985839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7767813205718994 2.9189071655273438 30.965852737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7732101678848267 3.2928528785705566 34.70173645019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7770692110061646 3.30375599861145 34.81462860107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.782611608505249 3.668379545211792 38.466407775878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7850929498672485 3.0620603561401367 32.405696868896484
  batch 60 loss: 1.7850929498672485, 3.0620603561401367, 32.405696868896484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7845628261566162 3.4452600479125977 36.237159729003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.783201813697815 3.1924290657043457 33.70749282836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7808057069778442 3.3546571731567383 35.32737731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7790701389312744 2.8025755882263184 29.804826736450195
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7783054113388062 2.2370476722717285 24.148780822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7815240621566772 3.404160499572754 35.82312774658203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7812830209732056 2.733259677886963 29.11387825012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7813645601272583 3.0782418251037598 32.56378173828125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7829301357269287 2.5561978816986084 27.34490966796875
Total LOSS train 34.57994918823242 valid 31.211424350738525
CE LOSS train 1.7535410826022808 valid 0.4457325339317322
Contrastive LOSS train 3.282640834955069 valid 0.6390494704246521
EPOCH 62:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7830581665039062 3.075502634048462 32.5380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7851364612579346 3.3250765800476074 35.03590393066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7818224430084229 3.214683771133423 33.92866134643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7799240350723267 2.9052858352661133 30.832782745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7791835069656372 2.5555200576782227 27.33438491821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7751855850219727 2.588446617126465 27.659652709960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7755378484725952 3.181579113006592 33.591331481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7744381427764893 2.787592887878418 29.650367736816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7714923620224 3.2344393730163574 34.11588668823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7733161449432373 3.7602739334106445 39.37605285644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7685197591781616 3.1582753658294678 33.35127258300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7682769298553467 4.049248695373535 42.26076126098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7652140855789185 3.8355424404144287 40.120635986328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7621160745620728 3.9986040592193604 41.7481575012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7674846649169922 3.06990647315979 32.466548919677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7677913904190063 2.9077868461608887 30.845661163330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7584816217422485 3.065274715423584 32.41122817993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7630369663238525 3.114773988723755 32.9107780456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.759578824043274 3.2447187900543213 34.20676803588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7662307024002075 3.0869369506835938 32.63560104370117
  batch 20 loss: 1.7662307024002075, 3.0869369506835938, 32.63560104370117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7621619701385498 3.0566184520721436 32.328346252441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.76004958152771 2.8222849369049072 29.982898712158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.762110948562622 3.1626176834106445 33.38828659057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7643582820892334 3.357715368270874 35.34151077270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.767722487449646 3.5066099166870117 36.833824157714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.763174057006836 3.2677831649780273 34.441009521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.768796443939209 3.5417704582214355 37.186500549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7663952112197876 3.298358201980591 34.749977111816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7620311975479126 3.7283337116241455 39.04536819458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7704659700393677 3.4751625061035156 36.522090911865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7629189491271973 3.580815076828003 37.57107162475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7708877325057983 3.3619141578674316 35.39002990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7687751054763794 3.5415775775909424 37.18455123901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7695056200027466 3.3454816341400146 35.22431945800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7670899629592896 3.5330488681793213 37.09757995605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7709442377090454 3.7805607318878174 39.5765495300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7698888778686523 3.3639347553253174 35.409236907958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7764873504638672 3.5623199939727783 37.399688720703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7763643264770508 3.9651710987091064 41.42807388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7769774198532104 3.769411325454712 39.471092224121094
  batch 40 loss: 1.7769774198532104, 3.769411325454712, 39.471092224121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7728897333145142 3.559583902359009 37.36872863769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772605538368225 3.64762282371521 38.24883270263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7687429189682007 3.87231183052063 40.491859436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772294521331787 3.132831573486328 33.100608825683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7677068710327148 3.8872106075286865 40.63981246948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7695711851119995 3.0577683448791504 32.347251892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7725504636764526 3.6497867107391357 38.270416259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7671570777893066 3.3017873764038086 34.7850341796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7772551774978638 3.5164153575897217 36.941410064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7704765796661377 3.1973981857299805 33.74445724487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7712125778198242 3.264040470123291 34.411617279052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7737369537353516 3.1164660453796387 32.93840026855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7739166021347046 2.915508985519409 30.929006576538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.776192307472229 2.5456414222717285 27.23260498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7735012769699097 2.6948084831237793 28.721586227416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7795439958572388 3.268223762512207 34.4617805480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772127628326416 3.8983871936798096 40.75600051879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734520435333252 3.8917040824890137 40.690494537353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734912633895874 3.988887071609497 41.66236114501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7760730981826782 3.244319200515747 34.21926498413086
  batch 60 loss: 1.7760730981826782, 3.244319200515747, 34.21926498413086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7697162628173828 4.011041164398193 41.880126953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7725837230682373 4.124695301055908 43.019535064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7690902948379517 3.4533472061157227 36.30255889892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7654041051864624 2.9054195880889893 30.819599151611328
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.762721061706543 2.559749126434326 27.360214233398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7671077251434326 2.942282199859619 31.18992805480957
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.767895221710205 2.995046377182007 31.718358993530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7675548791885376 2.8748576641082764 30.516132354736328
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7695528268814087 2.884749174118042 30.61704444885254
Total LOSS train 35.291324498103215 valid 31.010365962982178
CE LOSS train 1.7704145339819102 valid 0.4423882067203522
Contrastive LOSS train 3.352090993294349 valid 0.7211872935295105
EPOCH 63:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7723082304000854 3.1028785705566406 32.80109405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7741913795471191 3.2730252742767334 34.50444412231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7681334018707275 2.801255226135254 29.780685424804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7690789699554443 2.755955457687378 29.32863426208496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.76994788646698 3.5611205101013184 37.38115310668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7671363353729248 3.372331142425537 35.490447998046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.770172119140625 3.641045331954956 38.180625915527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.768747329711914 3.9623472690582275 41.39221954345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7670644521713257 3.4144301414489746 35.9113655090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.771535038948059 3.8213717937469482 39.985252380371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7677077054977417 3.5869877338409424 37.6375846862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7698651552200317 3.523549795150757 37.00536346435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7691181898117065 3.5862925052642822 37.63204574584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7693978548049927 3.7135674953460693 38.90507125854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774271011352539 3.243911027908325 34.21337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7740228176116943 3.713792562484741 38.91194534301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7700815200805664 2.877591133117676 30.54599380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7730244398117065 3.2321391105651855 34.094417572021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7713642120361328 2.870457649230957 30.475940704345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7754030227661133 2.8220160007476807 29.995563507080078
  batch 20 loss: 1.7754030227661133, 2.8220160007476807, 29.995563507080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775986671447754 2.923848867416382 31.014476776123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775620460510254 3.225904941558838 34.03466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.776893138885498 3.380103588104248 35.57793045043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7779070138931274 3.511672019958496 36.894630432128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7774232625961304 3.443138837814331 36.20881271362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7756693363189697 3.6252031326293945 38.0276985168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7785557508468628 3.6528897285461426 38.30745315551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7761989831924438 3.09489369392395 32.725135803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7708537578582764 3.1938369274139404 33.709224700927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775411605834961 3.0841288566589355 32.61669921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7698090076446533 3.1832919120788574 33.60272979736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7761561870574951 3.1563961505889893 33.340118408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.773144245147705 2.9962780475616455 31.735923767089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.773498296737671 3.151669502258301 33.290191650390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7719923257827759 2.8861243724823 30.633235931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734780311584473 3.7594423294067383 39.36790084838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772418737411499 3.2031445503234863 33.803863525390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7779903411865234 3.257652521133423 34.354515075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7777822017669678 3.5975358486175537 37.75313949584961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7779704332351685 3.719778537750244 38.97575378417969
  batch 40 loss: 1.7779704332351685, 3.719778537750244, 38.97575378417969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775220274925232 2.918734550476074 30.962566375732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774215817451477 3.635964870452881 38.13386535644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7720552682876587 3.307748317718506 34.84954071044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775565266609192 3.9209043979644775 40.98461151123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7719248533248901 3.7396769523620605 39.16869354248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774405598640442 3.838811159133911 40.16251754760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.776930570602417 3.3741347789764404 35.518280029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7727329730987549 3.8814094066619873 40.58682632446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7790507078170776 3.0204567909240723 31.983617782592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7730506658554077 3.0357019901275635 32.130069732666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7732337713241577 3.707960844039917 38.85284423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7747410535812378 2.8420586585998535 30.19532585144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774419903755188 2.700047254562378 28.774892807006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775654673576355 2.670563220977783 28.481285095214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7711890935897827 2.8684134483337402 30.455324172973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7795028686523438 3.777488946914673 39.55439376831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7724155187606812 3.1960253715515137 33.732669830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7725024223327637 3.552807092666626 37.300575256347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7733197212219238 3.5077757835388184 36.851078033447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.778827428817749 4.100142478942871 42.78025436401367
  batch 60 loss: 1.778827428817749, 4.100142478942871, 42.78025436401367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734509706497192 2.740436553955078 29.17781639099121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7758647203445435 2.499011278152466 26.76597785949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7739605903625488 2.623661756515503 28.010578155517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7711466550827026 2.231109857559204 24.082244873046875
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7678524255752563 2.2479584217071533 24.2474365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772586464881897 2.6720471382141113 28.493057250976562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772169589996338 2.688959836959839 28.661767959594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.77241051197052 2.360175132751465 25.374162673950195
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7748794555664062 2.4367408752441406 26.142288208007812
Total LOSS train 34.52139408405011 valid 27.167819023132324
CE LOSS train 1.7733933026974018 valid 0.44371986389160156
Contrastive LOSS train 3.2748000658475434 valid 0.6091852188110352
Saved best model. Old loss 28.084115982055664 and new best loss 27.167819023132324
EPOCH 64:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.77627694606781 2.645143747329712 28.22771453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7788499593734741 2.914246082305908 30.921310424804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7743427753448486 2.364370822906494 25.41805076599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775744080543518 2.346123218536377 25.236974716186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.776755690574646 2.95998477935791 31.376604080200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775291919708252 2.5270094871520996 27.045387268066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7762081623077393 3.1726880073547363 33.50308609008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7753417491912842 2.8750505447387695 30.525846481323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734376192092896 2.835479974746704 30.128236770629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7761039733886719 2.467618465423584 26.452289581298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7731138467788696 2.9627039432525635 31.4001522064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774867057800293 3.3152787685394287 34.92765426635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7734075784683228 3.5219290256500244 36.992698669433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7740073204040527 3.9013516902923584 40.78752136230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7782714366912842 3.448518991470337 36.26346206665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7787271738052368 3.3021585941314697 34.800315856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7748486995697021 2.9625048637390137 31.399898529052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7772150039672852 3.451632261276245 36.29353713989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7762479782104492 2.59535813331604 27.729827880859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.779329776763916 2.951319932937622 31.29252815246582
  batch 20 loss: 1.779329776763916, 2.951319932937622, 31.29252815246582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7784945964813232 2.44266414642334 26.205135345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7780512571334839 3.1148478984832764 32.92652893066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7792152166366577 2.7745935916900635 29.525150299072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7805463075637817 3.025773525238037 32.03828430175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7808682918548584 3.2324814796447754 34.105682373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7794111967086792 2.952052354812622 31.29993438720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.78125 2.630457639694214 28.085826873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7790250778198242 2.5248184204101562 27.027210235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7753149271011353 3.003389835357666 31.80921173095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.779244303703308 3.1522529125213623 33.30177307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.774422526359558 3.077995538711548 32.554378509521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7781459093093872 3.02195405960083 31.9976863861084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7758173942565918 2.811835527420044 29.89417266845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7752577066421509 3.327603578567505 35.051292419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.771886944770813 2.9519731998443604 31.29161834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7735950946807861 3.0532004833221436 32.305599212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.772725224494934 3.2364156246185303 34.136878967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7789981365203857 3.24875807762146 34.266578674316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7787690162658691 3.653428792953491 38.31305694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7786136865615845 3.19006085395813 33.679222106933594
  batch 40 loss: 1.7786136865615845, 3.19006085395813, 33.679222106933594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7767287492752075 4.085513114929199 42.63186264038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7762455940246582 4.115789890289307 42.93414306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7742146253585815 3.3125271797180176 34.89948654174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7773271799087524 3.4522624015808105 36.29995346069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7741841077804565 3.2140798568725586 33.91498565673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7765436172485352 2.684976816177368 28.626312255859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.780231237411499 3.3491103649139404 35.27133560180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.775547981262207 4.308228492736816 44.85783004760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.781093955039978 3.859860420227051 40.379695892333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7775428295135498 3.006458282470703 31.842124938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.778045892715454 2.9295523166656494 31.07356834411621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7789506912231445 2.550611734390259 27.28506851196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.778828501701355 2.5377981662750244 27.156808853149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.780079960823059 2.556457996368408 27.34465980529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.777462124824524 3.7279069423675537 39.0565299987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.783288836479187 2.977264642715454 31.55593490600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7791551351547241 3.486633539199829 36.64549255371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7799688577651978 2.8227665424346924 30.007633209228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.782148838043213 2.8417563438415527 30.1997127532959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7864253520965576 2.836052179336548 30.14694595336914
  batch 60 loss: 1.7864253520965576, 2.836052179336548, 30.14694595336914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7873053550720215 3.8876423835754395 40.663726806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7883673906326294 3.9931344985961914 41.71971130371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.786789059638977 2.9732255935668945 31.519044876098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7843549251556396 3.81931734085083 39.9775276184082
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7821972370147705 2.7372806072235107 29.155004501342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7835774421691895 3.0567071437835693 32.350650787353516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.783305048942566 3.0244669914245605 32.027976989746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7831617593765259 2.7132821083068848 28.91598129272461
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.783927083015442 2.8524928092956543 30.308856964111328
Total LOSS train 32.85697567279522 valid 30.900866508483887
CE LOSS train 1.7780164553568913 valid 0.4459817707538605
Contrastive LOSS train 3.1078959465026856 valid 0.7131232023239136
EPOCH 65:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7849477529525757 3.144559144973755 33.23053741455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7856868505477905 3.2156591415405273 33.94227981567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7839527130126953 3.149425506591797 33.27820587158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7850940227508545 2.8841421604156494 30.626514434814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7862390279769897 2.6879971027374268 28.666210174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891701459884644 3.1532440185546875 33.32160949707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892383337020874 2.700378894805908 28.793025970458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7888766527175903 3.0528454780578613 32.31732940673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790209174156189 2.6531457901000977 28.321666717529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79025137424469 3.070371150970459 32.49396514892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7893918752670288 2.79695200920105 29.758913040161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789587140083313 2.324420690536499 25.033794403076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7867794036865234 2.60591459274292 27.84592628479004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.786412239074707 2.558067560195923 27.367088317871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7876276969909668 2.247352123260498 24.261150360107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.78739333152771 2.835378408432007 30.141178131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7859421968460083 2.494154691696167 26.727489471435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7864165306091309 2.6979260444641113 28.765676498413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.784657597541809 3.1866323947906494 33.65098190307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.785278558731079 2.662423610687256 28.409515380859375
  batch 20 loss: 1.785278558731079, 2.662423610687256, 28.409515380859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.786852478981018 2.554532527923584 27.332178115844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7884981632232666 3.4450197219848633 36.23869323730469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7879976034164429 3.334977626800537 35.13777542114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891647815704346 3.380629539489746 35.595462799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904305458068848 3.8540709018707275 40.33114242553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792935848236084 3.825549364089966 40.04842758178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940045595169067 3.0715155601501465 32.50916290283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918384075164795 2.6569433212280273 28.361270904541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7876133918762207 2.672935724258423 28.516969680786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.785535454750061 2.94724702835083 31.258005142211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7825047969818115 3.2876663208007812 34.6591682434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7834134101867676 3.0398757457733154 32.18217086791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.779936671257019 3.1150877475738525 32.93081283569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7790117263793945 3.2117602825164795 33.89661407470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7779964208602905 2.6237339973449707 28.015335083007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7792325019836426 2.953338861465454 31.312620162963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7795525789260864 3.000056743621826 31.780120849609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7849078178405762 3.017885684967041 31.963764190673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7847685813903809 3.282120704650879 34.60597229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7867680788040161 3.0019311904907227 31.806079864501953
  batch 40 loss: 1.7867680788040161, 3.0019311904907227, 31.806079864501953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7857471704483032 3.4744691848754883 36.53043746948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7853612899780273 2.8965020179748535 30.750381469726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7852345705032349 2.6718478202819824 28.503713607788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7865204811096191 2.9167394638061523 30.953914642333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.784964680671692 2.959951162338257 31.384477615356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7862231731414795 3.0229344367980957 32.015567779541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7868486642837524 3.4776158332824707 36.56300735473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7864596843719482 3.1906535625457764 33.692996978759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7895123958587646 3.007972240447998 31.86923599243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7887027263641357 3.477634906768799 36.5650520324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7886936664581299 3.3541903495788574 35.330596923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896356582641602 3.63832426071167 38.17287826538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904962301254272 3.080291748046875 32.593414306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896006107330322 3.2059991359710693 33.84959030151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928434610366821 3.1620712280273438 33.41355514526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902883291244507 3.8265814781188965 40.05610275268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7881591320037842 3.6566736698150635 38.354896545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7874467372894287 3.7872655391693115 39.660099029541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.783859133720398 3.234328269958496 34.12714385986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7853025197982788 3.2488491535186768 34.2737922668457
  batch 60 loss: 1.7853025197982788, 3.2488491535186768, 34.2737922668457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7828339338302612 3.1335935592651367 33.118770599365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7846519947052002 3.0485587120056152 32.270240783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7828351259231567 3.6775710582733154 38.55854797363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7855441570281982 3.0021770000457764 31.807313919067383
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.787119746208191 3.1959943771362305 33.74706268310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790877103805542 2.745460271835327 29.245479583740234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911046743392944 3.3150856494903564 34.941959381103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913434505462646 3.1808412075042725 33.599754333496094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7913217544555664 2.6880979537963867 28.67230224609375
Total LOSS train 32.51688608022837 valid 31.6148738861084
CE LOSS train 1.7866307955521803 valid 0.4478304386138916
Contrastive LOSS train 3.073025527367225 valid 0.6720244884490967
EPOCH 66:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790954828262329 2.560523509979248 27.396190643310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934892177581787 3.208517551422119 33.878662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79422926902771 2.884488344192505 30.63911247253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929513454437256 3.5361948013305664 37.1548957824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909677028656006 3.6304473876953125 38.09543991088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902084589004517 3.4697468280792236 36.487674713134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791313648223877 3.4814839363098145 36.60615158081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7847840785980225 2.7770750522613525 29.55553436279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7871205806732178 2.9525701999664307 31.312822341918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.785714030265808 2.9124624729156494 30.910337448120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7856327295303345 3.1746444702148438 33.53207778930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7875152826309204 3.4224774837493896 36.01228713989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7873057126998901 3.5630719661712646 37.41802215576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7869092226028442 3.196800708770752 33.75491714477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904725074768066 3.0272791385650635 32.063262939453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790332317352295 3.425058603286743 36.04092025756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789528727531433 3.283389091491699 34.62342071533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789380431175232 3.3361804485321045 35.15118408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788752555847168 3.271390676498413 34.50265884399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7874846458435059 2.766634941101074 29.453834533691406
  batch 20 loss: 1.7874846458435059, 2.766634941101074, 29.453834533691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898211479187012 2.539306163787842 27.182884216308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891356945037842 2.2146339416503906 23.935474395751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908902168273926 3.0309441089630127 32.1003303527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788629412651062 3.274162530899048 34.53025436401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7884496450424194 3.7440133094787598 39.228580474853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907390594482422 3.228564739227295 34.076385498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791088342666626 3.8886501789093018 40.677589416503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916390895843506 3.7446892261505127 39.238529205322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902752161026 3.5664687156677246 37.45496368408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916967868804932 3.6993441581726074 38.78514099121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916394472122192 3.4950106143951416 36.74174880981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918771505355835 2.8527028560638428 30.318906784057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789113163948059 2.9611501693725586 31.400615692138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896071672439575 2.7922048568725586 29.71165657043457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919862270355225 3.4460015296936035 36.25199890136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917718887329102 3.235171318054199 34.14348602294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791250467300415 3.2748124599456787 34.53937530517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792662262916565 2.613978385925293 27.932445526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791080117225647 2.6621110439300537 28.41219139099121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928515672683716 2.428154468536377 26.07439613342285
  batch 40 loss: 1.7928515672683716, 2.428154468536377, 26.07439613342285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 2.649897813796997 28.291011810302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944200038909912 3.256976366043091 34.36418151855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939696311950684 2.136319637298584 23.157167434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7947955131530762 2.016385793685913 21.958654403686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927495241165161 2.2794008255004883 24.58675765991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7952818870544434 2.529604911804199 27.091331481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943315505981445 2.9978692531585693 31.773025512695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792211651802063 2.9112865924835205 30.90507698059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936673164367676 3.4540815353393555 36.3344841003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929935455322266 2.317986249923706 24.972856521606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794569730758667 2.7706875801086426 29.50144386291504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.795568823814392 3.1460423469543457 33.2559928894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939517498016357 2.8094370365142822 29.888320922851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933109998703003 2.975621461868286 31.549524307250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924085855484009 2.8372600078582764 30.165008544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790486454963684 3.0284717082977295 32.07520294189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7888343334197998 3.8623061180114746 40.411895751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903741598129272 3.1568784713745117 33.35915756225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7890981435775757 3.4920079708099365 36.70917510986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912415266036987 3.294642686843872 34.737667083740234
  batch 60 loss: 1.7912415266036987, 3.294642686843872, 34.737667083740234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793940782546997 4.067907333374023 42.47301483154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7946662902832031 3.007066011428833 31.865325927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792189359664917 3.0975027084350586 32.767215728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792149305343628 3.336203098297119 35.154178619384766
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.789358139038086 2.6262731552124023 28.05208969116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790550708770752 2.4913241863250732 26.703792572021484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899415493011475 2.2273592948913574 24.063535690307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907899618148804 2.80754017829895 29.866191864013672
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7906345129013062 2.6655728816986084 28.44636344909668
Total LOSS train 32.657324952345625 valid 27.269970893859863
CE LOSS train 1.791074694119967 valid 0.44765862822532654
Contrastive LOSS train 3.0866250625023475 valid 0.6663932204246521
EPOCH 67:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79033625125885 2.566901683807373 27.459354400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918583154678345 3.441789388656616 36.20975112915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907639741897583 3.1989047527313232 33.77981185913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897911071777344 2.5183966159820557 26.973756790161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919113636016846 2.826237916946411 30.054290771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912238836288452 2.739000082015991 29.181224822998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936866283416748 2.511812448501587 26.91180992126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920960187911987 2.8433313369750977 30.22540855407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929712533950806 2.562595844268799 27.418928146362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940313816070557 2.7296597957611084 29.09062957763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914677858352661 2.3730978965759277 25.52244758605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911534309387207 3.532836437225342 37.1195182800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896445989608765 2.903989553451538 30.829540252685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898401021957397 3.0232865810394287 32.022705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914623022079468 2.950106620788574 31.29252815246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7895311117172241 3.482584238052368 36.61537551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789535403251648 3.2090299129486084 33.87983322143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922121286392212 3.1829683780670166 33.62189483642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902441024780273 2.9860916137695312 31.651161193847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789878249168396 2.6559786796569824 28.349666595458984
  batch 20 loss: 1.789878249168396, 2.6559786796569824, 28.349666595458984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790879487991333 2.9897186756134033 31.688066482543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934350967407227 3.224921941757202 34.04265213012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937976121902466 2.8839035034179688 30.63283348083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916827201843262 3.048222064971924 32.273902893066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904642820358276 3.1558499336242676 33.348960876464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902987003326416 3.1843888759613037 33.63418960571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891287803649902 3.2682275772094727 34.47140121459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911626100540161 2.939025640487671 31.181419372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7887234687805176 2.8837246894836426 30.62596893310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905457019805908 3.639540433883667 38.185951232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789534568786621 2.674456834793091 28.534103393554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917842864990234 2.785670042037964 29.64848518371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915427684783936 3.129084825515747 33.08238983154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.985856056213379 31.650419235229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920845746994019 2.8538689613342285 30.330772399902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911052703857422 3.7441153526306152 39.232261657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7895376682281494 3.246256113052368 34.25210189819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928718328475952 2.9073970317840576 30.86684226989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914570569992065 2.9804725646972656 31.596181869506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793766975402832 3.298609495162964 34.77986145019531
  batch 40 loss: 1.793766975402832, 3.298609495162964, 34.77986145019531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913752794265747 3.5844790935516357 37.636165618896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792231798171997 3.260100841522217 34.39324188232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792501449584961 3.41911244392395 35.98362731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924119234085083 3.4268484115600586 36.06089782714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790230393409729 3.124805450439453 33.03828430175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921217679977417 3.2883546352386475 34.67566680908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927041053771973 3.2125966548919678 33.918670654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928454875946045 3.7192752361297607 38.985599517822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923394441604614 3.5546722412109375 37.33906173706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914197444915771 3.193154811859131 33.72296905517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922439575195312 3.268270969390869 34.474952697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921643257141113 3.5540993213653564 37.33315658569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934961318969727 3.363555669784546 35.42905044555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920525074005127 3.5692033767700195 37.48408508300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922651767730713 2.949841022491455 31.290674209594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917096614837646 3.3295271396636963 35.08698272705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908903360366821 3.7148783206939697 38.939674377441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903848886489868 3.225112199783325 34.041507720947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7888461351394653 3.2470250129699707 34.25909423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789775013923645 2.652644157409668 28.31621742248535
  batch 60 loss: 1.789775013923645, 2.652644157409668, 28.31621742248535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892074584960938 3.487267017364502 36.6618766784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923052310943604 3.6779086589813232 38.57139205932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790469765663147 3.4816625118255615 36.607093811035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912132740020752 3.9998767375946045 41.789981842041016
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7895541191101074 2.394890069961548 25.738454818725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923262119293213 2.934382200241089 31.13614845275879
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791795253753662 3.22043776512146 33.99617004394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.9266061782836914 31.058637619018555
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7922313213348389 3.312408447265625 34.916316986083984
Total LOSS train 33.13918125446026 valid 32.77681827545166
CE LOSS train 1.7913240102621226 valid 0.4480578303337097
Contrastive LOSS train 3.1347857291881853 valid 0.8281021118164062
EPOCH 68:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916672229766846 2.732783317565918 29.1195011138916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942917346954346 3.2455027103424072 34.24932098388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919187545776367 3.322841167449951 35.02033233642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790532112121582 2.7651190757751465 29.441722869873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919249534606934 3.115114450454712 32.94306945800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901911735534668 2.6487185955047607 28.27737808227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941557168960571 3.0854294300079346 32.6484489440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944958209991455 2.898061513900757 30.77511215209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939860820770264 3.047678232192993 32.27076721191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928521633148193 2.927645683288574 31.06930923461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897109985351562 2.738903760910034 29.178749084472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903757095336914 3.9397389888763428 41.18776321411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904714345932007 3.313173770904541 34.92220687866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907607555389404 3.1786141395568848 33.576900482177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940665483474731 3.5682480335235596 37.47654724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906858921051025 3.7715795040130615 39.5064811706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791457176208496 2.9923880100250244 31.715335845947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930387258529663 3.6622984409332275 38.41602325439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911258935928345 2.772765636444092 29.518783569335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920129299163818 2.433363199234009 26.12564468383789
  batch 20 loss: 1.7920129299163818, 2.433363199234009, 26.12564468383789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934950590133667 3.462468385696411 36.41817855834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909331321716309 3.4978065490722656 36.76899719238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919611930847168 2.8255984783172607 30.04794692993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916189432144165 3.5619301795959473 37.410919189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909303903579712 3.495845079421997 36.74938201904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911608219146729 3.3239502906799316 35.030662536621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909822463989258 3.169222116470337 33.48320388793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940654754638672 3.5870578289031982 37.664642333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789933681488037 3.4889895915985107 36.67982864379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920750379562378 3.4728686809539795 36.5207633972168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908209562301636 3.8432304859161377 40.22312545776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792065978050232 2.8589365482330322 30.381431579589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790807843208313 3.006920099258423 31.860008239746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911157608032227 2.9607996940612793 31.399112701416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902663946151733 2.950212001800537 31.292387008666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917711734771729 3.909595251083374 40.88772201538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789976716041565 3.145026683807373 33.2402458190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926342487335205 2.8565571308135986 30.358205795288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910867929458618 3.0933005809783936 32.72409439086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933740615844727 3.4423305988311768 36.216678619384766
  batch 40 loss: 1.7933740615844727, 3.4423305988311768, 36.216678619384766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930163145065308 2.9359307289123535 31.15232276916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929540872573853 2.9682328701019287 31.475282669067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927992343902588 3.1312592029571533 33.10539245605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927534580230713 3.158041477203369 33.3731689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892601490020752 3.1378321647644043 33.16758346557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897615432739258 2.9558403491973877 31.348163604736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7893229722976685 3.088852882385254 32.677852630615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892217636108398 3.3261871337890625 35.05109405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612148284912 3.376788854598999 35.55949783325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920187711715698 2.9796254634857178 31.588274002075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940397262573242 3.340459108352661 35.198631286621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927569150924683 3.748507022857666 39.27782440185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920985221862793 3.3916916847229004 35.709014892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79080331325531 3.277961015701294 34.570411682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915335893630981 3.313636064529419 34.927894592285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790568232536316 3.782562494277954 39.61619567871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900440692901611 3.1871588230133057 33.6616325378418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902787923812866 3.0135092735290527 31.925373077392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7889600992202759 3.2563323974609375 34.3522834777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907929420471191 2.906093120574951 30.85172462463379
  batch 60 loss: 1.7907929420471191, 2.906093120574951, 30.85172462463379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913264036178589 3.2798855304718018 34.590179443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909404039382935 2.9559905529022217 31.350847244262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908118963241577 2.8187508583068848 29.97831916809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901561260223389 2.917630434036255 30.966459274291992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7910763025283813 3.0757532119750977 32.548606872558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922900915145874 3.359419822692871 35.38648986816406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793880581855774 3.301651954650879 34.81039810180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938958406448364 3.218979835510254 33.98369216918945
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919648885726929 3.3750624656677246 35.5425910949707
Total LOSS train 33.70493792020358 valid 34.930792808532715
CE LOSS train 1.7915339304850653 valid 0.4479912221431732
Contrastive LOSS train 3.1913404097923865 valid 0.8437656164169312
EPOCH 69:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918108701705933 3.1524722576141357 33.316532135009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937594652175903 3.3054771423339844 34.84852981567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910860776901245 2.877086639404297 30.561952590942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902612686157227 3.2721216678619385 34.511478424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916693687438965 3.58272123336792 37.61888122558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894655466079712 3.609055757522583 37.88002395629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792885661125183 3.317096710205078 34.96385192871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921375036239624 3.0624380111694336 32.41651916503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918212413787842 2.798832893371582 29.780149459838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793650507926941 3.171062469482422 33.504276275634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905550003051758 3.083254814147949 32.623104095458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921534776687622 3.6939682960510254 38.731834411621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907315492630005 2.775014638900757 29.540878295898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908464670181274 3.1346142292022705 33.13698959350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927626371383667 3.3829538822174072 35.6223030090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900768518447876 3.616065502166748 37.95073318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902722358703613 3.7851226329803467 39.64149856567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925883531570435 3.3194541931152344 34.98712921142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928107976913452 3.559788942337036 37.39070129394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913532257080078 3.008117914199829 31.87253189086914
  batch 20 loss: 1.7913532257080078, 3.008117914199829, 31.87253189086914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911944389343262 2.5902817249298096 27.694011688232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904196977615356 2.7525534629821777 29.315956115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79128897190094 3.5087382793426514 36.8786735534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790698766708374 3.375445604324341 35.5451545715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907577753067017 3.8662712574005127 40.453468322753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790873646736145 3.6356606483459473 38.14748001098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921645641326904 3.015692949295044 31.949092864990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939655780792236 3.3201797008514404 34.99576187133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909787893295288 3.3964123725891113 35.75510025024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904045581817627 4.039566993713379 42.186073303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788205623626709 2.8456571102142334 30.24477767944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914677858352661 3.627115488052368 38.062625885009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891074419021606 3.501551866531372 36.80462646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7893426418304443 3.2697720527648926 34.487060546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902251482009888 3.4559717178344727 36.34994125366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924983501434326 3.760301351547241 39.395511627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793353796005249 2.9176764488220215 30.97011947631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7945805788040161 2.5670125484466553 27.464706420898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916373014450073 3.194791316986084 33.73955154418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920258045196533 3.2851970195770264 34.64399719238281
  batch 40 loss: 1.7920258045196533, 3.2851970195770264, 34.64399719238281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922779321670532 3.2710468769073486 34.50274658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905588150024414 2.9184958934783936 30.97551727294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792532205581665 3.3352763652801514 35.14529800415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793846607208252 3.364260196685791 35.43644714355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190993309021 3.481025218963623 36.6021614074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921143770217896 3.1208271980285645 33.00038528442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913544178009033 3.2617948055267334 34.4093017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916069030761719 3.9196255207061768 40.98786163330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943326234817505 3.6240720748901367 38.035057067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792277216911316 3.009645700454712 31.88873291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935441732406616 3.4868874549865723 36.662418365478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919621467590332 3.6867823600769043 38.659786224365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919217348098755 3.3619234561920166 35.41115951538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908567190170288 3.3853657245635986 35.64451217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790936827659607 3.259423017501831 34.38516616821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900136709213257 3.3761308193206787 35.55131912231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900233268737793 3.5408146381378174 37.19816970825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919261455535889 3.462048292160034 36.412410736083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905875444412231 3.58174729347229 37.608062744140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790864109992981 2.9397459030151367 31.188323974609375
  batch 60 loss: 1.790864109992981, 2.9397459030151367, 31.188323974609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791364073753357 3.114039897918701 32.9317626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793336033821106 3.1358442306518555 33.15177917480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919647693634033 2.7966995239257812 29.758960723876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921303510665894 3.5588624477386475 37.38075256347656
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.790798306465149 2.383277654647827 25.62357521057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917759418487549 3.1711575984954834 33.503353118896484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791804313659668 3.796006441116333 39.751869201660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921102046966553 3.595431327819824 37.74642562866211
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791454792022705 3.349395990371704 35.28541564941406
Total LOSS train 34.68515780522273 valid 36.5717658996582
CE LOSS train 1.791583543557387 valid 0.44786369800567627
Contrastive LOSS train 3.2893574201143707 valid 0.837348997592926
EPOCH 70:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911159992218018 3.879077911376953 40.58189392089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930282354354858 3.275106191635132 34.544090270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912296056747437 3.223158121109009 34.02281188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908320426940918 3.3872523307800293 35.66335678100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791684865951538 3.308479070663452 34.87647247314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907817363739014 3.161226511001587 33.403045654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924836874008179 3.2569780349731445 34.36226272583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927663326263428 3.5990679264068604 37.783447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920619249343872 3.0880463123321533 32.672523498535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792104959487915 3.0895283222198486 32.6873893737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789537787437439 3.6877920627593994 38.66746139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910280227661133 3.3494977951049805 35.286006927490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910501956939697 2.7761809825897217 29.552860260009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925769090652466 3.078204393386841 32.57461929321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944828271865845 3.308458089828491 34.87906265258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918055057525635 3.52777361869812 37.069541931152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912218570709229 3.833946943283081 40.13069152832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791542649269104 3.2881269454956055 34.672813415527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790355920791626 3.5430667400360107 37.22102355957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913957834243774 3.047513246536255 32.26652908325195
  batch 20 loss: 1.7913957834243774, 3.047513246536255, 32.26652908325195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936216592788696 2.919189214706421 30.98551368713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921391725540161 3.053023099899292 32.32237243652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928320169448853 3.6015233993530273 37.808067321777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910326719284058 2.9972503185272217 31.76353645324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905775308609009 3.3655405044555664 35.445980072021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915321588516235 3.6154396533966064 37.945926666259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906429767608643 3.311758518218994 34.908226013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931851148605347 3.410447835922241 35.89766311645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899483442306519 3.75026798248291 39.292625427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791942834854126 3.615601062774658 37.94795227050781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904099225997925 3.6060190200805664 37.8505973815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926688194274902 3.4220688343048096 36.01335525512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913298606872559 3.267455577850342 34.465885162353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903077602386475 4.080552101135254 42.5958251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790004014968872 3.4870693683624268 36.66069793701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79123854637146 3.213923215866089 33.93046951293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912243604660034 3.760354518890381 39.394771575927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928379774093628 3.585716724395752 37.65000534057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912218570709229 3.66961932182312 38.4874153137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793587565422058 3.525294303894043 37.046531677246094
  batch 40 loss: 1.793587565422058, 3.525294303894043, 37.046531677246094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922842502593994 3.0368638038635254 32.16092300415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792154312133789 3.1647303104400635 33.439456939697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921814918518066 3.4734179973602295 36.526363372802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921873331069946 3.480046272277832 36.592647552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906190156936646 3.115717649459839 32.94779586791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917019128799438 3.0993573665618896 32.785274505615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924084663391113 3.558746814727783 37.37987518310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927615642547607 4.243239402770996 44.22515869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794026494026184 4.3252854347229 45.046878814697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910487651824951 3.44566011428833 36.247650146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924082279205322 2.629611015319824 28.088518142700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920161485671997 3.0589115619659424 32.38113021850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925721406936646 3.3538849353790283 35.3314208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914544343948364 3.6720130443573 38.5115852355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905634641647339 3.430112361907959 36.0916862487793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900428771972656 2.917495012283325 30.96499252319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894624471664429 3.202059030532837 33.810054779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908647060394287 3.5383176803588867 37.174041748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789438247680664 3.0967788696289062 32.757225036621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899274826049805 3.0902416706085205 32.692344665527344
  batch 60 loss: 1.7899274826049805, 3.0902416706085205, 32.692344665527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894971370697021 4.031705379486084 42.10655212402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916982173919678 3.241042375564575 34.20212173461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913384437561035 3.1100356578826904 32.89169692993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925920486450195 3.5302011966705322 37.0946044921875
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.791351318359375 3.6101009845733643 37.89236068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921910285949707 2.8758459091186523 30.550649642944336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792083501815796 2.6984410285949707 28.776493072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924144268035889 3.001068115234375 31.8030948638916
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7925069332122803 3.085712432861328 32.64963150024414
Total LOSS train 35.70267201937162 valid 30.94496726989746
CE LOSS train 1.791568814791166 valid 0.44812673330307007
Contrastive LOSS train 3.3911103395315316 valid 0.771428108215332
EPOCH 71:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918179035186768 2.9447593688964844 31.239412307739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928215265274048 3.2126355171203613 33.91917419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907065153121948 2.863360643386841 30.424312591552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906705141067505 3.075345754623413 32.54412841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790748119354248 2.36051082611084 25.395856857299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896735668182373 2.378936290740967 25.579036712646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923393249511719 3.3169376850128174 34.96171569824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916805744171143 2.3116791248321533 24.908472061157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917437553405762 2.486151695251465 26.653261184692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925368547439575 3.2894322872161865 34.686859130859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901734113693237 3.241826295852661 34.20843505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791214942932129 2.9702844619750977 31.494060516357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791223168373108 2.5851919651031494 27.643142700195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922046184539795 2.594236135482788 27.73456573486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 3.0244431495666504 32.03724670410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789128303527832 2.9526302814483643 31.3154296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894327640533447 2.603290557861328 27.822338104248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791412591934204 2.463124990463257 26.42266273498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904876470565796 3.0791001319885254 32.58148956298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914754152297974 2.4639620780944824 26.43109703063965
  batch 20 loss: 1.7914754152297974, 2.4639620780944824, 26.43109703063965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918566465377808 1.7969392538070679 19.761249542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913670539855957 2.9031999111175537 30.823366165161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918329238891602 2.864231586456299 30.434146881103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903248071670532 2.6512162685394287 28.302488327026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905222177505493 2.7511379718780518 29.30190086364746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903625965118408 2.9338042736053467 31.128406524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789936900138855 3.1858041286468506 33.647979736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918916940689087 2.9232394695281982 31.0242862701416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789605736732483 3.0658679008483887 32.44828414916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915347814559937 2.9724810123443604 31.51634407043457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 3.280851125717163 34.60023880004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936217784881592 2.649538278579712 28.289003372192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915626764297485 3.1850154399871826 33.64171600341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790819525718689 3.066380500793457 32.45462417602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790347933769226 3.1550800800323486 33.341148376464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79087495803833 3.3553152084350586 35.34402847290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905126810073853 3.687828779220581 38.668800354003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925200462341309 3.0673394203186035 32.465911865234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917221784591675 3.439716339111328 36.18888473510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928884029388428 3.358304738998413 35.37593460083008
  batch 40 loss: 1.7928884029388428, 3.358304738998413, 35.37593460083008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79176664352417 3.475283145904541 36.54459762573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916431427001953 2.941908359527588 31.210725784301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791998267173767 3.32875919342041 35.07958984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935088872909546 3.3894917964935303 35.68842697143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79062819480896 2.634006977081299 28.13069725036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923023700714111 3.135714292526245 33.149444580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921735048294067 3.197273015975952 33.7649040222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923521995544434 3.712907552719116 38.921424865722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793357491493225 3.5619165897369385 37.41252517700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170524597168 2.470310926437378 26.495281219482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924840450286865 3.5202207565307617 36.994693756103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916700839996338 3.8086352348327637 39.87802505493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791985034942627 3.657625913619995 38.36824417114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910188436508179 2.9976327419281006 31.76734733581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79081130027771 2.980750322341919 31.59831428527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906527519226074 2.5048210620880127 26.838863372802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790861964225769 3.383446216583252 35.62532424926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790724277496338 3.5078442096710205 36.86916732788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898553609848022 2.9911181926727295 31.701038360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912884950637817 3.555327892303467 37.34457015991211
  batch 60 loss: 1.7912884950637817, 3.555327892303467, 37.34457015991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790461540222168 3.7904675006866455 39.69513702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791597604751587 3.1282882690429688 33.07448196411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908693552017212 3.3580167293548584 35.371036529541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918781042099 2.984192132949829 31.633798599243164
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7907706499099731 2.9523727893829346 31.314498901367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790618658065796 3.0787737369537354 32.57835388183594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902573347091675 3.1058402061462402 32.84865951538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911020517349243 3.147022247314453 33.26132583618164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7908183336257935 2.836125135421753 30.152070999145508
Total LOSS train 32.172824595524716 valid 32.210102558135986
CE LOSS train 1.7913688512948842 valid 0.44770458340644836
Contrastive LOSS train 3.038145580658546 valid 0.7090312838554382
EPOCH 72:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903326749801636 2.939558982849121 31.185922622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929069995880127 3.412621259689331 35.91912078857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913672924041748 3.262580156326294 34.41716766357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791048526763916 3.0905449390411377 32.69649887084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912105321884155 3.230013370513916 34.09134292602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788854956626892 3.288024663925171 34.66910171508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 3.889705181121826 40.68892288208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921481132507324 3.136981248855591 33.16196060180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931543588638306 3.3643641471862793 35.436798095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939834594726562 3.561549425125122 37.40947723388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913622856140137 3.3062925338745117 34.85429000854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912763357162476 3.272557020187378 34.516845703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790187120437622 3.2465016841888428 34.25520324707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904194593429565 3.4588537216186523 36.37895965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929472923278809 3.2879269123077393 34.67221450805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911579608917236 3.682077646255493 38.611934661865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916251420974731 3.700976848602295 38.801395416259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923483848571777 3.858950614929199 40.38185501098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911760807037354 3.499850273132324 36.78968048095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908722162246704 3.256803274154663 34.35890197753906
  batch 20 loss: 1.7908722162246704, 3.256803274154663, 34.35890197753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896831035614014 2.958195686340332 31.371639251708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900350093841553 3.0120561122894287 31.91059684753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914211750030518 2.7325453758239746 29.11687660217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906692028045654 3.2665531635284424 34.456199645996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922264337539673 3.457505941390991 36.36728286743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917978763580322 3.6391525268554688 38.18332290649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791386604309082 3.4902291297912598 36.69367599487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917816638946533 3.4756040573120117 36.54782485961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7882634401321411 3.3382527828216553 35.17079162597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909607887268066 3.4353246688842773 36.14421081542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7890141010284424 3.179622173309326 33.585235595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921334505081177 3.3236944675445557 35.02907943725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899115085601807 3.1404178142547607 33.194091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904366254806519 3.4454731941223145 36.24516677856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905573844909668 3.1784982681274414 33.575538635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908878326416016 3.6258692741394043 38.049583435058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911494970321655 3.660445213317871 38.39560317993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923837900161743 3.524730682373047 37.03969192504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911936044692993 3.177304744720459 33.56424331665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929282188415527 3.333991527557373 35.132843017578125
  batch 40 loss: 1.7929282188415527, 3.333991527557373, 35.132843017578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921702861785889 3.123049736022949 33.022666931152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791409969329834 3.9260332584381104 41.05174255371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790880560874939 3.401545286178589 35.80633544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920303344726562 2.8953042030334473 30.745071411132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902193069458008 2.827509641647339 30.06531524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791006088256836 3.351958990097046 35.31059265136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909523248672485 3.0015065670013428 31.806018829345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791146159172058 3.326738119125366 35.05852508544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927355766296387 2.8613650798797607 30.406387329101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918580770492554 3.971606492996216 41.5079231262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930445671081543 2.853088140487671 30.32392692565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792235255241394 3.164827585220337 33.44050979614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926390171051025 2.7362916469573975 29.155555725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913838624954224 2.8892712593078613 30.68409538269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912517786026 2.939316987991333 31.18442153930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899726629257202 3.0655481815338135 32.44545364379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7889753580093384 3.2368743419647217 34.157718658447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789039969444275 3.914423942565918 40.93328094482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789198875427246 3.856517791748047 40.35437774658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906739711761475 3.877925157546997 40.569923400878906
  batch 60 loss: 1.7906739711761475, 3.877925157546997, 40.569923400878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901369333267212 3.273329496383667 34.523433685302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911381721496582 3.0550005435943604 32.34114456176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905595302581787 2.941148042678833 31.20203971862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911386489868164 3.079066753387451 32.58180618286133
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.790232539176941 2.7877087593078613 29.66731834411621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792110800743103 3.2049753665924072 33.84186553955078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919498682022095 2.564133882522583 27.43328857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922019958496094 3.187851905822754 33.67072296142578
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792134165763855 2.3992056846618652 25.784191131591797
Total LOSS train 34.791025807307314 valid 30.182517051696777
CE LOSS train 1.7911553914730365 valid 0.44803354144096375
Contrastive LOSS train 3.2999870263613187 valid 0.5998014211654663
EPOCH 73:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914657592773438 3.47481632232666 36.53962707519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792800784111023 3.4585957527160645 36.37875747680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792014241218567 3.3829433917999268 35.6214485168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915973663330078 3.381505012512207 35.60664367675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918872833251953 3.5838491916656494 37.63037872314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892646789550781 3.6062607765197754 37.851871490478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 3.8944597244262695 40.736202239990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906743288040161 3.144831657409668 33.238990783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914650440216064 2.823071241378784 30.022178649902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792197585105896 2.9192283153533936 30.984481811523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908574342727661 2.8066060543060303 29.856918334960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921991348266602 3.239776372909546 34.18996047973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924259901046753 3.2059717178344727 33.852142333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922519445419312 3.7388811111450195 39.181060791015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930412292480469 3.248889923095703 34.28194046020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903996706008911 3.525423526763916 37.04463577270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788928747177124 3.6244919300079346 38.03384780883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928931713104248 3.3717575073242188 35.510467529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914209365844727 3.3135087490081787 34.92650604248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907882928848267 2.3971612453460693 25.762401580810547
  batch 20 loss: 1.7907882928848267, 2.3971612453460693, 25.762401580810547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900813817977905 2.4874861240386963 26.664941787719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899658679962158 3.321690797805786 35.006874084472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901992797851562 2.711927652359009 28.909475326538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7890886068344116 3.157357692718506 33.362667083740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904471158981323 3.852893829345703 40.31938552856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918452024459839 3.0931522846221924 32.723365783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917448282241821 2.8786346912384033 30.57809066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926571369171143 2.8864285945892334 30.656944274902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.788710117340088 3.2539405822753906 34.32811737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904821634292603 2.9761037826538086 31.5515193939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897889614105225 2.7419822216033936 29.209611892700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929933071136475 2.952813148498535 31.321125030517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908974885940552 2.863964796066284 30.430545806884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792009711265564 3.4414479732513428 36.20648956298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923519611358643 2.8378188610076904 30.170541763305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 3.2628402709960938 34.421112060546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916789054870605 3.4319756031036377 36.11143493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926770448684692 3.201246738433838 33.805145263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899404764175415 2.972001791000366 31.509958267211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915486097335815 4.120500564575195 42.99655532836914
  batch 40 loss: 1.7915486097335815, 4.120500564575195, 42.99655532836914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899672985076904 3.1083643436431885 32.87361145019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899466753005981 2.686455011367798 28.654497146606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894788980484009 3.20129656791687 33.80244445800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919765710830688 3.418423652648926 35.97621154785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913389205932617 2.8645901679992676 30.437240600585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919254302978516 3.3898863792419434 35.69078826904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909117937088013 3.3132100105285645 34.923011779785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792088508605957 3.0822560787200928 32.61465072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929459810256958 2.7338404655456543 29.131351470947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919213771820068 2.9555962085723877 31.347883224487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914265394210815 2.8666810989379883 30.458236694335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907202243804932 3.616288185119629 37.9536018371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911975383758545 3.467643976211548 36.4676399230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907260656356812 3.7973594665527344 39.764320373535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791318416595459 3.190046787261963 33.6917839050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908918857574463 3.5916595458984375 37.707489013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910181283950806 3.752041816711426 39.31143569946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918037176132202 3.5183522701263428 36.97532653808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899389266967773 3.4236745834350586 36.02668762207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902714014053345 2.9568734169006348 31.359004974365234
  batch 60 loss: 1.7902714014053345, 2.9568734169006348, 31.359004974365234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896848917007446 2.594804525375366 27.737730026245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791233777999878 2.6028990745544434 27.82022476196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909537553787231 3.4163501262664795 35.9544563293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915412187576294 3.8567392826080322 40.35893630981445
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7899755239486694 2.3310067653656006 25.10004425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912102937698364 3.4079768657684326 35.87097930908203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914294004440308 3.6816866397857666 38.60829544067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916003465652466 2.875621795654297 30.547819137573242
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7910254001617432 3.2158119678497314 33.94914627075195
Total LOSS train 33.84112264193021 valid 34.744060039520264
CE LOSS train 1.7911876696806688 valid 0.4477563500404358
Contrastive LOSS train 3.204993497408353 valid 0.8039529919624329
EPOCH 74:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790542483329773 3.544818162918091 37.23872375488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929011583328247 3.5693583488464355 37.48648452758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791244626045227 3.5208096504211426 36.99934005737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902276515960693 3.1043484210968018 32.833709716796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910395860671997 3.073446035385132 32.5255012512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7892454862594604 3.1211612224578857 33.000858306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932809591293335 3.6301050186157227 38.094329833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792722225189209 3.6795570850372314 38.58829116821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927285432815552 3.2827723026275635 34.620452880859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917526960372925 3.649146795272827 38.28321838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891114950180054 3.0324134826660156 32.11324691772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789749264717102 3.6326658725738525 38.11640930175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894259691238403 3.410273790359497 35.89216232299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897268533706665 3.0566670894622803 32.35639953613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923104763031006 3.181490898132324 33.60721969604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908554077148438 4.2044596672058105 43.835453033447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913514375686646 3.301589250564575 34.80724334716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792639136314392 3.0435497760772705 32.2281379699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913647890090942 3.0783591270446777 32.57495880126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791296362876892 2.8056249618530273 29.847545623779297
  batch 20 loss: 1.791296362876892, 2.8056249618530273, 29.847545623779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903945446014404 2.803328514099121 29.823678970336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789867877960205 3.165215253829956 33.442020416259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912331819534302 3.462758779525757 36.418819427490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907835245132446 2.978706121444702 31.577844619750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79081130027771 3.5281012058258057 37.07182312011719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908692359924316 3.704409122467041 38.8349609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900351285934448 3.770759105682373 39.49762725830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917840480804443 3.448700189590454 36.278785705566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789854645729065 4.018828868865967 41.978145599365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906781435012817 3.63601016998291 38.150779724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903200387954712 2.7532613277435303 29.322933197021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931287288665771 3.71396803855896 38.93280792236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790500283241272 4.030340194702148 42.093902587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902389764785767 3.9312615394592285 41.10285186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908260822296143 3.5631165504455566 37.42198944091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912088632583618 3.391413927078247 35.70534896850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908729314804077 3.6897125244140625 38.6879997253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918736934661865 3.1219282150268555 33.01115417480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901567220687866 3.654934883117676 38.33950424194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916598320007324 3.141864538192749 33.210304260253906
  batch 40 loss: 1.7916598320007324, 3.141864538192749, 33.210304260253906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897077798843384 2.5642287731170654 27.431995391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900919914245605 2.7012200355529785 28.802291870117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912664413452148 2.7955853939056396 29.747119903564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793094277381897 3.1467206478118896 33.26029968261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913262844085693 2.8066415786743164 29.857742309570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048692703247 2.947160482406616 31.263654708862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912352085113525 3.3439879417419434 35.231117248535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910181283950806 3.6728670597076416 38.519691467285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79213547706604 2.7334415912628174 29.126550674438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908594608306885 2.4802086353302 26.592945098876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920279502868652 2.665327787399292 28.4453067779541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911239862442017 3.2380247116088867 34.17137145996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917877435684204 2.889347791671753 30.685266494750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917766571044922 3.4513211250305176 36.30498504638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792008399963379 3.352146625518799 35.313472747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790819525718689 3.1611745357513428 33.402565002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896535396575928 2.9333105087280273 31.122758865356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906432151794434 2.9916646480560303 31.707290649414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897692918777466 3.123955726623535 33.029327392578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899359464645386 2.6290295124053955 28.080230712890625
  batch 60 loss: 1.7899359464645386, 2.6290295124053955, 28.080230712890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899844646453857 3.0676627159118652 32.46661376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916948795318604 2.5534842014312744 27.326536178588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790327787399292 3.3329262733459473 35.119590759277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909008264541626 2.728506326675415 29.075963973999023
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.790858268737793 2.939584970474243 31.18670654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917388677597046 2.5790905952453613 27.582643508911133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916526794433594 2.6522183418273926 28.31383514404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919071912765503 2.480280876159668 26.594715118408203
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918142080307007 2.3208725452423096 25.000539779663086
Total LOSS train 34.203451743492714 valid 26.872933387756348
CE LOSS train 1.7910263171562781 valid 0.44795355200767517
Contrastive LOSS train 3.241242548135611 valid 0.5802181363105774
Saved best model. Old loss 27.167819023132324 and new best loss 26.872933387756348
EPOCH 75:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791164755821228 2.642177104949951 28.212936401367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921355962753296 3.0563876628875732 32.356014251708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907742261886597 3.085820198059082 32.64897537231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906523942947388 2.9115400314331055 30.90605354309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917786836624146 3.4054133892059326 35.84591293334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900047302246094 3.572165012359619 37.511653900146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916357517242432 3.099315881729126 32.784793853759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909778356552124 3.441746711730957 36.20844268798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918202877044678 3.1611342430114746 33.40316390991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919679880142212 3.8224587440490723 40.01655578613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904753684997559 3.2028145790100098 33.81861877441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907116413116455 3.543614149093628 37.22685241699219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903093099594116 2.975738048553467 31.547691345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902683019638062 3.0683677196502686 32.47394561767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925211191177368 3.8509271144866943 40.30179214477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901129722595215 3.8086023330688477 39.87613296508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898974418640137 3.0396013259887695 32.185909271240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915621995925903 2.8673477172851562 30.46504020690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917191982269287 3.414719343185425 35.93891143798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79205322265625 3.267962694168091 34.4716796875
  batch 20 loss: 1.79205322265625, 3.267962694168091, 34.4716796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908525466918945 2.9435362815856934 31.226215362548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902027368545532 2.632474660873413 28.11495018005371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909960746765137 2.870028495788574 30.491281509399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906652688980103 2.3921868801116943 25.712533950805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790713906288147 3.259150505065918 34.38222122192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905538082122803 3.688062906265259 38.67118453979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790515661239624 3.319096326828003 34.98147964477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920117378234863 3.782527208328247 39.61728286743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896560430526733 3.6987814903259277 38.77747344970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907273769378662 3.45589280128479 36.34965515136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790138840675354 2.849778652191162 30.287925720214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924646139144897 3.4548652172088623 36.34111785888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790221929550171 3.4648125171661377 36.43834686279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900086641311646 3.3721892833709717 35.51190185546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791013479232788 3.23416805267334 34.132694244384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791014313697815 3.474189043045044 36.53290557861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906696796417236 3.7522435188293457 39.31310272216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919518947601318 2.8558273315429688 30.3502254486084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908835411071777 3.3821725845336914 35.612606048583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923341989517212 2.8239376544952393 30.031709671020508
  batch 40 loss: 1.7923341989517212, 2.8239376544952393, 30.031709671020508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908855676651 2.5984487533569336 27.775373458862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79119873046875 3.343366861343384 35.22486877441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912360429763794 3.4114153385162354 35.905391693115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925457954406738 3.543221950531006 37.22476577758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790937066078186 2.4527907371520996 26.318845748901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914049625396729 2.7249629497528076 29.041034698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909573316574097 2.7133560180664062 28.924516677856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909907102584839 3.673034429550171 38.521331787109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925609350204468 3.2553486824035645 34.346046447753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678952217102 2.992793083190918 31.720609664916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927823066711426 2.9130685329437256 30.9234676361084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909764051437378 3.107332706451416 32.86430358886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913954257965088 3.309194803237915 34.88334655761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909356355667114 3.646982431411743 38.26076126098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917330265045166 3.524240016937256 37.03413391113281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925336360931396 3.026062488555908 32.053157806396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922333478927612 3.2719461917877197 34.511695861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791988730430603 3.220858335494995 34.000572204589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790287733078003 3.250030994415283 34.29059600830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791015625 3.538266658782959 37.173683166503906
  batch 60 loss: 1.791015625, 3.538266658782959, 37.173683166503906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7897595167160034 3.5073282718658447 36.86304473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909351587295532 3.0946974754333496 32.737911224365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912310361862183 3.3902957439422607 35.69418716430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925019264221191 3.5798003673553467 37.59050750732422
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7901333570480347 2.426788806915283 26.058019638061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912391424179077 3.9895436763763428 41.686676025390625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910046577453613 3.361276865005493 35.40377426147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911818027496338 3.788860559463501 39.67979049682617
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791033148765564 3.632194995880127 38.11298370361328
Total LOSS train 34.015693283081056 valid 38.72080612182617
CE LOSS train 1.7911688969685482 valid 0.447758287191391
Contrastive LOSS train 3.2224524314586933 valid 0.9080487489700317
EPOCH 76:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904845476150513 2.937009334564209 31.160579681396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914965152740479 3.4222488403320312 36.01398468017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912968397140503 3.2328851222991943 34.120147705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913719415664673 3.407336950302124 35.86473846435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792253851890564 2.956063747406006 31.35289192199707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917425632476807 3.291576623916626 34.70751190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920935153961182 2.7442445755004883 29.234539031982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913098335266113 2.67400860786438 28.531394958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918092012405396 2.900116205215454 30.792970657348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921867370605469 3.3356144428253174 35.14833068847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909883260726929 2.9899075031280518 31.6900634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914403676986694 3.2459592819213867 34.251033782958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790995478630066 3.401735782623291 35.808353424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896322011947632 3.0978500843048096 32.768131256103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912242412567139 2.7678773403167725 29.46999740600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896454334259033 3.4911513328552246 36.7011604309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7887084484100342 3.3680572509765625 35.46928024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911276817321777 3.615757942199707 37.94870376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913150787353516 3.158721685409546 33.37853240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792244791984558 2.8139138221740723 29.931381225585938
  batch 20 loss: 1.792244791984558, 2.8139138221740723, 29.931381225585938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928377389907837 2.8604049682617188 30.396886825561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913142442703247 3.291447401046753 34.705787658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912248373031616 2.6677958965301514 28.46918487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894694805145264 2.792015552520752 29.709623336791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906742095947266 3.13754940032959 33.166168212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908823490142822 3.470811128616333 36.498992919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911088466644287 3.946302890777588 41.25413513183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919014692306519 3.3145945072174072 34.93784713745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7893047332763672 2.8042471408843994 29.831775665283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790653109550476 2.835057020187378 30.141223907470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909966707229614 2.6289687156677246 28.080684661865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927552461624146 3.302572011947632 34.81847381591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909845113754272 2.4488754272460938 26.279739379882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911192178726196 2.437678813934326 26.16790771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909295558929443 3.616337537765503 37.95430374145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909913063049316 3.240025520324707 34.191246032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908955812454224 3.289597272872925 34.68687057495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928601503372192 3.5267715454101562 37.060577392578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920998334884644 4.264638423919678 44.43848419189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923614978790283 4.007376194000244 41.86612319946289
  batch 40 loss: 1.7923614978790283, 4.007376194000244, 41.86612319946289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791139006614685 2.8795340061187744 30.58647918701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910465002059937 3.6872658729553223 38.66370391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919063568115234 3.300978422164917 34.80168914794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922426462173462 2.9397661685943604 31.189903259277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917108535766602 3.8602547645568848 40.394256591796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924946546554565 3.829087257385254 40.08336639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918152809143066 4.020650863647461 41.99832534790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917592525482178 3.7571613788604736 39.363372802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934650182724 3.1665263175964355 33.4587287902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920199632644653 3.0415890216827393 32.207908630371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921570539474487 3.0777699947357178 32.56985855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791237711906433 3.0384650230407715 32.17588806152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911348342895508 2.6126320362091064 27.91745376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913618087768555 2.7458956241607666 29.250316619873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919600009918213 3.2821807861328125 34.61376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912031412124634 2.9864141941070557 31.655344009399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898956537246704 3.138707160949707 33.17696762084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790600061416626 3.1793243885040283 33.58384323120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903788089752197 3.1037325859069824 32.82770538330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912116050720215 2.8445305824279785 30.23651695251465
  batch 60 loss: 1.7912116050720215, 2.8445305824279785, 30.23651695251465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790987491607666 3.3166840076446533 34.957828521728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926137447357178 2.838963747024536 30.1822509765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920124530792236 2.794100522994995 29.73301887512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923184633255005 2.8434484004974365 30.226802825927734
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7920210361480713 2.7686729431152344 29.478750228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925344705581665 2.912343740463257 30.915971755981445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254949092865 3.042494297027588 32.217491149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923606634140015 3.4143431186676025 35.935794830322266
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792302131652832 2.8372716903686523 30.165019989013672
Total LOSS train 33.45128942636343 valid 32.30856943130493
CE LOSS train 1.7913757782716018 valid 0.448075532913208
Contrastive LOSS train 3.1659913833324724 valid 0.7093179225921631
EPOCH 77:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918871641159058 2.642662763595581 28.218515396118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923142910003662 4.021615028381348 42.008460998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790250301361084 2.9793195724487305 31.583446502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894401550292969 3.4913299083709717 36.70273971557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905974388122559 3.863574504852295 40.42634201049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7889398336410522 3.470475196838379 36.493690490722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914841175079346 3.563939094543457 37.43087387084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916489839553833 3.062878370285034 32.420433044433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924258708953857 3.1221840381622314 33.01426696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924249172210693 3.2285754680633545 34.07817840576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911123037338257 3.039419651031494 32.185306549072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791663646697998 3.3815948963165283 35.60761260986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790802001953125 3.089960813522339 32.69041061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904618978500366 3.494246244430542 36.73292541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919093370437622 3.5063366889953613 36.85527420043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910219430923462 3.2771496772766113 34.56251907348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905527353286743 3.446592092514038 36.256473541259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916605472564697 3.532480001449585 37.116458892822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914057970046997 3.9962449073791504 41.75385284423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919872999191284 3.510963201522827 36.90161895751953
  batch 20 loss: 1.7919872999191284, 3.510963201522827, 36.90161895751953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916736602783203 3.583113431930542 37.62281036376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912812232971191 3.33670973777771 35.15837860107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791126012802124 2.7974517345428467 29.765644073486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900335788726807 3.156290292739868 33.352935791015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907488346099854 3.8730974197387695 40.52172088623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910338640213013 3.7784645557403564 39.575679779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908281087875366 3.627026319503784 38.061092376708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926580905914307 3.513913869857788 36.93179702758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903083562850952 3.1733243465423584 33.52355194091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921959161758423 3.384648084640503 35.638675689697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791109323501587 2.9518518447875977 31.309627532958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924073934555054 3.1598329544067383 33.3907356262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909082174301147 3.1974682807922363 33.76559066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790999174118042 3.137646436691284 33.16746520996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907065153121948 2.6163151264190674 27.953857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907465696334839 3.5303494930267334 37.09423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790612816810608 2.900883674621582 30.799449920654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928439378738403 2.9518930912017822 31.31177520751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919386625289917 3.284350872039795 34.63544845581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930058240890503 3.743699312210083 39.22999954223633
  batch 40 loss: 1.7930058240890503, 3.743699312210083, 39.22999954223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792106032371521 2.912787675857544 30.91998291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917678356170654 3.2546610832214355 34.33837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918003797531128 3.4241693019866943 36.03349304199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792000412940979 3.9518728256225586 41.31072998046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901691198349 3.5323095321655273 37.11326599121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916324138641357 3.1315858364105225 33.10749053955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907781600952148 3.22269868850708 34.017765045166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921257019042969 3.456857204437256 36.36069869995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930258512496948 2.873016834259033 30.523193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926690578460693 3.196364164352417 33.756309509277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928953170776367 2.9756178855895996 31.549076080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906749248504639 3.3965959548950195 35.75663375854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790596842765808 3.4809043407440186 36.599639892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902592420578003 3.383680820465088 35.62706756591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902638912200928 3.366217851638794 35.45244216918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902511358261108 3.0526506900787354 32.31675720214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899216413497925 3.073878049850464 32.52870178222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916001081466675 3.4511373043060303 36.30297088623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916045188903809 3.427903175354004 36.07063293457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914156913757324 3.224330186843872 34.03471755981445
  batch 60 loss: 1.7914156913757324, 3.224330186843872, 34.03471755981445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898942232131958 3.7848551273345947 39.63844680786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899552583694458 2.987934112548828 31.669296264648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7891650199890137 3.349932909011841 35.28849411010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909818887710571 3.029118299484253 32.0821647644043
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7902708053588867 2.802258253097534 29.812854766845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917033433914185 3.234645128250122 34.138153076171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913821935653687 3.522847890853882 37.019859313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 2.8427085876464844 30.218950271606445
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7922002077102661 3.072366952896118 32.515869140625
Total LOSS train 34.893247369619516 valid 33.47320795059204
CE LOSS train 1.7912151098251343 valid 0.44805005192756653
Contrastive LOSS train 3.310203247803908 valid 0.7680917382240295
EPOCH 78:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915236949920654 3.5232784748077393 37.02430725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928715944290161 3.1501505374908447 33.294376373291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790818214416504 3.3589978218078613 35.380794525146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905179262161255 3.1658101081848145 33.4486198425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917088270187378 3.782377004623413 39.615478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789811611175537 3.0800108909606934 32.58992004394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926212549209595 3.2397682666778564 34.190303802490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792183518409729 3.3831136226654053 35.62331771850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792269229888916 3.2720587253570557 34.51285934448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917158603668213 3.7401793003082275 39.193511962890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898942232131958 2.919309139251709 30.982986450195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905969619750977 3.1458446979522705 33.24904251098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908987998962402 2.9006917476654053 30.79781723022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912602424621582 3.7529103755950928 39.32036209106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792165994644165 3.1001312732696533 32.793479919433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7894586324691772 3.842881679534912 40.21827697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789445161819458 3.367797613143921 35.4674186706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913788557052612 3.7223026752471924 39.014404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791945219039917 2.876903772354126 30.56098175048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918801307678223 3.0278892517089844 32.07077407836914
  batch 20 loss: 1.7918801307678223, 3.0278892517089844, 32.07077407836914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914246320724487 2.8566558361053467 30.357982635498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904900312423706 3.060340404510498 32.39389419555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790958285331726 3.2589471340179443 34.380428314208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899503707885742 3.40665340423584 35.85648727416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907862663269043 3.310586452484131 34.89665222167969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912951707839966 3.2754664421081543 34.54595947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791502833366394 3.448695421218872 36.27845764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791713833808899 3.571086883544922 37.50258255004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789654016494751 3.3206841945648193 34.99649429321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904077768325806 3.7028069496154785 38.818477630615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905901670455933 3.110483407974243 32.895423889160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519211769104 3.3392722606658936 35.18524169921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790795922279358 3.2303717136383057 34.094512939453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914425134658813 3.2947463989257812 34.73890686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906067371368408 3.07967472076416 32.58735275268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910200357437134 3.60703182220459 37.8613395690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909550666809082 3.244986057281494 34.240814208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791703462600708 2.895679235458374 30.748497009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 3.052834987640381 32.319969177246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925193309783936 3.135279893875122 33.14531707763672
  batch 40 loss: 1.7925193309783936, 3.135279893875122, 33.14531707763672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917969226837158 2.563687562942505 27.428672790527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910178899765015 3.33625864982605 35.15360641479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910736799240112 4.103445529937744 42.82552719116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919361591339111 3.3505165576934814 35.29710006713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910810708999634 2.85349702835083 30.3260498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915654182434082 2.9039230346679688 30.830795288085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912845611572266 3.3098654747009277 34.88993835449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914915084838867 3.1986234188079834 33.77772521972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929283380508423 3.5383920669555664 37.17684555053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913204431533813 3.2019829750061035 33.81114959716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920300960540771 3.293243885040283 34.72446823120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913875579833984 3.8123998641967773 39.91539001464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919394969940186 3.40740966796875 35.86603546142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924820184707642 3.3391029834747314 35.183509826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917780876159668 3.578979730606079 37.58157730102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790266513824463 2.997687578201294 31.767141342163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900503873825073 3.596761465072632 37.75766372680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905431985855103 4.135697841644287 43.14752197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899833917617798 3.3075833320617676 34.865814208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912323474884033 3.0563533306121826 32.354766845703125
  batch 60 loss: 1.7912323474884033, 3.0563533306121826, 32.354766845703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908697128295898 3.195136785507202 33.74223709106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907588481903076 3.179792642593384 33.58868408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899779081344604 3.490607976913452 36.6960563659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923235893249512 3.212704658508301 33.919368743896484
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7909190654754639 2.6576309204101562 28.36722755432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908935546875 2.481837034225464 26.609264373779297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905670404434204 2.7824106216430664 29.614673614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907854318618774 2.0479671955108643 22.270456314086914
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7908179759979248 2.242072820663452 24.211545944213867
Total LOSS train 34.74136458176833 valid 25.676485061645508
CE LOSS train 1.7912147430273202 valid 0.4477044939994812
Contrastive LOSS train 3.2950150086329533 valid 0.560518205165863
Saved best model. Old loss 26.872933387756348 and new best loss 25.676485061645508
EPOCH 79:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906471490859985 2.288243055343628 24.673078536987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912040948867798 2.6619579792022705 28.410783767700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912819385528564 2.358788251876831 25.379165649414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916266918182373 2.478116750717163 26.57279396057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927991151809692 3.0602166652679443 32.39496612548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913031578063965 2.846374034881592 30.25504493713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913836240768433 2.942678689956665 31.218170166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905616760253906 2.728667974472046 29.077241897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921499013900757 2.949047327041626 31.282623291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918673753738403 3.0026443004608154 31.81831169128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912169694900513 2.7751851081848145 29.543067932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791150450706482 3.6486380100250244 38.277530670166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907298803329468 2.857231616973877 30.36304473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906901836395264 3.2369654178619385 34.16034698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915288209915161 3.0939598083496094 32.73112869262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790075659751892 3.4787936210632324 36.5780143737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898377180099487 2.891141176223755 30.701248168945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914292812347412 3.4035398960113525 35.82682800292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906494140625 3.335353136062622 35.14418029785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79080069065094 3.6391448974609375 38.1822509765625
  batch 20 loss: 1.79080069065094, 3.6391448974609375, 38.1822509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909877300262451 3.054645538330078 32.33744430541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915891408920288 3.4080681800842285 35.87226867675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919557094573975 3.2495808601379395 34.28776168823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910263538360596 3.094325304031372 32.73427963256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903653383255005 2.9871103763580322 31.661468505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898160219192505 2.5420773029327393 27.210588455200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903718948364258 3.047508478164673 32.26545715332031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923332452774048 2.57435941696167 27.53592872619629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791427731513977 3.4396791458129883 36.18821716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791575312614441 3.499070167541504 36.78227615356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790630578994751 2.806821346282959 29.858844757080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919505834579468 3.4556872844696045 36.34882354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791931390762329 3.51423716545105 36.934303283691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920538187026978 3.017954111099243 31.971593856811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909029722213745 2.8799033164978027 30.589937210083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911454439163208 2.4985783100128174 26.776927947998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790454626083374 2.9745419025421143 31.535873413085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921867370605469 3.8592641353607178 40.38482666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915079593658447 4.354689121246338 45.33839797973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919198274612427 3.3583741188049316 35.37565994262695
  batch 40 loss: 1.7919198274612427, 3.3583741188049316, 35.37565994262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916845083236694 3.1222994327545166 33.014678955078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790794014930725 2.837280035018921 30.16359519958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916725873947144 3.5457167625427246 37.24884033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919286489486694 3.0168309211730957 31.960237503051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790818691253662 3.0948803424835205 32.739620208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791278600692749 2.3908498287200928 25.699777603149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913814783096313 2.985551357269287 31.646896362304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912969589233398 2.659959077835083 28.390888214111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923524379730225 2.889873504638672 30.69108772277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791466236114502 2.9818837642669678 31.61030387878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922394275665283 2.401479482650757 25.807035446166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791892170906067 3.214001178741455 33.93190383911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791853427886963 3.117647886276245 32.96833419799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911330461502075 2.9921140670776367 31.7122745513916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911325693130493 3.532647132873535 37.11760330200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907103300094604 2.910519599914551 30.895906448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905957698822021 3.1317689418792725 33.10828399658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906898260116577 3.4214088916778564 36.00477981567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903821468353271 3.31557297706604 34.94611358642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911319732666016 3.5164031982421875 36.955162048339844
  batch 60 loss: 1.7911319732666016, 3.5164031982421875, 36.955162048339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911183834075928 3.6088528633117676 37.87964630126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519045829773 3.28525710105896 34.64408874511719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908697128295898 2.946314573287964 31.254016876220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916364669799805 2.9907586574554443 31.699222564697266
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7913854122161865 3.3781442642211914 35.57282638549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916209697723389 3.4619011878967285 36.4106330871582
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919117212295532 3.082883358001709 32.62074661254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916473150253296 3.052640676498413 32.31805419921875
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7914438247680664 3.373040199279785 35.52184295654297
Total LOSS train 32.64990498469426 valid 34.21781921386719
CE LOSS train 1.7912620012576763 valid 0.4478609561920166
Contrastive LOSS train 3.0858642944922816 valid 0.8432600498199463
EPOCH 80:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914810180664062 3.3594603538513184 35.386085510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791979193687439 3.5900185108184814 37.69216537475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912181615829468 2.823643684387207 30.02765464782715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912267446517944 3.4390602111816406 36.181827545166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920544147491455 3.7014920711517334 38.80697250366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916244268417358 3.0348143577575684 32.1397705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925153970718384 3.5375914573669434 37.16843032836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 3.2640540599823 34.43214797973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912274599075317 3.237492799758911 34.16615676879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926260232925415 3.1716418266296387 33.5090446472168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911903858184814 3.3130881786346436 34.92207336425781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909618616104126 3.474844455718994 36.539405822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913093566894531 3.4580771923065186 36.3720817565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790811538696289 3.0463600158691406 32.25440979003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791365385055542 2.9877543449401855 31.668909072875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906224727630615 2.552720308303833 27.317825317382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907605171203613 2.883862257003784 30.629383087158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911109924316406 3.354353427886963 35.33464431762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911666631698608 3.4140255451202393 35.93142318725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919862270355225 3.2512588500976562 34.30457305908203
  batch 20 loss: 1.7919862270355225, 3.2512588500976562, 34.30457305908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921539545059204 2.91233491897583 30.915502548217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910873889923096 2.952986717224121 31.320955276489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791237473487854 3.07045841217041 32.49582290649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790394902229309 2.6685755252838135 28.476150512695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790848970413208 3.5074944496154785 36.86579132080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915152311325073 4.058614253997803 42.37765884399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916395664215088 3.426475763320923 36.056400299072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913175821304321 3.277618169784546 34.56749725341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789729118347168 3.076840400695801 32.55813217163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908141613006592 3.248166799545288 34.27248001098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791487216949463 3.0795364379882812 32.58685302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792708396911621 3.045097589492798 32.243682861328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913683652877808 3.4403231143951416 36.19459915161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912131547927856 3.3802297115325928 35.593509674072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914483547210693 3.5605225563049316 37.396671295166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918438911437988 3.2375223636627197 34.16706848144531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905405759811401 2.8841559886932373 30.63210105895996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791670560836792 3.1107046604156494 32.89871597290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791035771369934 2.992780923843384 31.71884536743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917929887771606 3.0088579654693604 31.880373001098633
  batch 40 loss: 1.7917929887771606, 3.0088579654693604, 31.880373001098633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921497821807861 3.241763114929199 34.209781646728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791499376296997 3.1858572959899902 33.65007400512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911325693130493 3.075465202331543 32.54578399658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791446328163147 2.973268985748291 31.52413558959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909332513809204 2.8734400272369385 30.525333404541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919511795043945 2.927257537841797 31.064525604248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917314767837524 3.399911642074585 35.79084777832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915139198303223 2.7728614807128906 29.52012825012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792016625404358 2.9988644123077393 31.78066062927246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910617589950562 3.5265207290649414 37.05626678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914916276931763 3.246880292892456 34.26029586791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915061712265015 3.1400797367095947 33.19230270385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917251586914062 3.7074429988861084 38.866153717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913880348205566 3.2804834842681885 34.59622573852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915257215499878 3.2500853538513184 34.292381286621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910469770431519 3.5878682136535645 37.66972732543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900867462158203 3.720106840133667 38.99115753173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790359616279602 2.7188501358032227 28.97886085510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898887395858765 3.1196725368499756 32.98661422729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900545597076416 2.9431066513061523 31.221120834350586
  batch 60 loss: 1.7900545597076416, 2.9431066513061523, 31.221120834350586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790609359741211 3.7382607460021973 39.1732177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920581102371216 3.962585926055908 41.41791534423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920161485671997 3.064666509628296 32.438682556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928662300109863 3.014955997467041 31.942424774169922
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7918182611465454 2.3746798038482666 25.538616180419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919373512268066 3.098458766937256 32.776527404785156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791715383529663 2.681943655014038 28.61115264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918360233306885 3.000763416290283 31.799468994140625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7922260761260986 2.88710880279541 30.663314819335938
Total LOSS train 33.895954308143025 valid 30.962615966796875
CE LOSS train 1.7913626175660353 valid 0.44805651903152466
Contrastive LOSS train 3.210459173642672 valid 0.7217772006988525
EPOCH 81:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919981479644775 3.30652117729187 34.85721206665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920945882797241 2.769977569580078 29.491870880126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791279911994934 2.857689142227173 30.36817169189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906520366668701 2.871492862701416 30.505578994750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908357381820679 2.510662078857422 26.897457122802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898564338684082 2.832900285720825 30.118858337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791085124015808 3.0472042560577393 32.263126373291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909542322158813 2.3349287509918213 25.140241622924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916518449783325 2.5345070362091064 27.136722564697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917531728744507 2.6080219745635986 27.871973037719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917108535766602 2.7393603324890137 29.185314178466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79171884059906 3.248293161392212 34.27465057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791780710220337 3.814380168914795 39.935585021972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911205291748047 2.9996485710144043 31.787607192993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917431592941284 2.8237643241882324 30.029386520385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792267084121704 3.182687520980835 33.619144439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791810393333435 3.276716709136963 34.55897521972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 3.1357264518737793 33.14980697631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918070554733276 2.9922945499420166 31.714752197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913357019424438 3.1323728561401367 33.11506271362305
  batch 20 loss: 1.7913357019424438, 3.1323728561401367, 33.11506271362305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791740894317627 2.4547321796417236 26.339061737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909975051879883 3.867466449737549 40.465660095214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916723489761353 2.9040377140045166 30.832048416137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909899950027466 2.6280908584594727 28.0718994140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912976741790771 3.2037441730499268 33.828739166259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915356159210205 3.3477771282196045 35.26930618286133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919056415557861 3.5826268196105957 37.61817169189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918044328689575 3.060370683670044 32.395511627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909456491470337 2.8313333988189697 30.104278564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911441326141357 3.8177683353424072 39.96883010864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914925813674927 3.0107522010803223 31.89901351928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919039726257324 3.2410833835601807 34.20273971557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910605669021606 2.709601402282715 28.887075424194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904523611068726 2.328526496887207 25.07571792602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905423641204834 2.5395047664642334 27.185590744018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912412881851196 3.1312432289123535 33.10367202758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911810874938965 3.9134232997894287 40.925411224365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928533554077148 2.8326849937438965 30.119705200195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920182943344116 2.970106363296509 31.49308204650879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929335832595825 2.7446072101593018 29.23900604248047
  batch 40 loss: 1.7929335832595825, 2.7446072101593018, 29.23900604248047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920072078704834 2.8613781929016113 30.40578842163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918559312820435 3.0825445652008057 32.61730194091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917265892028809 2.988084077835083 31.67256736755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79196035861969 3.2396326065063477 34.18828582763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790303111076355 2.5117027759552 26.907329559326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907530069351196 2.7840640544891357 29.631393432617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906420230865479 2.4949769973754883 26.74041175842285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921521663665771 3.526071786880493 37.05287170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792877197265625 2.949223041534424 31.285106658935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927302122116089 2.7437703609466553 29.23043441772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919422388076782 3.4540939331054688 36.332881927490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792055606842041 2.51766300201416 26.968685150146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923833131790161 3.5246827602386475 37.03921127319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79209566116333 2.948166847229004 31.27376365661621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912051677703857 3.5082638263702393 36.873844146728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911019325256348 3.4544382095336914 36.33548355102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900476455688477 3.067406177520752 32.464107513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906485795974731 2.616166353225708 27.952312469482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899096012115479 3.3846707344055176 35.63661575317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903269529342651 3.037938356399536 32.169708251953125
  batch 60 loss: 1.7903269529342651, 3.037938356399536, 32.169708251953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906477451324463 3.0212178230285645 32.00282669067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791695475578308 3.0273635387420654 32.065330505371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916648387908936 3.4574215412139893 36.36587905883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791806697845459 3.3442444801330566 35.234249114990234
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.790980577468872 2.215700626373291 23.947986602783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914754152297974 3.017204761505127 31.96352195739746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791426181793213 2.641831398010254 28.209739685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914302349090576 2.827064037322998 30.062070846557617
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919261455535889 3.073173761367798 32.52366256713867
Total LOSS train 31.929852236234225 valid 30.689748764038086
CE LOSS train 1.791465084369366 valid 0.4479815363883972
Contrastive LOSS train 3.0138387313255897 valid 0.7682934403419495
EPOCH 82:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791567087173462 2.97963285446167 31.5878963470459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920080423355103 3.084538459777832 32.637393951416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916442155838013 2.875385284423828 30.54549789428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912710905075073 2.8088676929473877 29.879947662353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911078929901123 2.9501051902770996 31.292160034179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899305820465088 3.087470531463623 32.66463851928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917866706848145 2.911505937576294 30.906845092773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914373874664307 3.299058198928833 34.782020568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912421226501465 3.259474277496338 34.385982513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912613153457642 3.0937998294830322 32.7292594909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909272909164429 2.8120455741882324 29.91138458251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915204763412476 3.7191250324249268 38.98276901245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791421890258789 3.2980763912200928 34.772186279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912523746490479 3.622406244277954 38.015316009521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912046909332275 2.929248332977295 31.083688735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905800342559814 3.4016804695129395 35.8073844909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902089357376099 2.9588329792022705 31.378538131713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916940450668335 3.577242851257324 37.564125061035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909127473831177 3.4828500747680664 36.61941146850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914135456085205 3.6625618934631348 38.417030334472656
  batch 20 loss: 1.7914135456085205, 3.6625618934631348, 38.417030334472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913228273391724 2.497157335281372 26.762895584106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911925315856934 3.0972228050231934 32.76342010498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917214632034302 2.8844094276428223 30.635814666748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911022901535034 3.0508527755737305 32.29962921142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916247844696045 3.728970766067505 39.08133316040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912095785140991 3.743750810623169 39.22871780395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908962965011597 2.9300849437713623 31.091745376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908154726028442 2.7605671882629395 29.396486282348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899811267852783 3.8063344955444336 39.853328704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912163734436035 3.1634228229522705 33.425445556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792271375656128 3.1774070262908936 33.566341400146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925803661346436 2.8325283527374268 30.117862701416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913044691085815 3.226754665374756 34.05885314941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912497520446777 3.82973575592041 40.08860397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791008472442627 3.9841628074645996 41.63263702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919306755065918 3.3525681495666504 35.31761169433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911852598190308 3.5780463218688965 37.57164764404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791517972946167 3.0401854515075684 32.19337463378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906570434570312 2.8259823322296143 30.050479888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916561365127563 3.155507802963257 33.34673309326172
  batch 40 loss: 1.7916561365127563, 3.155507802963257, 33.34673309326172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791047215461731 2.7243051528930664 29.034099578857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900995016098022 3.656243324279785 38.35253143310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906074523925781 3.5085408687591553 36.876014709472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913763523101807 2.8977572917938232 30.768949508666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791190505027771 3.281064033508301 34.601829528808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791735053062439 3.1156837940216064 32.94857406616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791526198387146 2.930629014968872 31.097816467285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792220115661621 3.2832090854644775 34.62431335449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932558059692383 3.1460087299346924 33.25334167480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919929027557373 3.5916061401367188 37.70805358886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910621166229248 3.67781662940979 38.56922912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791579246520996 3.4229736328125 36.02131652832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917648553848267 2.673401355743408 28.52577781677246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791199803352356 3.0842208862304688 32.63340759277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915064096450806 4.114129066467285 42.932796478271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917264699935913 4.013911724090576 41.930843353271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907770872116089 3.4784765243530273 36.57554244995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906829118728638 3.4176886081695557 35.96757125854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907098531723022 3.232117176055908 34.111881256103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915681600570679 3.077198028564453 32.56354904174805
  batch 60 loss: 1.7915681600570679, 3.077198028564453, 32.56354904174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916725873947144 3.7199981212615967 38.99165344238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792121410369873 3.9886486530303955 41.67860794067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905477285385132 3.061434745788574 32.4048957824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912150621414185 3.394008159637451 35.73129653930664
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.791560411453247 2.614837169647217 27.93993377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791324496269226 2.73118257522583 29.1031494140625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912721633911133 2.8115105628967285 29.906375885009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 3.153136730194092 33.32289123535156
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7912940979003906 2.8204972743988037 29.996267318725586
Total LOSS train 34.34292714045598 valid 30.582170963287354
CE LOSS train 1.7913166449620173 valid 0.44782352447509766
Contrastive LOSS train 3.2551610469818115 valid 0.7051243185997009
EPOCH 83:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910290956497192 3.1083145141601562 32.874176025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791089653968811 3.340526580810547 35.196353912353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911372184753418 3.1998279094696045 33.7894172668457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790909767150879 3.8323721885681152 40.11463165283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792061448097229 3.5597939491271973 37.38999938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910667657852173 3.207442045211792 33.86548614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927175760269165 3.3462977409362793 35.25569534301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916449308395386 3.4217734336853027 36.00938034057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921451330184937 3.137941598892212 33.17156219482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916444540023804 3.294207811355591 34.73372268676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908979654312134 3.5174310207366943 36.965206146240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916098833084106 3.1788887977600098 33.58049774169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913645505905151 3.347874879837036 35.270111083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915796041488647 3.753864288330078 39.330223083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917925119400024 3.512120246887207 36.912994384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912790775299072 3.3854169845581055 35.645450592041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908391952514648 3.1763203144073486 33.55404281616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 3.0905988216400146 32.69853973388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919176816940308 2.9161550998687744 30.953468322753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916748523712158 3.769986867904663 39.49154281616211
  batch 20 loss: 1.7916748523712158, 3.769986867904663, 39.49154281616211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925183773040771 3.3129806518554688 34.922325134277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900011539459229 2.5958993434906006 27.748994827270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790359616279602 2.8084800243377686 29.875160217285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900946140289307 3.150510549545288 33.29520034790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905634641647339 3.3186049461364746 34.97661209106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791282296180725 3.7707715034484863 39.49899673461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792019248008728 3.343292474746704 35.224945068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925101518630981 2.9634318351745605 31.426830291748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918580770492554 3.295494556427002 34.746803283691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79151451587677 3.2086570262908936 33.87808609008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911568880081177 2.854909658432007 30.340253829956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915599346160889 3.072705030441284 32.518611907958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908656597137451 3.488457202911377 36.675437927246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79140305519104 3.5840342044830322 37.63174819946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910830974578857 3.3130242824554443 34.92132568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791203498840332 3.04290509223938 32.220252990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791352391242981 3.15342378616333 33.32558822631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925026416778564 3.1124320030212402 32.91682434082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917711734771729 3.1857519149780273 33.6492919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920150756835938 3.254183769226074 34.33385467529297
  batch 40 loss: 1.7920150756835938, 3.254183769226074, 34.33385467529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791035771369934 3.0163066387176514 31.954103469848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911183834075928 3.3766226768493652 35.55734634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917732000350952 3.556213617324829 37.353912353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792264699935913 2.954153537750244 31.333799362182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915503978729248 2.6657559871673584 28.44911003112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921220064163208 3.1727468967437744 33.51959228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918405532836914 2.8411591053009033 30.20343017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915104627609253 2.9794089794158936 31.585599899291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 3.465181827545166 36.4444694519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919026613235474 3.161543607711792 33.40734100341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918869256973267 3.773106336593628 39.52294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791526198387146 3.553680419921875 37.328330993652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791419506072998 2.9231982231140137 31.02340316772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911747694015503 2.9074501991271973 30.86567497253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911685705184937 2.9877936840057373 31.669105529785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916609048843384 2.991058349609375 31.70224380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918791770935059 3.209770679473877 33.889583587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915291786193848 3.928746461868286 41.07899475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902135848999023 3.098749876022339 32.777713775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790292739868164 2.9717061519622803 31.507354736328125
  batch 60 loss: 1.790292739868164, 2.9717061519622803, 31.507354736328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902064323425293 3.181553363800049 33.60573959350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912849187850952 3.260120153427124 34.392486572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913869619369507 3.6530303955078125 38.32168960571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914179563522339 3.475705862045288 36.5484733581543
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7921850681304932 3.02984881401062 32.090675354003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917221784591675 2.8630261421203613 30.421981811523438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922824621200562 2.6991751194000244 28.784032821655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918680906295776 2.7371761798858643 29.16362953186035
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917486429214478 2.763786792755127 29.429615020751953
Total LOSS train 34.26250413747934 valid 29.449814796447754
CE LOSS train 1.791470635854281 valid 0.44793716073036194
Contrastive LOSS train 3.2471033353071945 valid 0.6909466981887817
EPOCH 84:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915818691253662 2.6607213020324707 28.398794174194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791652798652649 3.1306729316711426 33.09838104248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908581495285034 2.8613667488098145 30.404523849487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901344299316406 3.0345520973205566 32.13565444946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910778522491455 3.536504030227661 37.1561164855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910200357437134 3.4776906967163086 36.567928314208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79240083694458 3.484607458114624 36.63847351074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925916910171509 3.581589698791504 37.60848617553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792310118675232 2.968078851699829 31.473098754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915353775024414 3.8438305854797363 40.22983932495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907662391662598 2.8921308517456055 30.712074279785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906417846679688 3.338972806930542 35.18037033081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905726432800293 3.303269147872925 34.823265075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7896294593811035 3.6938369274139404 38.72800064086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914406061172485 3.3726773262023926 35.518211364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910962104797363 3.1111247539520264 32.90234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903324365615845 3.573491334915161 37.525245666503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914471626281738 3.704599618911743 38.83744430541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790766954421997 3.3471224308013916 35.261993408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909746170043945 3.2423031330108643 34.21400451660156
  batch 20 loss: 1.7909746170043945, 3.2423031330108643, 34.21400451660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912355661392212 3.191997528076172 33.711212158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926734685897827 3.2487294673919678 34.27996826171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792585849761963 2.964060068130493 31.433185577392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917757034301758 3.0858590602874756 32.650367736816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909995317459106 3.9295969009399414 41.08696746826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905243635177612 3.512516736984253 36.91569137573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903566360473633 3.4268317222595215 36.05867385864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790747046470642 2.9824020862579346 31.61476707458496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905327081680298 2.7909493446350098 29.70002555847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911207675933838 3.4150326251983643 35.94144821166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912598848342896 2.813992500305176 29.931184768676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792002558708191 2.8831608295440674 30.62360954284668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909796237945557 3.36991810798645 35.49016189575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791339635848999 2.9710631370544434 31.501972198486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911325693130493 2.6639721393585205 28.43085289001465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916947603225708 2.583871364593506 27.630409240722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912436723709106 2.8674299716949463 30.465543746948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792279839515686 3.1291005611419678 33.08328628540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791407823562622 3.748023271560669 39.27164077758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917221784591675 3.3842592239379883 35.63431167602539
  batch 40 loss: 1.7917221784591675, 3.3842592239379883, 35.63431167602539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790700912475586 2.9799227714538574 31.589929580688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910175323486328 2.829857110977173 30.089588165283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919875383377075 3.819136142730713 39.98334884643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918106317520142 3.1401472091674805 33.19328308105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904016971588135 3.293208360671997 34.72248458862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911324501037598 3.125359058380127 33.04472351074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914210557937622 3.8860549926757812 40.65196990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916375398635864 4.0315937995910645 42.107574462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931848764419556 3.473918914794922 36.53237533569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924379110336304 3.4796712398529053 36.589149475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914929389953613 3.0008108615875244 31.79960060119629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913011312484741 3.360076904296875 35.39207077026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791106104850769 2.9919354915618896 31.710460662841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912123203277588 3.2070152759552 33.861366271972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917463779449463 3.3337669372558594 35.129417419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913280725479126 3.636312246322632 38.154449462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79062020778656 3.4078054428100586 35.868675231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909717559814453 3.0047810077667236 31.838781356811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907377481460571 3.233161211013794 34.12234878540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910823822021484 3.3511743545532227 35.302825927734375
  batch 60 loss: 1.7910823822021484, 3.3511743545532227, 35.302825927734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908610105514526 4.33300256729126 45.12088394165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791247844696045 3.5438344478607178 37.229591369628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906699180603027 3.2980761528015137 34.77143096923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917912006378174 3.1717209815979004 33.50899887084961
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7922263145446777 2.8883016109466553 30.675243377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914984226226807 2.9599668979644775 31.39116668701172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915661334991455 2.375892162322998 25.55048942565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917431592941284 2.7787811756134033 29.579553604125977
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919018268585205 2.7556028366088867 29.347930908203125
Total LOSS train 34.551693872305066 valid 28.96728515625
CE LOSS train 1.7913011220785289 valid 0.4479754567146301
Contrastive LOSS train 3.276039299598107 valid 0.6889007091522217
EPOCH 85:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917754650115967 2.779921293258667 29.590988159179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792057991027832 2.9405791759490967 31.19784927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791746735572815 2.8885622024536133 30.6773681640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913134098052979 2.889650821685791 30.687820434570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918530702590942 3.6615490913391113 38.407344818115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900431156158447 2.8916685581207275 30.706727981567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791427731513977 3.21091628074646 33.90058898925781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909364700317383 3.020582437515259 31.996761322021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791383147239685 3.091891288757324 32.710296630859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136311531067 2.8674778938293457 30.466915130615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912594079971313 3.6845827102661133 38.6370849609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911911010742188 2.998708486557007 31.778276443481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910341024398804 3.0483522415161133 32.27455520629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908798456192017 3.1014411449432373 32.80529022216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918426990509033 3.293370008468628 34.72554397583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915269136428833 3.222867727279663 34.02020263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914892435073853 2.656338930130005 28.354877471923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922537326812744 2.8893423080444336 30.68567657470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910908460617065 2.6973156929016113 28.764245986938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909038066864014 2.458552598953247 26.376428604125977
  batch 20 loss: 1.7909038066864014, 2.458552598953247, 26.376428604125977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913172245025635 2.5001561641693115 26.792879104614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918083667755127 3.4812705516815186 36.604515075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921885251998901 2.399733781814575 25.789525985717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916043996810913 2.741772413253784 29.20932960510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914353609085083 3.28818678855896 34.673301696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912331819534302 3.2995693683624268 34.78692626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910305261611938 3.7565321922302246 39.356353759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79157292842865 3.3933022022247314 35.72459411621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913740873336792 3.8188178539276123 39.97955322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791535496711731 3.1873795986175537 33.66533279418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914482355117798 2.944056987762451 31.232019424438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791940689086914 3.552654266357422 37.3184814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914152145385742 2.857609987258911 30.367515563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910569906234741 2.9015755653381348 30.806812286376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908796072006226 3.356738805770874 35.3582649230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917242050170898 3.3666634559631348 35.45835876464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919893264770508 3.5712690353393555 37.50468063354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927374839782715 3.1772096157073975 33.56483459472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914929389953613 3.675977945327759 38.551273345947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 3.8321945667266846 40.11448669433594
  batch 40 loss: 1.792541742324829, 3.8321945667266846, 40.11448669433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790359377861023 2.5188841819763184 26.979202270507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790144681930542 3.1224496364593506 33.01464080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790156602859497 2.9579713344573975 31.369871139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910362482070923 2.7654917240142822 29.445953369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791024088859558 2.988532781600952 31.67635154724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791383147239685 4.082725524902344 42.61863708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912055253982544 3.3660192489624023 35.45140075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911287546157837 3.5179641246795654 36.97077178955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924377918243408 2.98390531539917 31.631492614746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922776937484741 3.2164313793182373 33.95659255981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914937734603882 4.010934829711914 41.900840759277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918577194213867 3.43550705909729 36.14693069458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791977882385254 3.1029891967773438 32.821868896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918860912322998 3.0170722007751465 31.962608337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913672924041748 3.303877592086792 34.830142974853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907272577285767 2.8341474533081055 30.1322021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901867628097534 2.956305742263794 31.353242874145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790722131729126 2.9712634086608887 31.50335693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907891273498535 3.1084697246551514 32.87548828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910466194152832 2.918645143508911 30.977497100830078
  batch 60 loss: 1.7910466194152832, 2.918645143508911, 30.977497100830078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913137674331665 3.0696728229522705 32.488040924072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921926975250244 2.965965747833252 31.45184898376465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922946214675903 3.1051993370056152 32.84428787231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925517559051514 3.8026113510131836 39.818668365478516
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7924672365188599 2.3386449813842773 25.178916931152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791890263557434 3.1585936546325684 33.37782669067383
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913696765899658 2.679532051086426 28.58669090270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917890548706055 3.569180727005005 37.48359680175781
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920145988464355 2.911390781402588 30.905921936035156
Total LOSS train 33.154226743257965 valid 32.58850908279419
CE LOSS train 1.791453851186312 valid 0.4480036497116089
Contrastive LOSS train 3.1362772904909573 valid 0.727847695350647
EPOCH 86:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919856309890747 2.8573663234710693 30.365650177001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918514013290405 3.388167381286621 35.673526763916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791472315788269 3.2801454067230225 34.592926025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906086444854736 2.9340667724609375 31.131277084350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790728211402893 3.337585210800171 35.16658020019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7898924350738525 3.1775498390197754 33.565391540527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913063764572144 2.9991960525512695 31.783266067504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912200689315796 2.9251413345336914 31.042633056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923657894134521 3.3723511695861816 35.51587677001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920681238174438 3.4899723529815674 36.69179153442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909730672836304 3.32222318649292 35.013206481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910655736923218 3.5305190086364746 37.096256256103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916454076766968 2.8424570560455322 30.216215133666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791517972946167 2.629777431488037 28.089292526245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920658588409424 3.618915557861328 37.98122024536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917824983596802 2.7524161338806152 29.31594467163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911397218704224 3.0999739170074463 32.79087829589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925556898117065 3.0546445846557617 32.3390007019043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914258241653442 3.357346773147583 35.36489486694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912654876708984 3.1056177616119385 32.847442626953125
  batch 20 loss: 1.7912654876708984, 3.1056177616119385, 32.847442626953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910548448562622 3.2270255088806152 34.061309814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912535667419434 2.8046789169311523 29.838043212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915023565292358 3.2708611488342285 34.50011444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907358407974243 2.8316266536712646 30.10700225830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913848161697388 3.6568264961242676 38.359649658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912647724151611 3.4650468826293945 36.44173049926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911139726638794 3.086841106414795 32.65952682495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913072109222412 3.481832981109619 36.60963439941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907158136367798 3.598175048828125 37.772464752197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910957336425781 3.303225040435791 34.82334518432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790919303894043 3.1041975021362305 32.83289337158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921894788742065 2.994617462158203 31.73836326599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913833856582642 3.3503787517547607 35.295169830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911922931671143 3.2086243629455566 33.87743377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912739515304565 2.80002498626709 29.791522979736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791473388671875 3.120501756668091 32.996490478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791247010231018 3.131070137023926 33.10194778442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919176816940308 3.256075859069824 34.35267639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911792993545532 3.300523281097412 34.79641342163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920525074005127 3.663395404815674 38.42600631713867
  batch 40 loss: 1.7920525074005127, 3.663395404815674, 38.42600631713867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909801006317139 3.7081398963928223 38.872379302978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913800477981567 3.1759934425354004 33.55131530761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792142629623413 3.0225701332092285 32.01784133911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920807600021362 3.820465564727783 39.996734619140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908341884613037 3.0146262645721436 31.937097549438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791475772857666 2.599766731262207 27.789142608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908573150634766 3.3976199626922607 35.767059326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791200876235962 2.698016405105591 28.771364212036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927340269088745 2.960608720779419 31.398820877075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792676568031311 3.326927423477173 35.06195068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927186489105225 3.421765089035034 36.01036834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792102575302124 3.614563465118408 37.93773651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914583683013916 3.315284490585327 34.94430160522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912161350250244 3.1591780185699463 33.38299560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912797927856445 3.4503235816955566 36.29451370239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914302349090576 3.916076898574829 40.95220184326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79149329662323 3.435603141784668 36.147525787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912191152572632 3.1621615886688232 33.41283416748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905818223953247 3.7347443103790283 39.138023376464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912043333053589 3.0609421730041504 32.4006233215332
  batch 60 loss: 1.7912043333053589, 3.0609421730041504, 32.4006233215332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918760776519775 3.0224711894989014 32.0165901184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917457818984985 3.3537824153900146 35.329566955566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790055513381958 3.285222053527832 34.64227294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904132604599 2.9722630977630615 31.513044357299805
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7904008626937866 2.999833583831787 31.788738250732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790637493133545 2.9962573051452637 31.753211975097656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790021538734436 2.7844765186309814 29.63478660583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908676862716675 3.059539794921875 32.38626480102539
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7906157970428467 2.790930986404419 29.69992446899414
Total LOSS train 34.0006161909837 valid 30.868546962738037
CE LOSS train 1.7913961758980383 valid 0.44765394926071167
Contrastive LOSS train 3.2209220336033746 valid 0.6977327466011047
EPOCH 87:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907105684280396 2.9520609378814697 31.31131935119629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919864654541016 3.2089624404907227 33.88160705566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920880317687988 3.1643712520599365 33.4358024597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913849353790283 3.071699380874634 32.50837707519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792241096496582 3.6302480697631836 38.094722747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905337810516357 2.9085400104522705 30.875932693481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792571783065796 3.450683832168579 36.29941177368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791186809539795 3.1868832111358643 33.66001892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916263341903687 3.1688263416290283 33.479888916015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914828062057495 3.73006272315979 39.09210968017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914708852767944 3.0068812370300293 31.86028480529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917732000350952 3.1474692821502686 33.2664680480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791841745376587 3.5132877826690674 36.924720764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915427684783936 3.666490316390991 38.456443786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912213802337646 3.2076010704040527 33.86723327636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906947135925293 3.4717400074005127 36.508094787597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911149263381958 3.234273672103882 34.13385009765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922977209091187 3.5098066329956055 36.89036560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912893295288086 2.986187219619751 31.653160095214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913509607315063 3.035618782043457 32.14753723144531
  batch 20 loss: 1.7913509607315063, 3.035618782043457, 32.14753723144531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172682762146 3.0872721672058105 32.66444778442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907601594924927 2.6725094318389893 28.515853881835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910667657852173 2.5816900730133057 27.607967376708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909266948699951 3.082376718521118 32.61469268798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910441160202026 3.042113780975342 32.212181091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914412021636963 3.133776903152466 33.12921142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916946411132812 3.446495771408081 36.25665283203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921074628829956 3.1854705810546875 33.646812438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905250787734985 3.3779783248901367 35.570308685302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907246351242065 2.9600961208343506 31.391685485839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912558317184448 2.6530885696411133 28.322141647338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922896146774292 3.255359172821045 34.345882415771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917206287384033 3.213993787765503 33.93165969848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906732559204102 2.888262987136841 30.673301696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791035532951355 2.608405113220215 27.875085830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79142165184021 2.7917428016662598 29.70884895324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911442518234253 2.4840822219848633 26.63196563720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916404008865356 3.0670254230499268 32.46189498901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911995649337769 2.7989745140075684 29.78094482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918587923049927 2.7258479595184326 29.050338745117188
  batch 40 loss: 1.7918587923049927, 2.7258479595184326, 29.050338745117188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920385599136353 2.8983571529388428 30.775609970092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792264461517334 2.8006091117858887 29.798357009887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792309045791626 2.422536611557007 26.017675399780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792073369026184 3.052805185317993 32.320125579833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911738157272339 3.029953956604004 32.09071350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915703058242798 3.0624310970306396 32.4158821105957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914341688156128 3.3186392784118652 34.97782897949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916371822357178 3.2905383110046387 34.697021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79226553440094 2.975048065185547 31.54274559020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921085357666016 2.812969446182251 29.921802520751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791959524154663 2.865095853805542 30.44291877746582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917686700820923 2.913814067840576 30.92991065979004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916324138641357 2.713080406188965 28.922435760498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916399240493774 2.8040339946746826 29.831979751586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921247482299805 3.4261560440063477 36.05368423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917163372039795 3.594592332839966 37.737640380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911741733551025 3.901871919631958 40.80989456176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909702062606812 3.460162401199341 36.39259338378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911882400512695 3.370421886444092 35.49540710449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791178584098816 3.043983221054077 32.23101043701172
  batch 60 loss: 1.791178584098816, 3.043983221054077, 32.23101043701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908422946929932 3.8155055046081543 39.945899963378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791454553604126 3.2420413494110107 34.21186828613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913891077041626 3.6715919971466064 38.50730895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914562225341797 3.0496792793273926 32.288246154785156
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.791482925415039 2.52583646774292 27.049848556518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792039394378662 2.9905099868774414 31.697139739990234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919373512268066 2.545132875442505 27.24326515197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920318841934204 3.055013418197632 32.342166900634766
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920355796813965 2.9602344036102295 31.394380569458008
Total LOSS train 32.92534825251653 valid 30.669238090515137
CE LOSS train 1.7915003116314228 valid 0.4480088949203491
Contrastive LOSS train 3.1133847933549146 valid 0.7400586009025574
EPOCH 88:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918955087661743 2.950953245162964 31.301427841186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792363166809082 3.68416428565979 38.63400650024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610598564148 3.449007272720337 36.28168487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915763854980469 3.038647413253784 32.17805099487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919124364852905 2.994535207748413 31.73726463317871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913684844970703 3.5492589473724365 37.283958435058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918232679367065 3.5704805850982666 37.49663162231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912646532058716 4.196120738983154 43.752471923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916508913040161 2.9334628582000732 31.126279830932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913862466812134 2.6854560375213623 28.645946502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906824350357056 3.032663106918335 32.117313385009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908926010131836 3.2479212284088135 34.27010726928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912126779556274 3.120110511779785 32.99231719970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910345792770386 3.159928798675537 33.390323638916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914117574691772 4.183035373687744 43.62176513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916616201400757 3.3137786388397217 34.929447174072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915194034576416 3.392676591873169 35.718284606933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922954559326172 3.2065107822418213 33.85740661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927602529525757 3.360424757003784 35.39700698852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925091981887817 3.7247395515441895 39.0399055480957
  batch 20 loss: 1.7925091981887817, 3.7247395515441895, 39.0399055480957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921940088272095 3.169046640396118 33.48265838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792080283164978 3.4632952213287354 36.42503356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921830415725708 3.2241291999816895 34.03347396850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911860942840576 2.6972503662109375 28.763689041137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913156747817993 3.407646894454956 35.8677864074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911152839660645 3.3383283615112305 35.174400329589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916686534881592 3.0577948093414307 32.3696174621582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922723293304443 2.984952449798584 31.64179801940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920026779174805 3.2188732624053955 33.980735778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913422584533691 3.263841152191162 34.42975616455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916454076766968 3.0564584732055664 32.356231689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917019128799438 3.222421407699585 34.01591491699219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912060022354126 3.323864221572876 35.029850006103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919745445251465 2.993715524673462 31.729129791259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925148010253906 2.779540538787842 29.587921142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923011779785156 3.288989543914795 34.68219757080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923321723937988 3.133936643600464 33.13169860839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792884349822998 3.0351529121398926 32.144412994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912921905517578 2.797508955001831 29.766382217407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918485403060913 2.7296977043151855 29.08882713317871
  batch 40 loss: 1.7918485403060913, 2.7296977043151855, 29.08882713317871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916005849838257 2.834559679031372 30.137197494506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918356657028198 2.634152889251709 28.133365631103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918336391448975 2.5792174339294434 27.584009170532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926006317138672 2.675572156906128 28.548322677612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792527675628662 2.874298572540283 30.535512924194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933833599090576 3.018717050552368 31.980552673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926055192947388 2.4800326824188232 26.592933654785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919458150863647 3.663074016571045 38.42268753051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928411960601807 3.572664499282837 37.51948928833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926064729690552 3.212725877761841 33.919864654541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792594075202942 3.3048367500305176 34.84096145629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926583290100098 3.26776385307312 34.470298767089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928470373153687 3.2536873817443848 34.32971954345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793440580368042 3.067108392715454 32.46452331542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926748991012573 3.4338862895965576 36.13153839111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921637296676636 2.8117868900299072 29.910032272338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918248176574707 3.042343854904175 32.21526336669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915756702423096 2.960616111755371 31.397737503051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791750192642212 3.267760753631592 34.4693603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920290231704712 3.475367307662964 36.54570388793945
  batch 60 loss: 1.7920290231704712, 3.475367307662964, 36.54570388793945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924920320510864 4.009641647338867 41.88890838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921576499938965 3.5062568187713623 36.8547248840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79214346408844 3.5510714054107666 37.302860260009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79275381565094 3.086134672164917 32.65410232543945
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926766872406006 2.726901054382324 29.061687469482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921345233917236 3.1557743549346924 33.349876403808594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920206785202026 2.8792834281921387 30.584856033325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920377254486084 3.2176599502563477 33.96863555908203
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7924458980560303 2.7205820083618164 28.998266220092773
Total LOSS train 33.68283805847168 valid 31.72540855407715
CE LOSS train 1.7919916244653555 valid 0.44811147451400757
Contrastive LOSS train 3.1890845885643593 valid 0.6801455020904541
EPOCH 89:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919901609420776 3.0645976066589355 32.437965393066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922565937042236 3.6626150608062744 38.41840744018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917059659957886 2.85947585105896 30.386465072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917149066925049 2.9682884216308594 31.474599838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792497992515564 3.2952308654785156 34.744808197021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791914463043213 3.0221800804138184 32.01371765136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921807765960693 3.099444627761841 32.786624908447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919946908950806 3.074749708175659 32.539493560791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791707992553711 3.5745832920074463 37.53754425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926043272018433 3.155543804168701 33.34804153442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924350500106812 3.3353586196899414 35.146018981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919821739196777 3.0333104133605957 32.125083923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922124862670898 2.8958702087402344 30.75091552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918970584869385 3.232307195663452 34.114967346191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913987636566162 2.7561583518981934 29.352983474731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791925072669983 3.2458229064941406 34.250152587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918583154678345 2.9744787216186523 31.536645889282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924308776855469 3.152247905731201 33.314910888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918446063995361 2.9873480796813965 31.665327072143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917420864105225 3.40531325340271 35.844871520996094
  batch 20 loss: 1.7917420864105225, 3.40531325340271, 35.844871520996094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791334867477417 3.1852025985717773 33.64336013793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916028499603271 3.6625025272369385 38.416629791259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792406439781189 3.516566514968872 36.958072662353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791666865348816 3.1082494258880615 32.87416076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910587787628174 3.417022943496704 35.96128845214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916566133499146 3.1638331413269043 33.429988861083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792359471321106 3.490374803543091 36.69610595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917832136154175 3.0727033615112305 32.51881790161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792108416557312 3.3906586170196533 35.698692321777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916494607925415 3.2714197635650635 34.5058479309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791961431503296 3.053280830383301 32.32476806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792230486869812 3.5900137424468994 37.69236755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791465163230896 2.9695732593536377 31.487197875976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912296056747437 2.6787679195404053 28.578908920288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914890050888062 2.9591948986053467 31.383438110351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913637161254883 3.0148751735687256 31.94011688232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916043996810913 3.1883223056793213 33.674827575683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923656702041626 2.9211041927337646 31.003408432006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916486263275146 2.841568946838379 30.207338333129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923022508621216 3.6684975624084473 38.47727584838867
  batch 40 loss: 1.7923022508621216, 3.6684975624084473, 38.47727584838867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915297746658325 3.4062254428863525 35.85378646850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902120351791382 2.4147465229034424 25.93767738342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904874086380005 2.7386717796325684 29.17720603942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908227443695068 3.31292462348938 34.920066833496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908517122268677 2.682049036026001 28.61134147644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919983863830566 3.468195676803589 36.47395706176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915594577789307 2.924082040786743 31.032379150390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919116020202637 3.489856481552124 36.69047546386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926331758499146 2.9236297607421875 31.0289306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920572757720947 2.866374969482422 30.455806732177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919641733169556 2.1639020442962646 23.430984497070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911674976348877 3.375500202178955 35.54616928100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911853790283203 3.271939992904663 34.510581970214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905163764953613 3.7065415382385254 38.85593032836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900867462158203 3.4594814777374268 36.38490295410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909984588623047 3.0918922424316406 32.709922790527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791650414466858 3.7202248573303223 38.993896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610598564148 3.1402502059936523 33.19411087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790848731994629 2.9403090476989746 31.193939208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908382415771484 2.8470609188079834 30.26144790649414
  batch 60 loss: 1.7908382415771484, 2.8470609188079834, 30.26144790649414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790060043334961 2.924837350845337 31.038433074951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905224561691284 3.05326509475708 32.32317352294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907997369766235 3.0840649604797363 32.631446838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915380001068115 3.4046883583068848 35.83842086791992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792460322380066 3.4121274948120117 35.91373825073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791182041168213 2.825437068939209 30.04555320739746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911787033081055 2.880511999130249 30.596298217773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912390232086182 2.5806429386138916 27.597667694091797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7913322448730469 2.5867152214050293 27.658485412597656
Total LOSS train 33.29647513169509 valid 28.974501132965088
CE LOSS train 1.7916291145177987 valid 0.4478330612182617
Contrastive LOSS train 3.1504846096038817 valid 0.6466788053512573
EPOCH 90:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909607887268066 2.9856913089752197 31.647872924804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916287183761597 3.060574531555176 32.39737319946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908148765563965 3.361182928085327 35.40264129638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911150455474854 3.3720598220825195 35.51171112060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916820049285889 3.7849698066711426 39.641380310058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790164589881897 3.1932027339935303 33.722190856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913579940795898 2.8805580139160156 30.596939086914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790913462638855 3.3795676231384277 35.58658981323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918435335159302 3.1313469409942627 33.10531234741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791725993156433 3.0078136920928955 31.869861602783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906434535980225 3.232954502105713 34.12018585205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908751964569092 2.9873077869415283 31.663951873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907261848449707 3.0353758335113525 32.14448547363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909045219421387 3.4745874404907227 36.53677749633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919161319732666 3.4364137649536133 36.15605163574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791873574256897 3.5746257305145264 37.53813171386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790869116783142 2.946591377258301 31.25678253173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792425274848938 3.127976417541504 33.07218933105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906863689422607 3.019314765930176 31.98383331298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915924787521362 3.1605031490325928 33.39662551879883
  batch 20 loss: 1.7915924787521362, 3.1605031490325928, 33.39662551879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908556461334229 3.53070330619812 37.0978889465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790021300315857 3.4688291549682617 36.47831344604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914577722549438 3.2890188694000244 34.68164825439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919212579727173 3.5444679260253906 37.23659896850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913106679916382 3.8832616806030273 40.62392807006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908776998519897 3.393744945526123 35.728328704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912052869796753 3.4013235569000244 35.804443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908742427825928 3.0922927856445312 32.713802337646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902165651321411 3.5100162029266357 36.890380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908461093902588 4.037891387939453 42.169761657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915862798690796 3.6083920001983643 37.87550735473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928028106689453 3.4984548091888428 36.77735137939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915619611740112 3.196920156478882 33.760765075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790779948234558 2.8176960945129395 29.96773910522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904361486434937 3.127682685852051 33.067264556884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905423641204834 3.022775888442993 32.0182991027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907938957214355 3.674293041229248 38.53372573852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924641370773315 2.691013813018799 28.702600479125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912867069244385 2.7190351486206055 28.981637954711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922271490097046 3.05781888961792 32.370418548583984
  batch 40 loss: 1.7922271490097046, 3.05781888961792, 32.370418548583984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204523563385 2.884140729904175 30.633453369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916805744171143 3.156733989715576 33.3590202331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791509985923767 2.7852625846862793 29.644136428833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914817333221436 2.7546112537384033 29.33759307861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906806468963623 2.8152477741241455 29.943157196044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79160475730896 2.954636573791504 31.337970733642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908602952957153 3.395693063735962 35.7477912902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79154372215271 3.1619138717651367 33.410682678222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932615280151367 3.2523984909057617 34.3172492980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792359709739685 2.9489874839782715 31.282236099243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791872501373291 2.869863986968994 30.490510940551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914509773254395 2.758206367492676 29.37351417541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930399179458618 3.433007001876831 36.123111724853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918261289596558 3.236769676208496 34.159523010253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913882732391357 2.9888458251953125 31.679845809936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916072607040405 2.7986342906951904 29.777950286865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912967205047607 3.2131426334381104 33.922725677490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915828227996826 3.354173183441162 35.333316802978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909971475601196 3.972397565841675 41.51497268676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791102647781372 2.969747304916382 31.488576889038086
  batch 60 loss: 1.791102647781372, 2.969747304916382, 31.488576889038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914583683013916 3.2732043266296387 34.523502349853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913615703582764 2.9827260971069336 31.618621826171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914965152740479 3.618647813796997 37.97797393798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791725754737854 3.2290422916412354 34.082149505615234
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7910739183425903 2.9138059616088867 30.929134368896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914378643035889 2.9445183277130127 31.23661994934082
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913274765014648 3.136085271835327 33.15217971801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791467547416687 3.544710397720337 37.23857116699219
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791609764099121 3.3269436359405518 35.0610466003418
Total LOSS train 33.85907710148738 valid 34.172104358673096
CE LOSS train 1.7913707072918232 valid 0.4479024410247803
Contrastive LOSS train 3.206770625481239 valid 0.8317359089851379
EPOCH 91:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791460394859314 3.313027858734131 34.9217414855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79220449924469 3.4854021072387695 36.64622497558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919560670852661 3.5582826137542725 37.37478256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915147542953491 3.607083320617676 37.86234664916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920541763305664 3.432407855987549 36.11613082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910246849060059 3.2408668994903564 34.19969177246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918847799301147 3.2968709468841553 34.76059341430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791422724723816 2.9578232765197754 31.369653701782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916152477264404 2.473475933074951 26.52637481689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920730113983154 3.0356221199035645 32.148292541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915515899658203 3.3337900638580322 35.12945556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915928363800049 3.4196369647979736 35.98796081542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922136783599854 3.1132419109344482 32.92463302612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791381597518921 4.106556415557861 42.8569450378418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910795211791992 3.356529951095581 35.356380462646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903873920440674 3.591259479522705 37.702980041503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900139093399048 3.313823699951172 34.92824935913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905699014663696 3.700937509536743 38.79994583129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914141416549683 4.115145206451416 42.94286346435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791844367980957 3.8196017742156982 39.98786163330078
  batch 20 loss: 1.791844367980957, 3.8196017742156982, 39.98786163330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913424968719482 2.9449779987335205 31.241121292114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910369634628296 3.731767177581787 39.10871124267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791311264038086 3.40061092376709 35.79742431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790103554725647 2.8977532386779785 30.767635345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903695106506348 3.4300334453582764 36.09070587158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910518646240234 3.573406934738159 37.525123596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79142165184021 3.1278281211853027 33.0697021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907432317733765 3.6754989624023438 38.54573440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789453387260437 3.445073127746582 36.24018096923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905024290084839 3.5536885261535645 37.32738494873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791560411453247 3.153893232345581 33.33049392700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792506217956543 3.131880044937134 33.111305236816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917546033859253 3.402360200881958 35.81535720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917081117630005 3.5894174575805664 37.685882568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908587455749512 3.221684694290161 34.00770568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790109634399414 3.6522722244262695 38.312828063964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7900443077087402 3.1005094051361084 32.79513931274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914612293243408 3.272503137588501 34.51649475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909467220306396 3.241105556488037 34.202003479003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915085554122925 4.070923328399658 42.50074005126953
  batch 40 loss: 1.7915085554122925, 4.070923328399658, 42.50074005126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912646532058716 4.124078750610352 43.03205108642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913342714309692 3.145095109939575 33.242286682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911564111709595 3.082357883453369 32.6147346496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913882732391357 3.459505558013916 36.386444091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910830974578857 3.082288980484009 32.61397171020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921010255813599 2.8165929317474365 29.958030700683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917836904525757 3.0404069423675537 32.19585418701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919119596481323 2.8180835247039795 29.972747802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927219867706299 3.3914971351623535 35.70769119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912237644195557 3.5308074951171875 37.099300384521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912800312042236 3.147120237350464 33.262481689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909239530563354 3.0673816204071045 32.46474075317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792212963104248 3.791736602783203 39.70957946777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909568548202515 3.167949914932251 33.470455169677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907559871673584 3.3504929542541504 35.295684814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791369080543518 3.2530431747436523 34.32180404663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915400266647339 2.8936707973480225 30.728248596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791808843612671 3.0416204929351807 32.208011627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912226915359497 3.2579233646392822 34.37045669555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910605669021606 3.3318185806274414 35.10924530029297
  batch 60 loss: 1.7910605669021606, 3.3318185806274414, 35.10924530029297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790718913078308 3.0211877822875977 32.00259780883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909212112426758 2.7334249019622803 29.125171661376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906428575515747 3.4480478763580322 36.271121978759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912386655807495 3.1899232864379883 33.69047164916992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7904750108718872 2.415713310241699 25.947608947753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905102968215942 2.9195809364318848 30.986318588256836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790350317955017 3.7541444301605225 39.33179473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907264232635498 2.916473865509033 30.955463409423828
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7904508113861084 3.366321563720703 35.45366668701172
Total LOSS train 34.943624232365536 valid 34.18181085586548
CE LOSS train 1.7912638609225933 valid 0.4476127028465271
Contrastive LOSS train 3.315236043930054 valid 0.8415803909301758
EPOCH 92:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903474569320679 3.375779151916504 35.54813766479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914988994598389 3.283081293106079 34.622314453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914472818374634 2.9633891582489014 31.425338745117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915979623794556 3.0652287006378174 32.443885803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919180393218994 3.1258704662323 33.05062484741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911911010742188 3.236100196838379 34.152191162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792242169380188 3.6479508876800537 38.271751403808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791752576828003 3.057978630065918 32.37154006958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791196346282959 2.9458205699920654 31.24940299987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916256189346313 3.1635921001434326 33.427547454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907764911651611 3.377718925476074 35.56796646118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790934681892395 3.0877294540405273 32.66822814941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792399287223816 3.0647940635681152 32.44034194946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909170389175415 3.2886898517608643 34.67781448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791793942451477 3.5259575843811035 37.051368713378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914410829544067 3.458353281021118 36.374977111816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791763186454773 3.7080624103546143 38.87238693237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921637296676636 3.727381706237793 39.065982818603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911863327026367 3.899202823638916 40.7832145690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911081314086914 3.2473485469818115 34.264591217041016
  batch 20 loss: 1.7911081314086914, 3.2473485469818115, 34.264591217041016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910510301589966 2.7291600704193115 29.082653045654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790858507156372 2.7583084106445312 29.373943328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912482023239136 2.414318323135376 25.934431076049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790950894355774 3.265125036239624 34.44219970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913053035736084 3.07145094871521 32.50581359863281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916877269744873 3.804772138595581 39.83940887451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918152809143066 3.287581443786621 34.667633056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909125089645386 3.340700626373291 35.19791793823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906678915023804 2.653275728225708 28.32342529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79026460647583 3.466002941131592 36.450294494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790558099746704 2.4727489948272705 26.518047332763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921228408813477 2.650546073913574 28.297584533691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913738489151 3.5381455421447754 37.172828674316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913432121276855 2.9757161140441895 31.548503875732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914749383926392 3.3936026096343994 35.727500915527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913999557495117 3.991429328918457 41.705692291259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7902898788452148 3.478438377380371 36.574676513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914398908615112 2.973320245742798 31.524641036987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908098697662354 3.510586738586426 36.89667510986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918317317962646 3.03118634223938 32.103694915771484
  batch 40 loss: 1.7918317317962646, 3.03118634223938, 32.103694915771484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916988134384155 3.11686372756958 32.96033477783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520118713379 2.8664474487304688 30.45599365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918145656585693 3.5778658390045166 37.570472717285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910332679748535 3.3281655311584473 35.072689056396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908544540405273 3.7080416679382324 38.871273040771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791896939277649 3.5437979698181152 37.22987747192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912782430648804 3.1652145385742188 33.443424224853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79207444190979 2.342410087585449 25.216175079345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928801774978638 2.808556079864502 29.878440856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921675443649292 2.593269109725952 27.7248592376709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917622327804565 3.4929840564727783 36.72160339355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791446328163147 3.185659885406494 33.64804458618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79149329662323 3.36757230758667 35.46721649169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910135984420776 3.576324462890625 37.554256439208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912274599075317 3.4841816425323486 36.6330451965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916885614395142 3.533374071121216 37.12542724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909079790115356 3.6697375774383545 38.488285064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917914390563965 3.6674911975860596 38.46670150756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79133939743042 3.0328915119171143 32.12025451660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915173768997192 3.1331491470336914 33.123008728027344
  batch 60 loss: 1.7915173768997192, 3.1331491470336914, 33.123008728027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911494970321655 3.1918208599090576 33.70935821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913463115692139 3.3573272228240967 35.364620208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910693883895874 3.3649630546569824 35.44070053100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911386489868164 2.8616087436676025 30.4072265625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7911951541900635 2.221069097518921 24.00188636779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919456958770752 2.967451333999634 31.466459274291992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919198274612427 3.232069253921509 34.112613677978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921240329742432 2.952082872390747 31.312952041625977
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791897177696228 3.1280014514923096 33.0719108581543
Total LOSS train 33.98326688913198 valid 32.490983963012695
CE LOSS train 1.7913848125017606 valid 0.447974294424057
Contrastive LOSS train 3.219188195008498 valid 0.7820003628730774
EPOCH 93:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918421030044556 3.2500462532043457 34.29230499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920273542404175 3.45531964302063 36.34522247314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920513153076172 3.4638211727142334 36.430259704589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917735576629639 3.200211763381958 33.79389190673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 3.140749931335449 33.198585510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921682596206665 3.2936275005340576 34.72844314575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925546169281006 3.2844274044036865 34.63682556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789984107017517 3.093566656112671 32.725650787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790767788887024 2.890446662902832 30.695234298706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915048599243164 2.3583505153656006 25.375011444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911525964736938 2.9595327377319336 31.3864803314209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916427850723267 3.690685987472534 38.69850158691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919564247131348 3.3635828495025635 35.42778778076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914197444915771 2.9744296073913574 31.535717010498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 3.3813366889953613 35.6052360534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792043924331665 3.8139421939849854 39.93146896362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909599542617798 3.568554639816284 37.476505279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925312519073486 3.721890449523926 39.01143264770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791536569595337 3.2551469802856445 34.3430061340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903869152069092 2.537705421447754 27.16744041442871
  batch 20 loss: 1.7903869152069092, 2.537705421447754, 27.16744041442871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7899922132492065 2.961456060409546 31.404552459716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7901195287704468 2.6624481678009033 28.414600372314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910070419311523 2.6722333431243896 28.51333999633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908951044082642 2.972830295562744 31.519197463989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791526198387146 3.126856565475464 33.06009292602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922618389129639 3.269760847091675 34.489871978759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792379379272461 3.117448329925537 32.96686553955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924171686172485 3.185134172439575 33.64375686645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916979789733887 3.0044264793395996 31.83596420288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914447784423828 2.8644049167633057 30.43549346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791563630104065 3.3523221015930176 35.31478500366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919044494628906 2.7308549880981445 29.100454330444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914705276489258 3.4709553718566895 36.50102233886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913764715194702 3.245405912399292 34.24543762207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905741930007935 2.6668498516082764 28.459074020385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910690307617188 3.1385045051574707 33.17611312866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791146993637085 2.7674994468688965 29.466142654418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913163900375366 3.1214988231658936 33.00630569458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919046878814697 3.3243696689605713 35.03560256958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927920818328857 2.5136349201202393 26.929140090942383
  batch 40 loss: 1.7927920818328857, 2.5136349201202393, 26.929140090942383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791759967803955 2.389878511428833 25.69054412841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923825979232788 2.9742817878723145 31.535200119018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927950620651245 2.9777040481567383 31.569835662841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792568564414978 3.0545363426208496 32.33793258666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791749358177185 2.429084539413452 26.082595825195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919341325759888 3.250203847885132 34.29397201538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791999340057373 3.7204790115356445 38.996788024902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918832302093506 3.7088863849639893 38.88074493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927740812301636 4.237113952636719 44.16391372680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919503450393677 3.0248825550079346 32.040775299072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913540601730347 3.52075457572937 36.9989013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912931442260742 3.265072822570801 34.442020416259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911325693130493 3.548572301864624 37.27685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912968397140503 3.7100589275360107 38.89188766479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918654680252075 3.9482314586639404 41.2741813659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791989803314209 2.84173846244812 30.209375381469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918754816055298 3.241757869720459 34.20945358276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916089296340942 3.338449239730835 35.17610168457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791123390197754 3.1922032833099365 33.713157653808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909454107284546 2.8765478134155273 30.55642318725586
  batch 60 loss: 1.7909454107284546, 2.8765478134155273, 30.55642318725586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908388376235962 3.748713254928589 39.27797317504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904868125915527 2.9282562732696533 31.073049545288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910184860229492 3.552319288253784 37.314212799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919050455093384 3.3729424476623535 35.52132797241211
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7917404174804688 2.899181842803955 30.783557891845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792251467704773 3.0584158897399902 32.37641143798828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923599481582642 3.507493495941162 36.86729431152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923673391342163 3.220080614089966 33.99317169189453
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918282747268677 2.654891014099121 28.34073829650879
Total LOSS train 33.57897846515362 valid 32.89440393447876
CE LOSS train 1.7915752447568454 valid 0.4479570686817169
Contrastive LOSS train 3.1787403180049014 valid 0.6637227535247803
EPOCH 94:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791662335395813 3.0095202922821045 31.886865615844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916524410247803 3.12839937210083 33.075645446777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913596630096436 3.3485798835754395 35.277156829833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790921926498413 2.842046022415161 30.211381912231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909791469573975 2.8221476078033447 30.012454986572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.789780616760254 3.654529333114624 38.3350715637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909425497055054 3.165574312210083 33.446685791015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911733388900757 3.231983184814453 34.11100387573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918457984924316 3.8147778511047363 39.93962478637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918857336044312 4.059082984924316 42.382713317871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914040088653564 3.128002166748047 33.07142639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913320064544678 3.4769654273986816 36.56098556518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791096806526184 3.565553903579712 37.44663619995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791047215461731 3.1965980529785156 33.75702667236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791764259338379 3.8076140880584717 39.86790466308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908170223236084 3.529453992843628 37.085357666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907795906066895 3.209827184677124 33.8890495300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917635440826416 2.6317129135131836 28.1088924407959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915581464767456 3.145972967147827 33.25128936767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913004159927368 2.582841157913208 27.61971092224121
  batch 20 loss: 1.7913004159927368, 2.582841157913208, 27.61971092224121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913002967834473 2.9128053188323975 30.919353485107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913539409637451 2.964630603790283 31.437658309936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917673587799072 3.2731404304504395 34.523170471191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913243770599365 3.102541208267212 32.816734313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791089415550232 3.1047475337982178 32.838565826416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791108250617981 3.115981101989746 32.95092010498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916885614395142 3.3560304641723633 35.35198974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920722961425781 3.3505680561065674 35.297752380371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909191846847534 3.113708257675171 32.928001403808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913295030593872 3.43339204788208 36.125247955322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912235260009766 3.335893154144287 35.15015411376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920171022415161 3.128401041030884 33.076026916503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791733980178833 2.807445526123047 29.86618995666504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922598123550415 3.602240562438965 37.814666748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922284603118896 3.3986287117004395 35.77851486206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917386293411255 2.987297296524048 31.664710998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791095495223999 2.9593911170959473 31.385005950927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919707298278809 3.127286672592163 33.06483840942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915308475494385 3.8055758476257324 39.8472900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920995950698853 3.576359272003174 37.55569076538086
  batch 40 loss: 1.7920995950698853, 3.576359272003174, 37.55569076538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914667129516602 3.109689950942993 32.88836669921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790788173675537 3.0440402030944824 32.2311897277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915958166122437 3.731661319732666 39.10820770263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918890714645386 3.87503719329834 40.54226303100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916946411132812 3.6813573837280273 38.60527038574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792069911956787 3.6442413330078125 38.23448181152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915292978286743 3.4692041873931885 36.48357391357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912602424621582 3.6822760105133057 38.61402130126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921638488769531 3.2242064476013184 34.03422927856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918614149093628 3.0165016651153564 31.956876754760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791663646697998 3.1035146713256836 32.82680892944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791434407234192 3.6563520431518555 38.354957580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918815612792969 3.4300880432128906 36.0927619934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916324138641357 3.4996302127838135 36.78793716430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916392087936401 3.8116447925567627 39.908084869384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909772396087646 2.4260172843933105 26.051151275634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7905783653259277 3.7498745918273926 39.28932189941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540503501892 3.606971502304077 37.86125564575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916994094848633 3.007004499435425 31.861743927001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519045829773 3.404804229736328 35.839561462402344
  batch 60 loss: 1.791519045829773, 3.404804229736328, 35.839561462402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915018796920776 3.514652729034424 36.938026428222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912262678146362 3.205000877380371 33.84123611450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912538051605225 2.8717267513275146 30.508522033691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912479639053345 2.955127477645874 31.3425235748291
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7911514043807983 3.658125877380371 38.372413635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920645475387573 2.8988521099090576 30.78058624267578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920112609863281 3.261223316192627 34.40424346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920931577682495 2.824676275253296 30.038856506347656
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792077898979187 2.774435520172119 29.53643226623535
Total LOSS train 34.743140822190504 valid 31.190029621124268
CE LOSS train 1.7914489874472985 valid 0.44801947474479675
Contrastive LOSS train 3.2951692030980038 valid 0.6936088800430298
EPOCH 95:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 3.126530170440674 33.057159423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921563386917114 3.36745548248291 35.46670913696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923904657363892 2.983621835708618 31.62860870361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922314405441284 3.3729934692382812 35.52216720581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792123556137085 3.00254225730896 31.817546844482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923787832260132 2.5562753677368164 27.355133056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792792797088623 3.21451997756958 33.937992095947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929927110671997 3.021636724472046 32.009361267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793626308441162 2.8715927600860596 30.509553909301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939828634262085 2.6642861366271973 28.436843872070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7952800989151 3.012094259262085 31.916223526000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7954245805740356 3.4590981006622314 36.38640594482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.796105980873108 3.315903425216675 34.95513916015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7955952882766724 3.498530387878418 36.78090286254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931703329086304 3.3906760215759277 35.69993209838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7949438095092773 3.6956517696380615 38.751461029052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7948812246322632 3.521450996398926 37.0093879699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.795106291770935 3.4661872386932373 36.45697784423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794227123260498 3.874269723892212 40.53692626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925509214401245 3.7277402877807617 39.06995391845703
  batch 20 loss: 1.7925509214401245, 3.7277402877807617, 39.06995391845703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933937311172485 3.299321174621582 34.78660202026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925795316696167 3.4029276371002197 35.82185745239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917559146881104 2.807459592819214 29.866352081298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921113967895508 2.8352160453796387 30.144271850585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918367385864258 3.540813446044922 37.19997024536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791995644569397 3.5056235790252686 36.84823226928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792820930480957 3.5347554683685303 37.14037322998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791507363319397 3.2749524116516113 34.54103088378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935816049575806 3.2371773719787598 34.16535568237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907707691192627 3.131779432296753 33.10856628417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793103575706482 3.0335628986358643 32.12873077392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918568849563599 2.8416762351989746 30.208620071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930742502212524 3.2089738845825195 33.8828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936352491378784 3.480210542678833 36.595741271972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793166160583496 3.124671459197998 33.03988265991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794259786605835 3.1766197681427 33.560455322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935404777526855 3.204002618789673 33.83356857299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920645475387573 3.27087140083313 34.50077819824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918283939361572 3.3478283882141113 35.270111083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791555404663086 3.520781993865967 36.99937438964844
  batch 40 loss: 1.791555404663086, 3.520781993865967, 36.99937438964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922574281692505 3.0500948429107666 32.29320526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926236391067505 2.7728805541992188 29.52142906188965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923870086669922 2.5038435459136963 26.830821990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791487693786621 2.7540955543518066 29.332443237304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925751209259033 2.939086437225342 31.183441162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792504906654358 3.2961432933807373 34.753936767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921395301818848 2.6542367935180664 28.33450698852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791930913925171 2.9018476009368896 30.810407638549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920531034469604 3.5966758728027344 37.758811950683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792484164237976 3.1723551750183105 33.51603698730469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908265590667725 3.5963361263275146 37.75418472290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792637586593628 3.298736095428467 34.779998779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 3.3894402980804443 35.68690872192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934225797653198 3.4013795852661133 35.80721664428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918705940246582 2.877743721008301 30.569307327270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927026748657227 2.8011932373046875 29.80463409423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930651903152466 2.9182393550872803 30.975460052490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793782114982605 2.935049533843994 31.144275665283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793792486190796 3.034316301345825 32.13695526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922362089157104 2.8726489543914795 30.518726348876953
  batch 60 loss: 1.7922362089157104, 2.8726489543914795, 30.518726348876953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793521523475647 3.3782923221588135 35.576446533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911264896392822 3.448437452316284 36.2755012512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792249321937561 3.344465494155884 35.23690414428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791777491569519 3.4630448818206787 36.42222595214844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7929807901382446 2.5415704250335693 27.20868492126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792051911354065 2.9599385261535645 31.391435623168945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921110391616821 2.9078097343444824 30.870208740234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916992902755737 2.786242723464966 29.654125213623047
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792025089263916 2.6766724586486816 28.558748245239258
Total LOSS train 33.61814683767466 valid 30.118629455566406
CE LOSS train 1.7928492069244384 valid 0.448006272315979
Contrastive LOSS train 3.182529772244967 valid 0.6691681146621704
EPOCH 96:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926503419876099 3.1192867755889893 32.98551940917969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920747995376587 3.283440113067627 34.6264762878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935806512832642 2.8825976848602295 30.619558334350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931950092315674 3.035379648208618 32.14699172973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931807041168213 3.1431756019592285 33.224937438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794223666191101 3.6332056522369385 38.12628173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923474311828613 2.7830898761749268 29.623245239257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79189133644104 2.7837915420532227 29.629806518554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917364835739136 3.5258147716522217 37.04988479614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922751903533936 3.8110365867614746 39.90264129638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934696674346924 3.875624895095825 40.54971694946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933542728424072 3.858262062072754 40.375972747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941280603408813 3.2420871257781982 34.21500015258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793646216392517 3.3549818992614746 35.343467712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914460897445679 3.9721078872680664 41.51252365112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936053276062012 4.550500392913818 47.29861068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935631275177002 3.6965203285217285 38.758766174316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943627834320068 3.7738029956817627 39.53239059448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793005347251892 3.787280321121216 39.665809631347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791411280632019 3.121073007583618 33.002140045166016
  batch 20 loss: 1.791411280632019, 3.121073007583618, 33.002140045166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793562412261963 3.11814284324646 32.97499084472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557954788208 3.0859663486480713 32.6522216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925385236740112 2.739259958267212 29.185136795043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933557033538818 3.302975654602051 34.8231086730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792919397354126 3.911139965057373 40.904319763183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934433221817017 3.685344696044922 38.646888732910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943902015686035 3.213618278503418 33.93057632446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922658920288086 3.1273820400238037 33.06608581542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942405939102173 3.1056694984436035 32.85093307495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914046049118042 2.920032501220703 30.991729736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929834127426147 3.2360153198242188 34.15313720703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791089415550232 3.4455277919769287 36.24636459350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925848960876465 3.457646131515503 36.36904525756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930235862731934 3.192328929901123 33.716312408447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933181524276733 2.8034732341766357 29.82805061340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793440818786621 3.3122363090515137 34.91580581665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932817935943604 2.9199321269989014 30.992603302001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791966438293457 3.1401174068450928 33.19314193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792160153388977 3.027036428451538 32.06252670288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79198157787323 3.081874370574951 32.61072540283203
  batch 40 loss: 1.79198157787323, 3.081874370574951, 32.61072540283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792146921157837 3.22218918800354 34.014041900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928577661514282 3.1627137660980225 33.41999435424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924998998641968 3.1452906131744385 33.24540710449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911953926086426 3.231508731842041 34.10628128051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924067974090576 3.3239200115203857 35.03160858154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 3.4465463161468506 36.258052825927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923595905303955 3.7537448406219482 39.32980728149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917925119400024 2.9786746501922607 31.57853889465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917362451553345 3.683631658554077 38.6280517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916321754455566 2.7699368000030518 29.490999221801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904452085494995 3.2403204441070557 34.19364929199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932400703430176 2.961670160293579 31.409940719604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930495738983154 3.1616201400756836 33.40925216674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934684753417969 3.2830803394317627 34.624271392822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922285795211792 3.23341965675354 34.126426696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793274998664856 2.7182869911193848 28.976144790649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938231229782104 3.1708765029907227 33.502586364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794579029083252 2.8461179733276367 30.25575828552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7951631546020508 3.32798433303833 35.07500457763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934035062789917 2.420275926589966 25.99616241455078
  batch 60 loss: 1.7934035062789917, 2.420275926589966, 25.99616241455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944856882095337 3.1121442317962646 32.91592788696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915407419204712 3.32348370552063 35.0263786315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792452096939087 2.8275372982025146 30.067825317382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920349836349487 2.6019694805145264 27.811729431152344
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793026089668274 2.650501251220703 28.298038482666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921881675720215 3.122882127761841 33.0210075378418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918996810913086 3.3233375549316406 35.02527618408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917526960372925 2.813281297683716 29.924564361572266
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792081356048584 3.2480926513671875 34.273006439208984
Total LOSS train 34.20146654569186 valid 33.06096363067627
CE LOSS train 1.7928168003375713 valid 0.448020339012146
Contrastive LOSS train 3.2408649848057673 valid 0.8120231628417969
EPOCH 97:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925986051559448 3.365746021270752 35.45005798339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917957305908203 2.8318498134613037 30.110294342041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933825254440308 3.345275402069092 35.24613571166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929798364639282 3.1256680488586426 33.049659729003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927712202072144 3.9027485847473145 40.820255279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927831411361694 3.0477547645568848 32.270328521728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921608686447144 3.4551634788513184 36.34379577636719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918981313705444 2.337731122970581 25.16921043395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917232513427734 2.658696413040161 28.378686904907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923133373260498 3.6449217796325684 38.24153137207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933212518692017 2.9441943168640137 31.235265731811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792843222618103 2.6080803871154785 27.873645782470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940452098846436 3.1253058910369873 33.04710388183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925506830215454 3.471560001373291 36.508148193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7904525995254517 3.142005443572998 33.210506439208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933865785598755 3.7917098999023438 39.710487365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792805790901184 3.7954747676849365 39.74755096435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935091257095337 3.998791217803955 41.78142166137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926443815231323 3.3108465671539307 34.9011116027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916446924209595 2.841888904571533 30.210533142089844
  batch 20 loss: 1.7916446924209595, 2.841888904571533, 30.210533142089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930734157562256 2.7562294006347656 29.35536766052246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241943359375 3.2033989429473877 33.82640838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922791242599487 2.5109570026397705 26.90184783935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792874813079834 2.8240442276000977 30.03331756591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928547859191895 3.2458319664001465 34.25117492675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934013605117798 3.0849263668060303 32.64266586303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942029237747192 3.883742332458496 40.631629943847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926100492477417 3.272059679031372 34.513206481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943869829177856 2.487856864929199 26.672956466674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911690473556519 2.600322961807251 27.794397354125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793355941772461 2.760902166366577 29.40237808227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917183637619019 2.8973913192749023 30.7656307220459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928894758224487 2.4546830654144287 26.339719772338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792754054069519 2.8129403591156006 29.922157287597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926164865493774 3.060553789138794 32.398155212402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793210506439209 3.1298599243164062 33.0918083190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793177604675293 3.2963385581970215 34.75656509399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923229932785034 3.4289495944976807 36.08182144165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928510904312134 3.2588376998901367 34.381229400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792244553565979 2.850165843963623 30.293903350830078
  batch 40 loss: 1.792244553565979, 2.850165843963623, 30.293903350830078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924617528915405 3.515389919281006 36.94636154174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923831939697266 2.7538340091705322 29.33072280883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918232679367065 2.908695936203003 30.878782272338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909877300262451 2.8979382514953613 30.770368576049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930872440338135 3.0625579357147217 32.41866683959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792319416999817 3.4587035179138184 36.379356384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928965091705322 3.090784788131714 32.70074462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927824258804321 3.300518751144409 34.797969818115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792457938194275 3.61513352394104 37.94379425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928060293197632 2.9806458950042725 31.599266052246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909027338027954 3.551837682723999 37.30927658081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931252717971802 3.2387173175811768 34.1802978515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929935455322266 3.18266224861145 33.61961364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935681343078613 3.6291327476501465 38.084896087646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915409803390503 3.013827323913574 31.929813385009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924515008926392 2.5374059677124023 27.16651153564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937148809432983 3.2381293773651123 34.175010681152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941572666168213 4.020195960998535 41.996116638183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7946628332138062 3.1066126823425293 32.86079025268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922886610031128 3.19956636428833 33.7879524230957
  batch 60 loss: 1.7922886610031128, 3.19956636428833, 33.7879524230957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935982942581177 3.0302319526672363 32.095916748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915712594985962 2.993919610977173 31.73076629638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 3.2621896266937256 34.41460418701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929894924163818 2.994103193283081 31.73402214050293
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7937406301498413 2.3914666175842285 25.708406448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917228937149048 2.9225926399230957 31.017648696899414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916406393051147 3.2219841480255127 34.01148223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791325569152832 3.396317958831787 35.7545051574707
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7915551662445068 2.826273202896118 30.05428695678711
Total LOSS train 33.10649384718675 valid 32.70948076248169
CE LOSS train 1.7927083327220037 valid 0.4478887915611267
Contrastive LOSS train 3.1313785552978515 valid 0.7065683007240295
EPOCH 98:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921955585479736 3.04068660736084 32.19906234741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791139841079712 3.255854606628418 34.34968948364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928932905197144 2.8350939750671387 30.14383316040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930784225463867 3.4674417972564697 36.467498779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926183938980103 2.9468679428100586 31.26129722595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933624982833862 3.133723735809326 33.13059997558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918111085891724 3.563075304031372 37.42256546020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915748357772827 3.0778629779815674 32.57020568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915430068969727 3.3286848068237305 35.078392028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922966480255127 3.0240485668182373 32.03278350830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933728694915771 3.391792058944702 35.7112922668457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926478385925293 3.1486635208129883 33.27928161621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934668064117432 2.8565964698791504 30.35943031311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926946878433228 2.992171287536621 31.714406967163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906205654144287 3.1051738262176514 32.84235763549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932593822479248 3.592456579208374 37.71782302856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927100658416748 3.788400888442993 39.676719665527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934110164642334 3.269652843475342 34.48994064331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926318645477295 3.022448778152466 32.017120361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914808988571167 2.8949711322784424 30.741191864013672
  batch 20 loss: 1.7914808988571167, 2.8949711322784424, 30.741191864013672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79319167137146 3.029510259628296 32.088294982910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935487031936646 2.951131582260132 31.30486488342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934746742248535 3.572683572769165 37.52031326293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793428659439087 2.571276903152466 27.506196975708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926675081253052 3.540207624435425 37.19474411010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792741060256958 3.4800972938537598 36.593711853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938060760498047 3.443127155303955 36.225074768066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792232871055603 3.511134386062622 36.9035758972168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7950977087020874 3.598010301589966 37.77519989013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908501625061035 2.7249879837036133 29.040729522705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929043769836426 2.867737293243408 30.47027587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914600372314453 2.6436870098114014 28.228330612182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930599451065063 3.252948760986328 34.322547912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934592962265015 3.7179198265075684 38.972660064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927395105361938 3.652965784072876 38.3223991394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923325300216675 3.5489466190338135 37.28179931640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923880815505981 3.3568663597106934 35.361053466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791883111000061 3.1118271350860596 32.910152435302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793244481086731 3.804579019546509 39.83903503417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921587228775024 4.216501235961914 43.95717239379883
  batch 40 loss: 1.7921587228775024, 4.216501235961914, 43.95717239379883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926981449127197 3.577998161315918 37.57268142700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931855916976929 4.250988960266113 44.30307388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925587892532349 3.8342621326446533 40.13518142700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791258454322815 3.3089396953582764 34.88065719604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929376363754272 2.826599597930908 30.05893325805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792306900024414 2.729553461074829 29.087841033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919676303863525 3.460632085800171 36.39828872680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792075514793396 2.906630277633667 30.858379364013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79184889793396 3.689800977706909 38.689857482910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928440570831299 3.1797702312469482 33.590545654296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914049625396729 3.384087562561035 35.63227844238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939296960830688 3.529078722000122 37.084716796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935631275177002 3.3433566093444824 35.22713088989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933655977249146 2.911942720413208 30.912792205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791845440864563 3.180802345275879 33.59986877441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927502393722534 2.6964564323425293 28.757314682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932409048080444 2.690463066101074 28.697872161865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938700914382935 3.3406972885131836 35.200843811035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7948189973831177 3.6828224658966064 38.623043060302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793268084526062 3.2369418144226074 34.16268539428711
  batch 60 loss: 1.793268084526062, 3.2369418144226074, 34.16268539428711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7949912548065186 3.482982635498047 36.62481689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930119037628174 2.7505509853363037 29.29852294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944217920303345 3.423079013824463 36.025211334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940245866775513 3.4861252307891846 36.655277252197266
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7948641777038574 2.5052311420440674 26.84717559814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793066382408142 3.238438129425049 34.17744827270508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928560972213745 2.697185516357422 28.764711380004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925320863723755 2.8380837440490723 30.173368453979492
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7927733659744263 3.0829176902770996 32.621952056884766
Total LOSS train 34.36844021723821 valid 31.434370040893555
CE LOSS train 1.7928081732529861 valid 0.44819334149360657
Contrastive LOSS train 3.2575631911938006 valid 0.7707294225692749
EPOCH 99:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935357093811035 3.032381772994995 32.11735534667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922977209091187 3.489302635192871 36.685325622558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934510707855225 3.410268545150757 35.89613342285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792668342590332 3.7212960720062256 39.0056266784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921568155288696 3.2579104900360107 34.37126159667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933651208877563 3.279498815536499 34.58835220336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926586866378784 3.26309871673584 34.423648834228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931386232376099 2.9930098056793213 31.723236083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925989627838135 3.4795095920562744 36.58769607543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793331503868103 3.2585437297821045 34.37876892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937270402908325 2.9971117973327637 31.764846801757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929610013961792 3.0472235679626465 32.26519775390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937904596328735 3.3339617252349854 35.13340759277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934010028839111 3.5908119678497314 37.70151901245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917181253433228 3.0894715785980225 32.68643569946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794496774673462 3.347439765930176 35.26889419555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939016819000244 2.777385711669922 29.567758560180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939317226409912 2.88051700592041 30.599102020263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928310632705688 2.848978281021118 30.28261375427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792100191116333 2.460620164871216 26.39830207824707
  batch 20 loss: 1.792100191116333, 2.460620164871216, 26.39830207824707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79386568069458 2.709927558898926 28.89314079284668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940418720245361 3.4041178226470947 35.83522033691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793646216392517 3.009848117828369 31.892126083374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939801216125488 3.077723503112793 32.57121658325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793238639831543 3.3666415214538574 35.45965576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931742668151855 3.0496530532836914 32.289703369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938541173934937 3.460845947265625 36.402313232421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926439046859741 2.754171133041382 29.3343563079834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7954275608062744 3.138774871826172 33.18317794799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385220527649 3.9129855632781982 40.9222412109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934701442718506 3.5452983379364014 37.24645233154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916315793991089 3.780778646469116 39.59941482543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793146014213562 3.729261636734009 39.08576202392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793418288230896 3.2273223400115967 34.06664276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928876876831055 2.756439685821533 29.357284545898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934961318969727 3.007580041885376 31.86929702758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934173345565796 2.6632204055786133 28.425621032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920228242874146 3.08668851852417 32.65890884399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923634052276611 3.935302257537842 41.1453857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910751104354858 4.095786094665527 42.748939514160156
  batch 40 loss: 1.7910751104354858, 4.095786094665527, 42.748939514160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922199964523315 3.567845582962036 37.47067642211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926222085952759 3.390589714050293 35.69852066040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932748794555664 4.055371284484863 42.34698486328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921174764633179 3.886967182159424 40.66178894042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932313680648804 3.498870849609375 36.78194046020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923049926757812 3.330411195755005 35.09641647338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791987419128418 3.053837299346924 32.330360412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333960533142 2.7092437744140625 28.8847713470459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921959161758423 3.5459604263305664 37.25179672241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941274642944336 2.8657736778259277 30.451866149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921496629714966 3.862602710723877 40.418174743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929143905639648 3.181121826171875 33.60413360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927414178848267 2.7649741172790527 29.44248390197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933731079101562 3.5139029026031494 36.932403564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929400205612183 3.0085482597351074 31.8784236907959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793684720993042 3.2129263877868652 33.922950744628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940908670425415 3.3084664344787598 34.878753662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941421270370483 3.757532835006714 39.36947250366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944053411483765 2.890389919281006 30.698305130004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932652235031128 3.0147392749786377 31.940656661987305
  batch 60 loss: 1.7932652235031128, 3.0147392749786377, 31.940656661987305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938898801803589 3.1177103519439697 32.97099304199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792675256729126 3.331179141998291 35.10446548461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940740585327148 3.1386921405792236 33.18099594116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930067777633667 3.1948611736297607 33.74161911010742
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7934203147888184 3.072768449783325 32.52110290527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921103239059448 3.087517261505127 32.66728210449219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920279502868652 3.554110527038574 37.333133697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918028831481934 3.4061763286590576 35.85356521606445
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7915242910385132 3.115020513534546 32.94173049926758
Total LOSS train 34.33865230266864 valid 34.698927879333496
CE LOSS train 1.7931144090799185 valid 0.4478810727596283
Contrastive LOSS train 3.2545537801889273 valid 0.7787551283836365
EPOCH 100:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922776937484741 3.4497084617614746 36.289363861083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913912534713745 3.5901811122894287 37.693199157714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927261590957642 3.291531562805176 34.708038330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923589944839478 3.9693970680236816 41.486328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918766736984253 3.5746068954467773 37.53794860839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931101322174072 3.8078372478485107 39.871482849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791812777519226 4.078188896179199 42.57370376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917767763137817 3.6359305381774902 38.151084899902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909592390060425 3.248267412185669 34.27363204956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908129692077637 3.7092909812927246 38.883724212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791999340057373 3.3342981338500977 35.134979248046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792176365852356 3.9396016597747803 41.18819046020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938594818115234 2.9672234058380127 31.466093063354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793081283569336 3.0577168464660645 32.37024688720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903140783309937 3.083596706390381 32.62628173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920639514923096 3.6744821071624756 38.53688430786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919729948043823 3.6944572925567627 38.73654556274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922290563583374 3.868168354034424 40.47391128540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918846607208252 4.201437950134277 43.80626678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908660173416138 3.244795560836792 34.23882293701172
  batch 20 loss: 1.7908660173416138, 3.244795560836792, 34.23882293701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920829057693481 2.9072139263153076 30.86422348022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918404340744019 4.233733177185059 44.129173278808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922309637069702 2.749866247177124 29.2908935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927864789962769 2.8636720180511475 30.429506301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923661470413208 3.4904260635375977 36.696624755859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924816608428955 3.8449838161468506 40.24231719970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935713529586792 3.7694592475891113 39.488162994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918695211410522 3.5467982292175293 37.25985336303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938228845596313 3.1919922828674316 33.7137451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7906925678253174 3.58323335647583 37.623023986816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928636074066162 3.156367778778076 33.35654067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791817545890808 3.333845615386963 35.130271911621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933865785598755 3.0796895027160645 32.59028244018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940385341644287 2.8983099460601807 30.777137756347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930301427841187 3.217109441757202 33.9641227722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936755418777466 3.287663698196411 34.670310974121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934138774871826 2.8743927478790283 30.53734016418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792482614517212 3.1246707439422607 33.03919219970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928144931793213 3.1781163215637207 33.573978424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925281524658203 3.692413806915283 38.71666717529297
  batch 40 loss: 1.7925281524658203, 3.692413806915283, 38.71666717529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925044298171997 3.5233829021453857 37.02633285522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926214933395386 2.8193182945251465 29.98580551147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919282913208008 3.2556519508361816 34.348445892333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912039756774902 3.488419532775879 36.67539596557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793057918548584 3.3209831714630127 35.00288772583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923895120620728 3.3995156288146973 35.78754425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792104959487915 3.1846094131469727 33.63819885253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926198244094849 3.554476499557495 37.33738708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791917324066162 3.4936134815216064 36.728050231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792944312095642 2.956195592880249 31.354900360107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909306287765503 3.051002264022827 32.30095291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930489778518677 3.1681289672851562 33.47433853149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925546169281006 2.980559825897217 31.598154067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928712368011475 3.024775981903076 32.04063034057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791466236114502 3.509798526763916 36.88945007324219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929835319519043 3.7437193393707275 39.23017883300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933526039123535 3.577148914337158 37.564842224121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793671727180481 3.6156225204467773 37.94989776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941582202911377 3.5992205142974854 37.7863655090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926898002624512 3.070702314376831 32.49971389770508
  batch 60 loss: 1.7926898002624512, 3.070702314376831, 32.49971389770508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793737530708313 3.1374990940093994 33.16872787475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922232151031494 2.873257875442505 30.52480125427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79355788230896 3.166208028793335 33.45563888549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935725450515747 3.089355707168579 32.687129974365234
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7942135334014893 2.793126106262207 29.725475311279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923394441604614 3.505396842956543 36.846309661865234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919490337371826 3.3584306240081787 35.37625503540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920773029327393 3.591059446334839 37.70267105102539
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7923526763916016 3.8044848442077637 39.83720397949219
Total LOSS train 35.46032835153433 valid 37.4406099319458
CE LOSS train 1.7924872343356792 valid 0.4480881690979004
Contrastive LOSS train 3.3667841324439416 valid 0.9511212110519409
EPOCH 101:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792949914932251 3.475203275680542 36.54498291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921422719955444 3.3857052326202393 35.649192810058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937901020050049 3.46236515045166 36.41743850708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793448567390442 3.7675633430480957 39.46908187866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792786717414856 3.453279972076416 36.325584411621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937461137771606 3.782904863357544 39.62279510498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927429676055908 3.8950247764587402 40.74299240112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920953035354614 3.609225273132324 37.88434982299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922834157943726 3.5416648387908936 37.20893096923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791831135749817 3.6145613193511963 37.93744659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929294109344482 3.6213879585266113 38.00680923461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792226791381836 3.4630448818206787 36.42267608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935218811035156 3.3885676860809326 35.67919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927665710449219 3.3999693393707275 35.79246139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7903977632522583 3.400207757949829 35.792476654052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932976484298706 3.549403190612793 37.287330627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793026328086853 3.721118450164795 39.00421142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936047315597534 3.4375457763671875 36.169063568115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792802333831787 4.059071063995361 42.38351058959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918310165405273 3.7930009365081787 39.721839904785156
  batch 20 loss: 1.7918310165405273, 3.7930009365081787, 39.721839904785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936090230941772 3.5659658908843994 37.453269958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793163537979126 4.104348659515381 42.83665084838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931077480316162 2.861072301864624 30.403831481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933117151260376 3.968655824661255 41.4798698425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928224802017212 3.2969250679016113 34.7620735168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931251525878906 3.6998891830444336 38.79201889038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793960690498352 3.998288631439209 41.77684783935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792688012123108 3.967221736907959 41.46490478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794905662536621 3.942168951034546 41.21659469604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912977933883667 3.5981414318084717 37.77271270751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931897640228271 3.3744587898254395 35.537776947021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916653156280518 3.6201531887054443 37.993194580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793346643447876 3.3039093017578125 34.83243942260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935653924942017 3.2636544704437256 34.43010711669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932147979736328 3.0810706615448 32.603919982910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79367995262146 3.3847575187683105 35.64125442504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934378385543823 3.6695337295532227 38.488773345947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922415733337402 3.3824222087860107 35.61646270751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926416397094727 3.9264183044433594 41.05682373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923758029937744 3.6023175716400146 37.8155517578125
  batch 40 loss: 1.7923758029937744, 3.6023175716400146, 37.8155517578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925001382827759 2.873840570449829 30.53090476989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929002046585083 3.4129061698913574 35.92196273803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792479157447815 3.655074119567871 38.343223571777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916115522384644 3.609307050704956 37.884681701660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930402755737305 2.8748388290405273 30.541427612304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928377389907837 3.4625966548919678 36.41880416870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927285432815552 3.1756036281585693 33.54876708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923288345336914 3.413275957107544 35.925086975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922983169555664 3.7648658752441406 39.440956115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927496433258057 2.9305708408355713 31.09845733642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913198471069336 3.0599734783172607 32.391056060791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933812141418457 3.836602210998535 40.159400939941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929303646087646 3.235800266265869 34.15093231201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934913635253906 3.710967540740967 38.903167724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920019626617432 4.045609474182129 42.24809646606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930926084518433 3.5044138431549072 36.83723068237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932897806167603 4.131146430969238 43.10475158691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932662963867188 4.035417079925537 42.147438049316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938957214355469 3.8797075748443604 40.590972900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792524814605713 3.7066030502319336 38.858558654785156
  batch 60 loss: 1.792524814605713, 3.7066030502319336, 38.858558654785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937214374542236 3.8020248413085938 39.813968658447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916313409805298 4.067163944244385 42.4632682800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793170690536499 3.4760634899139404 36.55380630493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793000340461731 2.8731467723846436 30.52446937561035
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7936924695968628 2.56210994720459 27.414791107177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921432256698608 2.745445489883423 29.246597290039062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919784784317017 3.144239664077759 33.234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917404174804688 2.8493854999542236 30.285594940185547
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920631170272827 3.3362770080566406 35.15483474731445
Total LOSS train 37.259286675086386 valid 31.980350494384766
CE LOSS train 1.792853172008808 valid 0.4480157792568207
Contrastive LOSS train 3.546643356176523 valid 0.8340692520141602
EPOCH 102:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925704717636108 3.221080780029297 34.003379821777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916996479034424 2.955129384994507 31.342994689941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932595014572144 3.0082905292510986 31.876163482666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792924165725708 2.730334520339966 29.096269607543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927922010421753 3.619584321975708 37.9886360168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793574333190918 3.2134368419647217 33.92794418334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792742133140564 3.326935291290283 35.062095642089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924001216888428 3.2963311672210693 34.75571060180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922184467315674 3.721376657485962 39.005985260009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925485372543335 3.767944097518921 39.471988677978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934387922286987 3.854189395904541 40.335330963134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928693294525146 3.7263097763061523 39.05596923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941815853118896 3.54121470451355 37.206329345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934550046920776 3.3591086864471436 35.38454055786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907823324203491 3.3556857109069824 35.34764099121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934072017669678 3.6271848678588867 38.06525802612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793238878250122 3.640183210372925 38.195072174072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934404611587524 3.4826087951660156 36.619529724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79287850856781 3.7422845363616943 39.215721130371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918918132781982 3.2776944637298584 34.5688362121582
  batch 20 loss: 1.7918918132781982, 3.2776944637298584, 34.5688362121582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793461561203003 2.975022554397583 31.543685913085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932709455490112 3.2234489917755127 34.02775955200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793306827545166 3.4557406902313232 36.35071563720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936395406723022 3.352182626724243 35.315467834472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793075442314148 3.8125147819519043 39.9182243347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934247255325317 3.6716341972351074 38.509769439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941142320632935 3.3231418132781982 35.02553176879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927067279815674 2.980370283126831 31.596410751342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7948085069656372 3.7757623195648193 39.55242919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791211485862732 3.0377020835876465 32.16823196411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793044090270996 3.1894352436065674 33.68739700317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914358377456665 3.7096667289733887 38.88810348510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931355237960815 3.607903003692627 37.87216567993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793481469154358 3.3035974502563477 34.82945251464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932710647583008 3.418433666229248 35.97760772705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793441653251648 3.762852668762207 39.421966552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932043075561523 3.421405792236328 36.00726318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920852899551392 2.9437222480773926 31.229307174682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925591468811035 3.214860439300537 33.941165924072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792438268661499 2.754932403564453 29.34176254272461
  batch 40 loss: 1.792438268661499, 2.754932403564453, 29.34176254272461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927169799804688 2.693835973739624 28.731077194213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928197383880615 2.845618486404419 30.249004364013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792559266090393 2.9656178951263428 31.44873809814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791754126548767 3.321291208267212 35.004669189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931406497955322 3.2690582275390625 34.48372268676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928682565689087 3.239917755126953 34.192047119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925692796707153 3.9882493019104004 41.6750602722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792130708694458 3.7688915729522705 39.48104476928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922924757003784 3.1433982849121094 33.22627639770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927111387252808 2.960059881210327 31.393310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791212797164917 3.555769681930542 37.34891128540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933238744735718 3.1207916736602783 33.00123977661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927895784378052 2.577556848526001 27.568357467651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933133840560913 2.993670701980591 31.73002052307129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920650243759155 2.8421058654785156 30.213123321533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930035591125488 3.4941494464874268 36.7344970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934719324111938 3.0240378379821777 32.033851623535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936807870864868 3.867784023284912 40.47152328491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794141411781311 2.90268611907959 30.821002960205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926406860351562 3.40449857711792 35.83762741088867
  batch 60 loss: 1.7926406860351562, 3.40449857711792, 35.83762741088867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939653396606445 3.2398765087127686 34.19272994995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917943000793457 3.603215217590332 37.823944091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931767702102661 3.1833596229553223 33.626773834228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 3.5359885692596436 37.15270233154297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7935738563537598 2.6601243019104004 28.39481544494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792068362236023 2.9298365116119385 31.09043312072754
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 2.7754085063934326 29.545988082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916722297668457 2.3860366344451904 25.65203857421875
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792048454284668 2.0560219287872314 22.35226821899414
Total LOSS train 34.96255208528959 valid 27.160181999206543
CE LOSS train 1.7928917426329392 valid 0.448012113571167
Contrastive LOSS train 3.316966020143949 valid 0.5140054821968079
EPOCH 103:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600393295288 2.7523458003997803 29.316059112548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917492389678955 3.468020439147949 36.471954345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932767868041992 3.441179037094116 36.2050666809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792966604232788 3.401580333709717 35.80876922607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927354574203491 3.015040636062622 31.94314193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935270071029663 3.2390167713165283 34.183692932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926350831985474 2.9404454231262207 31.19708824157715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924771308898926 3.094386577606201 32.73634338378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792230248451233 3.068831205368042 32.48054122924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254150390625 2.274413585662842 24.536678314208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935245037078857 2.2792088985443115 24.585613250732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929242849349976 3.6049134731292725 37.84205627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942427396774292 3.711822271347046 38.9124641418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934210300445557 3.5782978534698486 37.57640075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908424139022827 2.906123161315918 30.852073669433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934991121292114 3.6513831615448 38.30733108520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793059229850769 3.0620687007904053 32.41374588012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793285846710205 3.5984013080596924 37.77729797363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927346229553223 3.7538564205169678 39.331298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917996644973755 2.8770902156829834 30.562702178955078
  batch 20 loss: 1.7917996644973755, 2.8770902156829834, 30.562702178955078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934503555297852 3.0937280654907227 32.73073196411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932149171829224 3.2855443954467773 34.64866256713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932085990905762 3.2799811363220215 34.593021392822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934420108795166 3.8893392086029053 40.686832427978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928621768951416 3.670182466506958 38.494686126708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932130098342896 3.431097984313965 36.10419464111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794023036956787 3.898420810699463 40.778228759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924506664276123 3.6242058277130127 38.034507751464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7945369482040405 3.819824695587158 39.99278259277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791182041168213 3.4814095497131348 36.60527801513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930169105529785 3.293238878250122 34.725406646728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791480541229248 3.5783417224884033 37.57489776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932173013687134 3.5086722373962402 36.879940032958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934767007827759 3.6790974140167236 38.584449768066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932696342468262 3.7202789783477783 38.99605941772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935587167739868 3.840491771697998 40.19847869873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932194471359253 3.431548595428467 36.108707427978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920961380004883 2.8775863647460938 30.56795883178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924683094024658 3.625018358230591 38.04265213012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922662496566772 3.224123477935791 34.03350067138672
  batch 40 loss: 1.7922662496566772, 3.224123477935791, 34.03350067138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926454544067383 3.2575180530548096 34.36782455444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792886734008789 2.413853883743286 25.931425094604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925586700439453 3.8253440856933594 40.045997619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916678190231323 3.453991413116455 36.331581115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930432558059692 3.5905425548553467 37.69847106933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928606271743774 3.1863436698913574 33.65629959106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792577862739563 3.414301633834839 35.93559646606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922735214233398 3.1981117725372314 33.77339172363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792065978050232 3.0663695335388184 32.45576095581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925599813461304 3.736758232116699 39.1601448059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910802364349365 3.4810569286346436 36.60165023803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931900024414062 3.462993621826172 36.423126220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928357124328613 3.287501573562622 34.667850494384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933820486068726 3.606131076812744 37.85469055175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922165393829346 2.8965330123901367 30.75754737854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793094515800476 3.377628803253174 35.56938171386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79343843460083 3.2832117080688477 34.625553131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935421466827393 3.085449695587158 32.64803695678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939313650131226 3.1531193256378174 33.32512283325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923928499221802 2.9972877502441406 31.765270233154297
  batch 60 loss: 1.7923928499221802, 2.9972877502441406, 31.765270233154297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934668064117432 3.327256679534912 35.066036224365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915114164352417 3.0722568035125732 32.51408004760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929542064666748 2.583621025085449 27.62916374206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926384210586548 2.575477361679077 27.547412872314453
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793310523033142 2.1110775470733643 22.904085159301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791951060295105 3.1104652881622314 32.896602630615234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918235063552856 2.652512788772583 28.316951751708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916150093078613 2.5842843055725098 27.634456634521484
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918047904968262 2.6730539798736572 28.5223445892334
Total LOSS train 34.62579689025879 valid 29.342588901519775
CE LOSS train 1.7928284883499146 valid 0.44795119762420654
Contrastive LOSS train 3.2832968455094558 valid 0.6682634949684143
EPOCH 104:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924079895019531 2.3201746940612793 24.994155883789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915594577789307 3.026862621307373 32.06018829345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793163537979126 3.009366273880005 31.886825561523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927677631378174 3.394179582595825 35.734561920166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926307916641235 2.9353110790252686 31.145742416381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935450077056885 2.6198227405548096 27.991771697998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926244735717773 2.3860700130462646 25.653324127197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923526763916016 3.102581024169922 32.81816101074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921688556671143 2.7894673347473145 29.68684196472168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792345404624939 3.380115032196045 35.59349822998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934660911560059 2.934697151184082 31.140438079833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928334474563599 2.9963269233703613 31.756101608276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940646409988403 2.760038137435913 29.394447326660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793318510055542 2.6154565811157227 27.94788360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907601594924927 3.3486573696136475 35.2773323059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933528423309326 3.9075894355773926 40.86924743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793100357055664 3.768103837966919 39.47413635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932671308517456 3.1869680881500244 33.66294860839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927364110946655 3.1816275119781494 33.609012603759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916591167449951 3.51981258392334 36.98978805541992
  batch 20 loss: 1.7916591167449951, 3.51981258392334, 36.98978805541992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932366132736206 3.596112012863159 37.754356384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930492162704468 4.003314018249512 41.82619094848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930389642715454 3.1586754322052 33.379791259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933627367019653 3.2320849895477295 34.11421203613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927696704864502 3.4477241039276123 36.27001190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793108344078064 3.1276259422302246 33.06937026977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939046621322632 3.563154697418213 37.42544937133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925115823745728 2.9625189304351807 31.417699813842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794405460357666 2.8964881896972656 30.759286880493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912354469299316 3.071906805038452 32.51030349731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931324243545532 3.1675608158111572 33.4687385559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915077209472656 3.347052812576294 35.26203536987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931195497512817 3.220595598220825 33.99907684326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933428287506104 3.6510095596313477 38.303436279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930983304977417 3.398622751235962 35.779327392578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934598922729492 3.8318140506744385 40.111602783203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933132648468018 3.4450347423553467 36.24365997314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920523881912231 3.2656214237213135 34.44826889038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924363613128662 3.6907429695129395 38.69986343383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921899557113647 3.686063289642334 38.65282440185547
  batch 40 loss: 1.7921899557113647, 3.686063289642334, 38.65282440185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792603850364685 3.291670799255371 34.709312438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929869890213013 3.117391347885132 32.96689987182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792677402496338 2.987729072570801 31.669967651367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917088270187378 3.3369839191436768 35.16154861450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930043935775757 3.7616095542907715 39.40909957885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79279625415802 3.300593852996826 34.798736572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925269603729248 3.451655149459839 36.309078216552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922542095184326 3.3361775875091553 35.154029846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920749187469482 2.46928071975708 26.484880447387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925533056259155 2.7492806911468506 29.28536033630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791037917137146 3.59501051902771 37.74114227294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932300567626953 3.109158754348755 32.88481903076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792907476425171 2.45885968208313 26.38150405883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934224605560303 3.653881788253784 38.33224105834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922534942626953 3.510633707046509 36.898590087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931588888168335 3.254847764968872 34.341636657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934720516204834 3.3781206607818604 35.57468032836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933646440505981 3.639113664627075 38.18450164794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939341068267822 3.0026655197143555 31.820589065551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923997640609741 2.219726085662842 23.989662170410156
  batch 60 loss: 1.7923997640609741, 2.219726085662842, 23.989662170410156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935198545455933 2.7834651470184326 29.628171920776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79171621799469 3.829657793045044 40.088294982910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793157696723938 3.1349875926971436 33.143035888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792860746383667 2.488966226577759 26.68252182006836
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7935855388641357 3.109689950942993 32.89048385620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921167612075806 3.168203115463257 33.47414779663086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919236421585083 3.7342331409454346 39.134254455566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917335033416748 3.4304354190826416 36.09608840942383
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920193672180176 3.160076379776001 33.392784118652344
Total LOSS train 33.780195412269 valid 35.52431869506836
CE LOSS train 1.7927939708416278 valid 0.4480048418045044
Contrastive LOSS train 3.198740133872399 valid 0.7900190949440002
EPOCH 105:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926275730133057 3.1295323371887207 33.08795166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917507886886597 3.736046552658081 39.152217864990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931689023971558 3.1180830001831055 32.9739990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927082777023315 3.3865466117858887 35.65817642211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79258394241333 3.2559213638305664 34.3517951965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935678958892822 3.869706869125366 40.49063491821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925727367401123 4.018224716186523 41.97481918334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923678159713745 2.9107768535614014 30.900136947631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792042851448059 3.135324239730835 33.145286560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923239469528198 3.5559275150299072 37.351600646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932723760604858 3.5660271644592285 37.45354461669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792833685874939 3.9804484844207764 41.597320556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940975427627563 3.5715441703796387 37.50954055786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793437123298645 2.9499642848968506 31.293081283569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908517122268677 2.6088876724243164 27.879728317260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934626340866089 3.106006145477295 32.85352325439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79307222366333 2.9416513442993164 31.209585189819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932260036468506 2.9043030738830566 30.83625602722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792705774307251 3.265132427215576 34.44403076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791671872138977 3.206293821334839 33.854610443115234
  batch 20 loss: 1.791671872138977, 3.206293821334839, 33.854610443115234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933578491210938 3.3100247383117676 34.89360427856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930934429168701 2.882725238800049 30.620344161987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793099045753479 2.1853058338165283 23.646156311035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79345703125 2.893764019012451 30.731098175048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929390668869019 2.943758249282837 31.230520248413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932177782058716 3.1804521083831787 33.597740173339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938636541366577 3.584944725036621 37.643314361572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923524379730225 3.180720329284668 33.59955596923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7946010828018188 3.4931459426879883 36.72605895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913682460784912 2.983386993408203 31.6252384185791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931519746780396 3.5131442546844482 36.92459487915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791428565979004 3.446733236312866 36.258758544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930387258529663 3.8312110900878906 40.10514831542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793189525604248 3.688997983932495 38.683170318603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793068289756775 3.063525676727295 32.42832565307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935162782669067 3.853008270263672 40.32360076904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932395935058594 3.417039155960083 35.96363067626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920387983322144 3.8624961376190186 40.41699981689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926342487335205 3.949665069580078 41.289283752441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920925617218018 3.8019556999206543 39.811649322509766
  batch 40 loss: 1.7920925617218018, 3.8019556999206543, 39.811649322509766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924902439117432 3.1855392456054688 33.647884368896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926757335662842 3.75813364982605 39.3740119934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922966480255127 3.821402072906494 40.006317138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914705276489258 3.3158605098724365 34.9500732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928768396377563 3.3718767166137695 35.51164245605469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925877571105957 3.4166531562805176 35.9591178894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 3.551318645477295 37.30555725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792201042175293 3.2679264545440674 34.471466064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917027473449707 3.2903144359588623 34.694847106933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924575805664062 3.54809308052063 37.27338790893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790881633758545 3.8578989505767822 40.369873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79302978515625 3.711703300476074 38.910064697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792728304862976 3.2258639335632324 34.051368713378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932366132736206 3.7826998233795166 39.620235443115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921556234359741 3.6901230812072754 38.69338607788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931911945343018 3.480123281478882 36.59442138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934075593948364 3.321910858154297 35.012516021728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79349684715271 3.522700786590576 37.020503997802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938841581344604 3.3016982078552246 34.81086730957031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924368381500244 3.0039546489715576 31.83198356628418
  batch 60 loss: 1.7924368381500244, 3.0039546489715576, 31.83198356628418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793459415435791 3.5400307178497314 37.19376754760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916194200515747 3.4607255458831787 36.39887237548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931077480316162 3.538883924484253 37.18194580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792791485786438 3.455101728439331 36.34381103515625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793447494506836 2.8472843170166016 30.26629066467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 3.6402578353881836 38.19464111328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919397354125977 3.9348299503326416 41.14023971557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917014360427856 3.405362844467163 35.84532928466797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918028831481934 3.1701934337615967 33.493736267089844
Total LOSS train 35.569705317570616 valid 37.16848659515381
CE LOSS train 1.7927553268579337 valid 0.44795072078704834
Contrastive LOSS train 3.3776949919187107 valid 0.7925483584403992
EPOCH 106:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924187183380127 4.191800117492676 43.710418701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915306091308594 3.6276307106018066 38.06783676147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929874658584595 3.4444892406463623 36.23788070678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79269540309906 3.5724191665649414 37.516883850097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924575805664062 3.5317704677581787 37.11016082763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934690713882446 3.748631477355957 39.279781341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924655675888062 3.814208507537842 39.93455123901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923022508621216 3.6186342239379883 37.978641510009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918893098831177 4.03504753112793 42.142364501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921806573867798 3.8789000511169434 40.581180572509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931023836135864 3.228665828704834 34.07976150512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792670726776123 4.092069149017334 42.71336364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938708066940308 3.854783535003662 40.341705322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930852174758911 3.6369760036468506 38.162845611572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79073965549469 3.6121718883514404 37.91246032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793520450592041 3.6993017196655273 38.78654098510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932814359664917 3.4409186840057373 36.20246887207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933975458145142 3.2486910820007324 34.28030776977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926521301269531 3.461909532546997 36.411746978759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915339469909668 3.50801157951355 36.87165069580078
  batch 20 loss: 1.7915339469909668, 3.50801157951355, 36.87165069580078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931383848190308 3.1542210578918457 33.33534622192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929080724716187 3.3915460109710693 35.70836639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928701639175415 3.2670094966888428 34.46296310424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933567762374878 3.170499563217163 33.49835205078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928446531295776 3.6409246921539307 38.202091217041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930161952972412 3.4147534370422363 35.940547943115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938178777694702 3.4065237045288086 35.85905838012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923827171325684 3.472419500350952 36.51657485961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944910526275635 3.8745276927948 40.53976821899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912569046020508 3.7201762199401855 38.993019104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931071519851685 3.1193976402282715 32.987083435058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791439175605774 3.87019944190979 40.49343490600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793121576309204 3.462296962738037 36.41609191894531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933342456817627 3.336533546447754 35.158668518066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793125033378601 3.1980948448181152 33.77407455444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935386896133423 3.1416282653808594 33.20981979370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793198585510254 3.493407964706421 36.72727584838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918940782546997 3.3249974250793457 35.041866302490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925063371658325 3.799391746520996 39.78642654418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792092204093933 3.93408465385437 41.132938385009766
  batch 40 loss: 1.792092204093933, 3.93408465385437, 41.132938385009766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792488932609558 3.2022922039031982 33.81541061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928414344787598 3.0009541511535645 31.80238151550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925368547439575 3.1882803440093994 33.67533874511719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916845083236694 3.149752616882324 33.28921127319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930347919464111 3.2859342098236084 34.652374267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792630910873413 3.2837090492248535 34.62971878051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792305588722229 3.574977397918701 37.54207992553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921444177627563 3.382169485092163 35.61383819580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916367053985596 3.313493013381958 34.92656707763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924017906188965 3.3578057289123535 35.37045669555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908381223678589 3.416513204574585 35.95596694946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792960286140442 3.2547662258148193 34.34062194824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927051782608032 3.2914979457855225 34.70768356323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931616306304932 3.737623691558838 39.16939926147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921631336212158 3.2603747844696045 34.395912170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931535243988037 3.3509671688079834 35.30282211303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934247255325317 3.4396402835845947 36.1898307800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934354543685913 3.509490489959717 36.88833999633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938975095748901 3.643242835998535 38.226322174072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924168109893799 2.9205970764160156 30.998388290405273
  batch 60 loss: 1.7924168109893799, 2.9205970764160156, 30.998388290405273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934231758117676 3.043018102645874 32.22360610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917375564575195 3.626295566558838 38.054691314697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931562662124634 3.4059975147247314 35.85313034057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792725682258606 3.115729808807373 32.95002365112305
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7933329343795776 3.2871005535125732 34.664337158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919633388519287 2.8333706855773926 30.125669479370117
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.908198356628418 30.873821258544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915493249893188 2.9799346923828125 31.590896606445312
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917630672454834 2.422356605529785 26.015329360961914
Total LOSS train 36.482227266751806 valid 29.651429176330566
CE LOSS train 1.7927065959343544 valid 0.44794076681137085
Contrastive LOSS train 3.468952120267428 valid 0.6055891513824463
EPOCH 107:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923706769943237 3.1673357486724854 33.465728759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791494607925415 3.434281587600708 36.13431167602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929296493530273 3.669693946838379 38.4898681640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79264235496521 3.4291129112243652 36.083770751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924450635910034 3.261439561843872 34.40684127807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935571670532227 2.9439268112182617 31.232826232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922258377075195 3.4195969104766846 35.98819351196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922598123550415 2.5815534591674805 27.6077938079834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918111085891724 3.330702543258667 35.098838806152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792176365852356 3.7120070457458496 38.91224670410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932759523391724 3.2470388412475586 34.263668060302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928197383880615 3.3752076625823975 35.54489517211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939571142196655 3.1061630249023438 32.855587005615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931276559829712 3.1169276237487793 32.962406158447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790701985359192 3.3245837688446045 35.03654098510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933447360992432 3.7451469898223877 39.244815826416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931077480316162 3.434840679168701 36.14151382446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793196678161621 3.3235864639282227 35.02906036376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926158905029297 3.2090556621551514 33.88317108154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916091680526733 3.735696315765381 39.14857482910156
  batch 20 loss: 1.7916091680526733, 3.735696315765381, 39.14857482910156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932571172714233 3.4933383464813232 36.72664260864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929954528808594 3.0799248218536377 32.59224319458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928882837295532 2.781787633895874 29.61076545715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933003902435303 3.175309419631958 33.54639434814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928822040557861 3.5239098072052 37.031978607177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930399179458618 3.8015873432159424 39.80891418457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938340902328491 3.4163687229156494 35.957523345947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922199964523315 3.469747304916382 36.48969268798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943304777145386 3.3753058910369873 35.54738998413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911521196365356 3.5518438816070557 37.309593200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793066382408142 3.8194117546081543 39.987186431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913668155670166 3.49238920211792 36.71525955200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929266691207886 2.7530477046966553 29.32340431213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932677268981934 3.013949394226074 31.932762145996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793104887008667 2.969271659851074 31.485820770263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793359637260437 3.1710524559020996 33.503883361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793107271194458 3.200152635574341 33.79463195800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917369604110718 2.8123843669891357 29.91558074951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922941446304321 3.251629590988159 34.308589935302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.9398577213287354 31.19041633605957
  batch 40 loss: 1.7918387651443481, 2.9398577213287354, 31.19041633605957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523980140686 2.6422119140625 28.214643478393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929142713546753 3.314742088317871 34.940338134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924528121948242 3.0262701511383057 32.055152893066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791738510131836 3.429102659225464 36.082763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793020486831665 3.110553741455078 32.8985595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925701141357422 2.8944242000579834 30.736812591552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 2.9580459594726562 31.372835159301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921233177185059 3.8877182006835938 40.66930389404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918477058410645 2.9903767108917236 31.695613861083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923935651779175 2.235039472579956 24.14278793334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909314632415771 2.332811117172241 25.119043350219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792978286743164 2.7615058422088623 29.408037185668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792697548866272 2.8254663944244385 30.047361373901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932432889938354 3.5576295852661133 37.369537353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920923233032227 2.893545389175415 30.72754669189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930928468704224 2.9961328506469727 31.75442123413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934081554412842 3.0941555500030518 32.734962463378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793445348739624 2.745959997177124 29.2530460357666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940292358398438 3.344506025314331 35.23908996582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792483925819397 3.391937017440796 35.71185302734375
  batch 60 loss: 1.792483925819397, 3.391937017440796, 35.71185302734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934309244155884 3.218071937561035 33.97414779663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917531728744507 2.941009759902954 31.20185089111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793107509613037 3.6504082679748535 38.29718780517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926360368728638 3.1191627979278564 32.9842643737793
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7934194803237915 2.5055835247039795 26.849254608154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921110391616821 3.753556489944458 39.327674865722656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791922688484192 3.481553554534912 36.607460021972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917017936706543 3.6281213760375977 38.072914123535156
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920045852661133 2.505481243133545 26.846817016601562
Total LOSS train 33.719842177170975 valid 35.21371650695801
CE LOSS train 1.7926822809072642 valid 0.4480011463165283
Contrastive LOSS train 3.1927159749544582 valid 0.6263703107833862
EPOCH 108:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926077842712402 3.8529651165008545 40.32225799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916077375411987 3.484745502471924 36.639060974121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930591106414795 3.053647756576538 32.32953643798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928245067596436 2.874063014984131 30.53345489501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923593521118164 2.9578161239624023 31.370521545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934799194335938 2.9777162075042725 31.570642471313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922173738479614 3.543487071990967 37.227088928222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921804189682007 3.495572805404663 36.74790573120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919970750808716 2.652773857116699 28.31973648071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921364307403564 2.5637848377227783 27.42998504638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793437123298645 2.322108507156372 25.014522552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792953610420227 2.468039035797119 26.473342895507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939000129699707 2.688246488571167 28.67636489868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933419942855835 3.424706220626831 36.0404052734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909584045410156 3.1947786808013916 33.738746643066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793552279472351 3.2411820888519287 34.20537185668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933145761489868 2.5283262729644775 27.076576232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932671308517456 2.7382190227508545 29.175457000732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792922019958496 3.583416700363159 37.62709045410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917277812957764 3.119617223739624 32.98789978027344
  batch 20 loss: 1.7917277812957764, 3.119617223739624, 32.98789978027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793121576309204 3.1034090518951416 32.827213287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930115461349487 3.405214548110962 35.845157623291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792921781539917 2.6190314292907715 27.98323631286621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933721542358398 3.18394136428833 33.63278579711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928590774536133 3.132366180419922 33.116519927978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931143045425415 3.4184465408325195 35.977577209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936826944351196 3.1687088012695312 33.480770111083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922250032424927 2.842411518096924 30.216339111328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944395542144775 3.773106813430786 39.525508880615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913953065872192 3.300884485244751 34.80024337768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932220697402954 3.0262644290924072 32.05586624145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915658950805664 3.341632127761841 35.2078857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932298183441162 4.186837673187256 43.66160583496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934107780456543 3.5679972171783447 37.473384857177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793119192123413 3.5189290046691895 36.98240661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793318748474121 3.68945050239563 38.68782424926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932624816894531 3.39510178565979 35.74428176879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918990850448608 3.4218714237213135 36.010616302490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792537808418274 3.51303768157959 36.922916412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920931577682495 3.835357427597046 40.14566421508789
  batch 40 loss: 1.7920931577682495, 3.835357427597046, 40.14566421508789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927067279815674 3.5629167556762695 37.421871185302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929298877716064 2.7935118675231934 29.728050231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924938201904297 3.2915098667144775 34.70759582519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918438911437988 3.0752627849578857 32.544471740722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793188452720642 3.4740192890167236 36.533382415771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792627215385437 3.613136053085327 37.92398452758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925310134887695 3.4204840660095215 35.997371673583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922221422195435 3.045478582382202 32.24700927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916265726089478 3.7772810459136963 39.56443786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924509048461914 3.7187211513519287 38.97966003417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910016775131226 3.6423327922821045 38.21432876586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929699420928955 3.392287254333496 35.715843200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927346229553223 3.509526491165161 36.88800048828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930833101272583 3.321361780166626 35.0067024230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921185493469238 3.5766196250915527 37.55831527709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931627035140991 2.9307713508605957 31.100875854492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933847904205322 3.2938058376312256 34.731441497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933971881866455 4.229039669036865 44.08379364013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938894033432007 3.1954331398010254 33.74821853637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925596237182617 3.249654531478882 34.28910446166992
  batch 60 loss: 1.7925596237182617, 3.249654531478882, 34.28910446166992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935659885406494 3.3674874305725098 35.468441009521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919905185699463 3.220052480697632 33.992515563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933731079101562 2.9581918716430664 31.37529182434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792509913444519 3.603316307067871 37.82567596435547
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7932392358779907 3.291189193725586 34.70513153076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918745279312134 3.7408502101898193 39.200374603271484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791675329208374 3.0969760417938232 32.761436462402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914526462554932 3.6801552772521973 38.5930061340332
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791916012763977 3.1743521690368652 33.535438537597656
Total LOSS train 34.525465950599084 valid 36.02256393432617
CE LOSS train 1.7927576596920307 valid 0.44797900319099426
Contrastive LOSS train 3.2732708270733175 valid 0.7935880422592163
EPOCH 109:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792396903038025 3.4284727573394775 36.077125549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915902137756348 3.7626185417175293 39.41777801513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931052446365356 3.4231209754943848 36.024314880371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792969822883606 3.118513822555542 32.97810745239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924613952636719 3.213385820388794 33.92631912231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935609817504883 3.483140230178833 36.624961853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792178988456726 3.4879541397094727 36.67171859741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924349308013916 3.3965439796447754 35.75787353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920746803283691 3.1038899421691895 32.83097457885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921265363693237 3.2028326988220215 33.82045364379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934049367904663 3.1473658084869385 33.26706314086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929787635803223 2.8568367958068848 30.361345291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793857216835022 3.532848596572876 37.122344970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932898998260498 3.654304027557373 38.33633041381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908046245574951 3.8576178550720215 40.36698532104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932769060134888 3.2823941707611084 34.617218017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931181192398071 3.2041118144989014 33.83423614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932014465332031 3.2642271518707275 34.43547439575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926989793777466 3.521559238433838 37.0082893371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916724681854248 3.565356731414795 37.44524002075195
  batch 20 loss: 1.7916724681854248, 3.565356731414795, 37.44524002075195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932405471801758 2.995624542236328 31.74948501586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793056607246399 3.8056640625 39.84969711303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929108142852783 2.898383855819702 30.776750564575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932803630828857 3.193495035171509 33.72822952270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927874326705933 3.4028983116149902 35.82176971435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930188179016113 2.8416714668273926 30.209732055664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936896085739136 2.7406511306762695 29.2002010345459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923067808151245 2.6312763690948486 28.105070114135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794414758682251 2.8941192626953125 30.735607147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914071083068848 3.6966261863708496 38.75767135620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793002724647522 3.305326223373413 34.84626388549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791359305381775 3.320098400115967 34.99234390258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930068969726562 3.4743146896362305 36.536155700683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933236360549927 3.078460216522217 32.57792663574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931429147720337 3.7448625564575195 39.24176788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934597730636597 3.3041574954986572 34.83503723144531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793257236480713 3.1627039909362793 33.4202995300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919212579727173 3.2898166179656982 34.690086364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925097942352295 3.412933826446533 35.92184829711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920405864715576 3.563781499862671 37.42985534667969
  batch 40 loss: 1.7920405864715576, 3.563781499862671, 37.42985534667969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925719022750854 3.707615375518799 38.86872482299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928682565689087 3.096449136734009 32.75735855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924498319625854 3.373176336288452 35.52421188354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915822267532349 3.56774640083313 37.46904754638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929496765136719 3.5579512119293213 37.37246322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925657033920288 3.897338390350342 40.76594924926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923659086227417 3.256596326828003 34.35832977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792126178741455 3.694251537322998 38.734642028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915582656860352 3.244229793548584 34.233856201171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792425274848938 3.475574493408203 36.54817199707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910432815551758 3.341040849685669 35.20145034790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930552959442139 3.057523488998413 32.368289947509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792734146118164 3.378530740737915 35.578041076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931488752365112 3.5880401134490967 37.67354965209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792061686515808 3.027954339981079 32.07160568237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930413484573364 3.3897883892059326 35.69092559814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793396234512329 3.2933497428894043 34.72689437866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793412446975708 3.1721739768981934 33.51515197753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939280271530151 3.978135347366333 41.575279235839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924830913543701 2.941091299057007 31.20339584350586
  batch 60 loss: 1.7924830913543701, 2.941091299057007, 31.20339584350586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934520244598389 3.195976495742798 33.75321578979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791770100593567 3.15622878074646 33.35405731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930357456207275 3.6918299198150635 38.71133804321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925432920455933 3.9468536376953125 41.261077880859375
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7934315204620361 2.4441821575164795 26.235254287719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792132019996643 3.5355522632598877 37.14765548706055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919766902923584 3.057511568069458 32.36709213256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791711449623108 3.6437175273895264 38.228885650634766
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919933795928955 2.713667869567871 28.928672790527344
Total LOSS train 35.13695746201735 valid 34.168076515197754
CE LOSS train 1.792712928698613 valid 0.4479983448982239
Contrastive LOSS train 3.334424448013306 valid 0.6784169673919678
EPOCH 110:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926347255706787 3.350605010986328 35.298683166503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917004823684692 3.5731348991394043 37.52305221557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930350303649902 3.727853298187256 39.07156753540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79274582862854 3.3243136405944824 35.035884857177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924299240112305 3.3247978687286377 35.040409088134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934616804122925 3.1003239154815674 32.79669952392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923072576522827 3.25604510307312 34.352760314941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792235016822815 3.577923536300659 37.57147216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919883728027344 3.110651969909668 32.89850616455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921335697174072 3.8637070655822754 40.429203033447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932655811309814 3.7910244464874268 39.70351028442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926967144012451 3.2940115928649902 34.73281478881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937958240509033 3.2274115085601807 34.06791305541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932261228561401 3.656428098678589 38.357505798339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908769845962524 3.3237695693969727 35.02857208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934566736221313 3.4510018825531006 36.30347442626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931867837905884 3.450699806213379 36.3001823425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932096719741821 3.3953096866607666 35.746307373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926721572875977 3.4116714000701904 35.909385681152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79159414768219 3.2967896461486816 34.759490966796875
  batch 20 loss: 1.79159414768219, 3.2967896461486816, 34.759490966796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931044101715088 2.4999725818634033 26.792829513549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930344343185425 3.488205671310425 36.67509078979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928489446640015 2.9560062885284424 31.3529109954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932602167129517 3.2064385414123535 33.857643127441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928169965744019 3.4762747287750244 36.555564880371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929887771606445 3.5259599685668945 37.05258560180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793715000152588 3.620609760284424 37.999813079833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923152446746826 3.226322889328003 34.055545806884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944365739822388 3.188594341278076 33.680381774902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913848161697388 3.1031699180603027 32.82308578491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929705381393433 3.5295767784118652 37.08873748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913798093795776 3.5492873191833496 37.28425216674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930880784988403 3.1980879306793213 33.77396774291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793260097503662 3.4643187522888184 36.43644714355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930840253829956 3.3969273567199707 35.76235580444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933349609375 3.4635841846466064 36.429176330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793074369430542 3.127236843109131 33.06544494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918436527252197 3.1486403942108154 33.27824783325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924578189849854 3.3167779445648193 34.960235595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919718027114868 3.325104236602783 35.04301452636719
  batch 40 loss: 1.7919718027114868, 3.325104236602783, 35.04301452636719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480707168579 3.115428924560547 32.94676971435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792764663696289 3.1569762229919434 33.362525939941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924704551696777 3.018357515335083 31.976045608520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916277647018433 3.370339870452881 35.495025634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792974591255188 3.0388479232788086 32.181453704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926576137542725 2.9510719776153564 31.303377151489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924022674560547 3.509535789489746 36.88776397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921466827392578 3.310690402984619 34.8990478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917243242263794 3.2108168601989746 33.89989471435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923849821090698 3.3325624465942383 35.11800765991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909455299377441 3.0639712810516357 32.430660247802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929505109786987 2.8321585655212402 30.11453628540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927433252334595 3.0577800273895264 32.37054443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793217658996582 2.7896063327789307 29.689281463623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921912670135498 2.9625325202941895 31.41751480102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930277585983276 3.1083593368530273 32.87662124633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79334557056427 3.417982578277588 35.97317123413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933579683303833 4.0966081619262695 42.759437561035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939037084579468 3.3093626499176025 34.88753128051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 3.0614511966705322 32.40705108642578
  batch 60 loss: 1.7925397157669067, 3.0614511966705322, 32.40705108642578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793527364730835 3.224168300628662 34.03520965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917251586914062 3.0189249515533447 31.980974197387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793075442314148 2.9521610736846924 31.314685821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926442623138428 3.4542360305786133 36.33500289916992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7934097051620483 2.6909701824188232 28.70311164855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920907735824585 2.797363042831421 29.765722274780273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919827699661255 2.6660141944885254 28.452123641967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791656255722046 2.7001519203186035 28.793174743652344
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919316291809082 3.1875016689300537 33.66695022583008
Total LOSS train 34.6193841787485 valid 30.169492721557617
CE LOSS train 1.792696340267475 valid 0.44798290729522705
Contrastive LOSS train 3.282668792284452 valid 0.7968754172325134
EPOCH 111:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924386262893677 2.547424793243408 27.266685485839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791656494140625 3.3614320755004883 35.405975341796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930519580841064 4.001481056213379 41.807861328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792824149131775 3.554615020751953 37.33897399902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924288511276245 2.7556827068328857 29.34925651550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79344642162323 3.0392088890075684 32.1855354309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921624183654785 3.0748565196990967 32.54072952270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921446561813354 2.569392204284668 27.486066818237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917835712432861 2.546154022216797 27.253324508666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920994758605957 3.1840004920959473 33.632102966308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932804822921753 3.385125160217285 35.64453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724609375 3.2154269218444824 33.94699478149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937800884246826 2.655761480331421 28.351394653320312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930359840393066 3.494065046310425 36.73368835449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907782793045044 3.4136977195739746 35.927757263183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934389114379883 3.5918118953704834 37.71155548095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932485342025757 3.3511030673980713 35.30427932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793289065361023 3.0197978019714355 31.991268157958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927409410476685 3.685258388519287 38.64532470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916252613067627 2.806903123855591 29.86065673828125
  batch 20 loss: 1.7916252613067627, 2.806903123855591, 29.86065673828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931591272354126 2.6357946395874023 28.151105880737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929755449295044 3.6356313228607178 38.149288177490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928249835968018 3.093869924545288 32.73152542114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932093143463135 2.849207639694214 30.28528594970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927942276000977 3.76141619682312 39.40695571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928833961486816 3.4100501537323 35.89338684082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793674349784851 2.8494420051574707 30.28809356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922519445419312 2.526811122894287 27.06036376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944786548614502 2.9098284244537354 30.892763137817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914955615997314 3.4522500038146973 36.313995361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932398319244385 3.249005079269409 34.28329086303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791483759880066 2.823747396469116 30.02895736694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930707931518555 2.4688172340393066 26.481243133544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933027744293213 3.460009813308716 36.393402099609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931931018829346 3.0805227756500244 32.598419189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933950424194336 3.3104209899902344 34.897605895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931737899780273 2.9023561477661133 30.816734313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79184091091156 3.462205171585083 36.41389083862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924931049346924 3.3164238929748535 34.956729888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918500900268555 3.36148738861084 35.4067268371582
  batch 40 loss: 1.7918500900268555, 3.36148738861084, 35.4067268371582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924551963806152 2.576582193374634 27.558277130126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927623987197876 3.030696392059326 32.099727630615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792307734489441 2.980023145675659 31.592538833618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917590141296387 2.481999397277832 26.611753463745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930808067321777 2.6598451137542725 28.39153289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924983501434326 3.0740599632263184 32.53310012817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792388677597046 3.212573289871216 33.918121337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921921014785767 3.1996657848358154 33.788848876953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917349338531494 3.2414987087249756 34.206722259521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79245126247406 2.6928207874298096 28.720659255981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909761667251587 2.67936110496521 28.58458709716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928447723388672 2.4949584007263184 26.742429733276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925387620925903 3.4548399448394775 36.340938568115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930281162261963 2.53973126411438 27.190340042114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919822931289673 3.0161776542663574 31.953760147094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930928468704224 2.550286293029785 27.295955657958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933100461959839 3.0850822925567627 32.64413070678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933050394058228 3.2097561359405518 33.890865325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938251495361328 3.6387991905212402 38.18181610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924479246139526 3.73702335357666 39.16267776489258
  batch 60 loss: 1.7924479246139526, 3.73702335357666, 39.16267776489258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933344841003418 3.9131929874420166 40.92526626586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916816473007202 2.723487377166748 29.02655601501465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929847240447998 3.2610371112823486 34.40335464477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925349473953247 2.6020073890686035 27.81260871887207
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7933520078659058 3.03469181060791 32.1402702331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920973300933838 3.2031195163726807 33.82329559326172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918668985366821 2.965379238128662 31.445659637451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791656732559204 2.8364450931549072 30.15610694885254
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919633388519287 2.6295900344848633 28.08786392211914
Total LOSS train 32.70077793414776 valid 30.878231525421143
CE LOSS train 1.7926713613363412 valid 0.4479908347129822
Contrastive LOSS train 3.0908106583815353 valid 0.6573975086212158
EPOCH 112:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925388813018799 3.225313425064087 34.04567337036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915655374526978 3.1812350749969482 33.60391616821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929489612579346 2.8860621452331543 30.65357208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929607629776 3.1947367191314697 33.740325927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.6380746364593506 28.173179626464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935874462127686 2.370972156524658 25.503307342529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920719385147095 2.8906137943267822 30.698209762573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921748161315918 3.349372148513794 35.28589630126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791923999786377 3.0420773029327393 32.21269607543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919379472732544 3.102625608444214 32.81819534301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932672500610352 2.7798871994018555 29.592140197753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792849063873291 2.8435492515563965 30.228342056274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938047647476196 2.9160194396972656 30.953998565673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933167219161987 3.415219306945801 35.94550704956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908605337142944 3.7142539024353027 38.93339920043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793224573135376 3.4186482429504395 35.979705810546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930320501327515 2.8210830688476562 30.003862380981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929774522781372 2.708871364593506 28.88169288635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927663326263428 2.882988929748535 30.622655868530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916394472122192 4.130730628967285 43.09894561767578
  batch 20 loss: 1.7916394472122192, 4.130730628967285, 43.09894561767578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793075442314148 3.4415502548217773 36.208580017089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793078064918518 3.2881898880004883 34.67497634887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792722225189209 2.9670722484588623 31.46344566345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933173179626465 3.244199275970459 34.23530960083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928484678268433 3.6780569553375244 38.57341766357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930936813354492 3.65230655670166 38.316158294677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937043905258179 3.1747734546661377 33.541439056396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922074794769287 3.51349139213562 36.927120208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942283153533936 3.709198474884033 38.88621139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914700508117676 3.5712811946868896 37.50428009033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793060064315796 3.811173439025879 39.90479278564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913252115249634 2.973137378692627 31.52269744873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792858600616455 3.5656189918518066 37.44904708862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932240962982178 3.4759597778320312 36.55282211303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931269407272339 3.4692397117614746 36.48552322387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932957410812378 3.319232702255249 34.98562240600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930073738098145 3.4465670585632324 36.2586784362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916736602783203 3.1060264110565186 32.85193634033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923624515533447 3.072392225265503 32.51628494262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919292449951172 3.566559076309204 37.45751953125
  batch 40 loss: 1.7919292449951172, 3.566559076309204, 37.45751953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926349639892578 3.7628729343414307 39.421363830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792962908744812 2.893892765045166 30.731889724731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924896478652954 3.3624861240386963 35.41735076904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791809320449829 2.8771777153015137 30.563587188720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929668426513672 3.399237632751465 35.78534698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924888134002686 3.5261709690093994 37.05419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923802137374878 3.0095953941345215 31.888334274291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921779155731201 3.1846094131469727 33.63827133178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519045829773 3.512704372406006 36.91856384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792236566543579 3.574326753616333 37.53550338745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908437252044678 3.3444571495056152 35.235416412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928255796432495 3.0498080253601074 32.2909049987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926267385482788 3.361961603164673 35.4122428894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792991280555725 3.1368093490600586 33.16108322143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921007871627808 3.67303204536438 38.52241897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931257486343384 3.98006534576416 41.59377670288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932653427124023 3.430875539779663 36.102020263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793307900428772 3.2273576259613037 34.0668830871582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939037084579468 3.1731326580047607 33.525230407714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925862073898315 3.4426279067993164 36.21886444091797
  batch 60 loss: 1.7925862073898315, 3.4426279067993164, 36.21886444091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934703826904297 3.2888131141662598 34.681602478027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918716669082642 3.4215493202209473 36.007362365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930670976638794 3.1804728507995605 33.59779739379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925121784210205 3.013572931289673 31.928241729736328
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7933461666107178 2.7171051502227783 28.964397430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792123794555664 3.6181249618530273 37.97337341308594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920269966125488 3.1678216457366943 33.470245361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916902303695679 3.721327543258667 39.004966735839844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919331789016724 3.057274580001831 32.364681243896484
Total LOSS train 34.42390365600586 valid 35.7033166885376
CE LOSS train 1.7926615403248713 valid 0.4479832947254181
Contrastive LOSS train 3.263124238527738 valid 0.7643186450004578
EPOCH 113:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925071716308594 3.426145315170288 36.053958892822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916895151138306 4.080441951751709 42.59611129760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929352521896362 2.9880502223968506 31.673437118530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928125858306885 3.1698825359344482 33.49163818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923675775527954 3.332629680633545 35.1186637878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937036752700806 3.3757479190826416 35.551185607910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921959161758423 3.742170572280884 39.21390151977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922933101654053 3.8451385498046875 40.24367904663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791837215423584 3.470968723297119 36.501522064208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919719219207764 3.4929964542388916 36.72193908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793221116065979 2.9527132511138916 31.32035255432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927896976470947 3.033513307571411 32.12792205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937908172607422 2.688706636428833 28.680856704711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933014631271362 3.6893019676208496 38.68632125854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790810227394104 3.5918867588043213 37.709678649902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932491302490234 3.4402334690093994 36.195587158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931157350540161 3.0879323482513428 32.67243957519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930980920791626 4.2200188636779785 43.9932861328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927064895629883 2.9007670879364014 30.800376892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791637897491455 3.033503532409668 32.12667465209961
  batch 20 loss: 1.791637897491455, 3.033503532409668, 32.12667465209961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931982278823853 3.0118818283081055 31.912015914916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931376695632935 3.5921740531921387 37.71487808227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927768230438232 2.600209951400757 27.794876098632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932628393173218 3.040780544281006 32.20106887817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927289009094238 3.4172279834747314 35.96500778198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929365634918213 3.5484399795532227 37.27733612060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793727159500122 3.7196743488311768 38.99047088623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922890186309814 3.0523178577423096 32.315467834472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943605184555054 2.868919849395752 30.483558654785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913475036621094 3.0525143146514893 32.316490173339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930395603179932 2.9839327335357666 31.632366180419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791385293006897 3.1467976570129395 33.259361267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793062448501587 2.978498935699463 31.57805061340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933729887008667 3.1371636390686035 33.165008544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932957410812378 2.637596845626831 28.16926383972168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933471202850342 3.130646228790283 33.09980773925781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793121576309204 3.011276960372925 31.90589141845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917540073394775 2.978930711746216 31.5810604095459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792467713356018 3.7950966358184814 39.74343490600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918802499771118 2.9209213256835938 31.0010929107666
  batch 40 loss: 1.7918802499771118, 2.9209213256835938, 31.0010929107666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792648434638977 2.635650396347046 28.149152755737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79292631149292 3.1606109142303467 33.3990364074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924002408981323 3.069544792175293 32.48784637451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918431758880615 3.117204189300537 32.96388626098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793131947517395 3.6030447483062744 37.82358169555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 3.1073503494262695 32.86600875854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924727201461792 3.480634927749634 36.59882354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922195196151733 3.7020010948181152 38.812232971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 3.732814073562622 39.11964797973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923078536987305 3.4818034172058105 36.61034393310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909072637557983 4.073511123657227 42.52602005004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927980422973633 3.1208295822143555 33.001094818115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925293445587158 2.3942489624023438 25.73501968383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929699420928955 3.711684226989746 38.909812927246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792030930519104 3.792991876602173 39.72195053100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930179834365845 3.6285698413848877 38.07871627807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933293581008911 3.431170701980591 36.105037689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933249473571777 3.7921745777130127 39.71506881713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793873906135559 3.4956700801849365 36.750572204589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925119400024414 3.3757779598236084 35.550289154052734
  batch 60 loss: 1.7925119400024414, 3.3757779598236084, 35.550289154052734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793416142463684 3.6397171020507812 38.19058609008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791733980178833 3.3107147216796875 34.89888000488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929648160934448 3.4935364723205566 36.728328704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792384147644043 3.310387372970581 34.89625930786133
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793227195739746 2.7548506259918213 29.341732025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920383214950562 3.65718412399292 38.3638801574707
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919007539749146 2.8633949756622314 30.42584991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916100025177002 3.8762052059173584 40.55366134643555
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791843295097351 3.173490285873413 33.5267448425293
Total LOSS train 34.80870722257174 valid 35.71753406524658
CE LOSS train 1.792669415473938 valid 0.44796082377433777
Contrastive LOSS train 3.301603779425988 valid 0.7933725714683533
EPOCH 114:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923544645309448 3.501948833465576 36.81184387207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916460037231445 3.339254856109619 35.1841926574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929236888885498 3.6618902683258057 38.411827087402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927314043045044 3.840453624725342 40.197269439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792319655418396 3.465069532394409 36.443016052246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934824228286743 3.9705936908721924 41.499420166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920641899108887 3.798396587371826 39.776031494140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921867370605469 3.0141570568084717 31.933757781982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918554544448853 3.51788330078125 36.97068786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919304370880127 2.7637736797332764 29.429668426513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932790517807007 3.4005353450775146 35.79862976074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927721738815308 3.6665425300598145 38.458194732666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938683032989502 3.1501078605651855 33.29494857788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933294773101807 3.8411831855773926 40.205162048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909458875656128 3.1476244926452637 33.26719284057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793425440788269 3.369884967803955 35.49227523803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793045997619629 2.9832658767700195 31.62570571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930516004562378 3.4709041118621826 36.50209426879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926162481307983 3.4380390644073486 36.17300796508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915749549865723 2.123840093612671 23.02997589111328
  batch 20 loss: 1.7915749549865723, 2.123840093612671, 23.02997589111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929774522781372 2.2640602588653564 24.43358039855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929959297180176 2.960874080657959 31.401737213134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927273511886597 2.5229873657226562 27.022600173950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79328191280365 2.8028085231781006 29.821367263793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927745580673218 3.1918606758117676 33.71138000488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928930521011353 2.752092123031616 29.313814163208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935974597930908 3.311617851257324 34.9097785949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922084331512451 3.377528429031372 35.5674934387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943542003631592 2.8335235118865967 30.129589080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791451334953308 3.523439645767212 37.025848388671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793023943901062 3.474832773208618 36.541351318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913215160369873 4.1341328620910645 43.13264846801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929339408874512 3.8477976322174072 40.270912170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930620908737183 3.842435121536255 40.217411041259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793123483657837 3.363363742828369 35.426761627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932147979736328 3.295968532562256 34.752899169921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931623458862305 3.2760324478149414 34.55348587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918174266815186 3.0812623500823975 32.60444259643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925108671188354 3.7614779472351074 39.407291412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920328378677368 3.4772889614105225 36.56492233276367
  batch 40 loss: 1.7920328378677368, 3.4772889614105225, 36.56492233276367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924686670303345 3.3441364765167236 35.23383331298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928521633148193 3.0017902851104736 31.810754776000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924574613571167 3.480471611022949 36.59717559814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916775941848755 4.102365970611572 42.815338134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929725646972656 3.3741331100463867 35.534305572509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792562484741211 3.034172296524048 32.13428497314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923610210418701 3.3515119552612305 35.3074836730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921741008758545 3.0782039165496826 32.574214935302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915630340576172 2.7420923709869385 29.212486267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923238277435303 3.600886583328247 37.80118942260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908929586410522 3.1848807334899902 33.63970184326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792834758758545 3.195941925048828 33.752254486083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925689220428467 2.8693909645080566 30.48647689819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930371761322021 3.2286136150360107 34.07917404174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921010255813599 3.6475870609283447 38.267974853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792994499206543 2.953325033187866 31.326244354248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934503555297852 3.25186824798584 34.3121337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934231758117676 3.7308290004730225 39.10171127319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793871283531189 3.659412384033203 38.387996673583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924691438674927 3.558659791946411 37.379066467285156
  batch 60 loss: 1.7924691438674927, 3.558659791946411, 37.379066467285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933964729309082 3.378361701965332 35.57701110839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791635513305664 3.8954648971557617 40.74628448486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930115461349487 3.4153401851654053 35.9464111328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925902605056763 2.900465250015259 30.797243118286133
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7933164834976196 2.8915770053863525 30.70908546447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920106649398804 2.806910514831543 29.861116409301758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 2.9758360385894775 31.550262451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916063070297241 3.184220314025879 33.63380813598633
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791855812072754 2.813070058822632 29.922557830810547
Total LOSS train 34.936062357975885 valid 31.241936206817627
CE LOSS train 1.7926442311360287 valid 0.4479639530181885
Contrastive LOSS train 3.3143417871915375 valid 0.703267514705658
EPOCH 115:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923756837844849 3.137946367263794 33.17184066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915029525756836 3.3690788745880127 35.48229217529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929776906967163 2.5691568851470947 27.484546661376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927263975143433 2.8779308795928955 30.57203483581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924222946166992 3.4872286319732666 36.664710998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933337688446045 3.6377782821655273 38.171119689941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920728921890259 3.443697690963745 36.22904968261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792094349861145 2.7418107986450195 29.210203170776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918601036071777 2.9588723182678223 31.380582809448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920920848846436 3.2954607009887695 34.74669647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932865619659424 2.758525848388672 29.3785457611084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928178310394287 2.874749183654785 30.54030990600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937427759170532 2.5273749828338623 27.067493438720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931710481643677 3.2495014667510986 34.288185119628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7907848358154297 3.608222246170044 37.873008728027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793213129043579 3.0767273902893066 32.56048583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793017864227295 3.5725417137145996 37.518436431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930529117584229 3.595531702041626 37.74837112426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926158905029297 3.1993563175201416 33.78617858886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916420698165894 3.564572334289551 37.43736267089844
  batch 20 loss: 1.7916420698165894, 3.564572334289551, 37.43736267089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930759191513062 2.8054301738739014 29.84737777709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929039001464844 3.3008639812469482 34.801544189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792729377746582 2.4564199447631836 26.356929779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932727336883545 3.5159151554107666 36.95242691040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928450107574463 3.3384158611297607 35.177005767822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929208278656006 3.0606648921966553 32.39957046508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793632984161377 3.3344027996063232 35.13766098022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79214608669281 3.367856025695801 35.470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942317724227905 2.670117139816284 28.495403289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791310429573059 2.3612124919891357 25.4034366607666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930161952972412 3.2841615676879883 34.63462829589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913898229599 3.669095039367676 38.482337951660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929450273513794 3.3305602073669434 35.098548889160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931455373764038 3.349703788757324 35.290184020996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930107116699219 2.758849859237671 29.38150978088379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932790517807007 2.968614101409912 31.479421615600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931336164474487 3.307192802429199 34.86506271362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918275594711304 2.952446937561035 31.31629753112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924174070358276 3.883070945739746 40.62312698364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918689250946045 3.2904984951019287 34.69685363769531
  batch 40 loss: 1.7918689250946045, 3.2904984951019287, 34.69685363769531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924551963806152 3.1608400344848633 33.400856018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927418947219849 2.573883533477783 27.53157615661621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923917770385742 3.1421265602111816 33.21365737915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79168701171875 3.440845489501953 36.20014190673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793030023574829 3.1248457431793213 33.04148864746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924898862838745 2.9733800888061523 31.526290893554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923446893692017 3.2088167667388916 33.88051223754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921102046966553 3.7477571964263916 39.269683837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915018796920776 2.87429141998291 30.53441619873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792406678199768 3.313464641571045 34.92705535888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909737825393677 3.2293622493743896 34.0845947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929251194000244 3.6081275939941406 37.874202728271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926325798034668 3.090637683868408 32.69900894165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930957078933716 3.9232656955718994 41.025753021240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 3.351656436920166 35.30861282348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931125164031982 3.1666669845581055 33.459781646728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933683395385742 3.482248067855835 36.615848541259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933419942855835 3.024663209915161 32.039974212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938841581344604 3.829434394836426 40.088226318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792493224143982 3.0475659370422363 32.268150329589844
  batch 60 loss: 1.792493224143982, 3.0475659370422363, 32.268150329589844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933783531188965 3.2450621128082275 34.24399948120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 3.055948495864868 32.35122299194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930618524551392 3.6370763778686523 38.16382598876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792511224746704 2.590909242630005 27.701602935791016
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7932369709014893 2.714451789855957 28.937755584716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919946908950806 3.542381763458252 37.21581268310547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918879985809326 3.679036855697632 38.58225631713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915781736373901 3.192183256149292 33.713409423828125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918740510940552 2.703908681869507 28.830961227416992
Total LOSS train 33.655534216073846 valid 34.585609912872314
CE LOSS train 1.7926287486002996 valid 0.4479685127735138
Contrastive LOSS train 3.186290531892043 valid 0.6759771704673767
EPOCH 116:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923814058303833 3.742450475692749 39.21688461303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915611267089844 3.449244499206543 36.28400802612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928344011306763 2.5556015968322754 27.34885025024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927818298339844 3.573201894760132 37.52479934692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923604249954224 2.6869895458221436 28.662256240844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934755086898804 3.0221292972564697 32.014766693115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919734716415405 2.7179110050201416 28.97108268737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919272184371948 2.691545009613037 28.707378387451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918039560317993 2.221313714981079 24.004940032958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919448614120483 2.7974419593811035 29.7663631439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932795286178589 3.0131208896636963 31.924488067626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792815923690796 2.278903007507324 24.581846237182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936227321624756 2.619990110397339 27.9935245513916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933604717254639 2.446936845779419 26.262727737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909762859344482 2.577505588531494 27.566030502319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793248176574707 2.6624767780303955 28.418014526367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931339740753174 3.0468132495880127 32.26126480102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793042778968811 2.9341959953308105 31.1350040435791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927900552749634 2.832827568054199 30.121065139770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916556596755981 2.718975782394409 28.981414794921875
  batch 20 loss: 1.7916556596755981, 2.718975782394409, 28.981414794921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928848266601562 2.6547274589538574 28.340160369873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792830228805542 3.0618159770965576 32.41099166870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541265487671 2.7341248989105225 29.133790969848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932096719741821 2.538409948348999 27.177309036254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927355766296387 3.062131643295288 32.4140510559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930469512939453 2.4265360832214355 26.058408737182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937051057815552 2.8980796337127686 30.77450180053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792251706123352 3.127763271331787 33.06988525390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944008111953735 2.6087534427642822 27.881935119628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914825677871704 2.9908621311187744 31.700103759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929747104644775 3.3225860595703125 35.018836975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791324257850647 3.2406275272369385 34.197601318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929261922836304 2.997533082962036 31.76825714111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931772470474243 2.8683278560638428 30.476455688476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930471897125244 2.954617977142334 31.3392276763916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931573390960693 3.192337989807129 33.71653747558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793038249015808 3.538335084915161 37.176387786865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917802333831787 3.042304515838623 32.21482467651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924401760101318 3.5614168643951416 37.40660858154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918651103973389 3.6327521800994873 38.119388580322266
  batch 40 loss: 1.7918651103973389, 3.6327521800994873, 38.119388580322266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926204204559326 3.3038337230682373 34.83095932006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929127216339111 3.1252686977386475 33.04560089111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924363613128662 3.408282995223999 35.87526321411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917637825012207 3.6630027294158936 38.421791076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929856777191162 2.818577766418457 29.978763580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 3.0170187950134277 31.962608337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923210859298706 3.403731107711792 35.82963180541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792108416557312 3.4699761867523193 36.49186706542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791464924812317 3.1575515270233154 33.366981506347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923219203948975 3.6383655071258545 38.17597579956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790905237197876 3.144331216812134 33.23421859741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79282808303833 3.364370822906494 35.4365348815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926000356674194 3.182802677154541 33.62062454223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793016791343689 3.545297384262085 37.24599075317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920958995819092 3.7525112628936768 39.31720733642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930744886398315 3.3636715412139893 35.42979049682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932370901107788 3.0780293941497803 32.57353210449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932305335998535 3.5213623046875 37.00685501098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938166856765747 3.708660840988159 38.88042449951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924877405166626 3.4280812740325928 36.073299407958984
  batch 60 loss: 1.7924877405166626, 3.4280812740325928, 36.073299407958984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934134006500244 3.477932929992676 36.5727424621582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79181706905365 4.075323581695557 42.54505157470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930670976638794 3.2780117988586426 34.573184967041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925664186477661 3.3056321144104004 34.8488883972168
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7933154106140137 2.6004340648651123 27.797657012939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921169996261597 3.0555481910705566 32.347599029541016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919760942459106 2.981156349182129 31.603540420532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916622161865234 3.0869028568267822 32.66069030761719
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919420003890991 2.7789149284362793 29.581092834472656
Total LOSS train 32.69657566363995 valid 31.54823064804077
CE LOSS train 1.7926248660454382 valid 0.4479855000972748
Contrastive LOSS train 3.090395087462205 valid 0.6947287321090698
EPOCH 117:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924869060516357 2.8291807174682617 30.084293365478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917124032974243 3.2244889736175537 34.03660202026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929600477218628 3.119954824447632 32.99250793457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929445505142212 3.7394134998321533 39.18708038330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924690246582031 2.851837158203125 30.310840606689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793661117553711 3.350482702255249 35.298484802246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920734882354736 3.1256158351898193 33.04823303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922357320785522 3.5094070434570312 36.88630676269531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918431758880615 3.951948642730713 41.31132888793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920429706573486 3.4984211921691895 36.77625274658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793282151222229 3.7914621829986572 39.70790481567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792729139328003 3.215759038925171 33.9503173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936339378356934 3.2333080768585205 34.126712799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931336164474487 2.9681081771850586 31.474214553833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908928394317627 3.0363452434539795 32.15434646606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932798862457275 3.6184768676757812 37.978050231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931288480758667 3.848236560821533 40.27549362182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930691242218018 3.5888943672180176 37.682010650634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927831411361694 3.5844836235046387 37.63761901855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791627287864685 3.2535367012023926 34.32699203491211
  batch 20 loss: 1.791627287864685, 3.2535367012023926, 34.32699203491211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930090427398682 3.506221055984497 36.855220794677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928664684295654 3.8470263481140137 40.26313018798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926945686340332 3.407963514328003 35.87232971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932770252227783 3.276412010192871 34.55739974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928467988967896 3.153913974761963 33.33198547363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930432558059692 3.6546523571014404 38.339569091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793666958808899 3.9780945777893066 41.57461166381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922300100326538 3.636894464492798 38.16117477416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943613529205322 3.6669633388519287 38.463993072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791589617729187 3.1595284938812256 33.38687515258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931873798370361 2.893725872039795 30.73044776916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791419506072998 3.1620140075683594 33.41156005859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929905652999878 3.1073122024536133 32.866111755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931747436523438 3.2556381225585938 34.34955596923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930651903152466 3.1486191749572754 33.27925491333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933534383773804 3.710794687271118 38.901302337646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930220365524292 3.6556408405303955 38.349430084228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791691541671753 3.3485233783721924 35.27692413330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923612594604492 2.698695421218872 28.779315948486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916996479034424 3.112865686416626 32.92035675048828
  batch 40 loss: 1.7916996479034424, 3.112865686416626, 32.92035675048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924857139587402 2.7316482067108154 29.10896873474121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792838454246521 2.109896421432495 22.891803741455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792318344116211 3.7209084033966064 39.00140380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918267250061035 2.929744243621826 31.089269638061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929742336273193 3.9845211505889893 41.63818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792524814605713 3.745596408843994 39.24848937988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924085855484009 4.389908313751221 45.691490173339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792299509048462 3.430781126022339 36.10011291503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915560007095337 3.2693898677825928 34.48545455932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924902439117432 3.833096981048584 40.12346267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909762859344482 3.117220640182495 32.96318435668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929065227508545 3.337416410446167 35.16707229614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925924062728882 3.3885371685028076 35.67796325683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929822206497192 3.4977684020996094 36.770668029785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920681238174438 3.1877171993255615 33.66923904418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930693626403809 3.3378241062164307 35.17131042480469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932541370391846 3.8075931072235107 39.86918640136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932484149932861 3.7057461738586426 38.8507080078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793736219406128 3.6200027465820312 37.9937629699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924188375473022 3.2721340656280518 34.51375961303711
  batch 60 loss: 1.7924188375473022, 3.2721340656280518, 34.51375961303711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933868169784546 3.620579957962036 37.99918746948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918479442596436 4.080183029174805 42.59367752075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930271625518799 3.641286611557007 38.20589065551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439579963684 3.534119129180908 37.133628845214844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931820154190063 2.8417720794677734 30.21090316772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79202139377594 4.187418460845947 43.66620635986328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791938304901123 3.5393340587615967 37.185279846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916340827941895 3.3915915489196777 35.707550048828125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919224500656128 3.32405161857605 35.032440185546875
Total LOSS train 35.77053727370042 valid 37.89786911010742
CE LOSS train 1.792652284182035 valid 0.4479806125164032
Contrastive LOSS train 3.397788506287795 valid 0.8310129046440125
EPOCH 118:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792399287223816 3.916008949279785 40.95248794555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791585922241211 3.9200403690338135 40.99198913574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928645610809326 3.8521628379821777 40.31449508666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927453517913818 3.009310245513916 31.885847091674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923204898834229 3.0380263328552246 32.172584533691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935370206832886 3.440030336380005 36.19384002685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921247482299805 3.209520101547241 33.887325286865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920591831207275 3.0794501304626465 32.58656311035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919145822525024 2.966770648956299 31.459619522094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920516729354858 2.54872989654541 27.27935028076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793277382850647 2.7443692684173584 29.236970901489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928770780563354 3.5536231994628906 37.32910919189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937811613082886 3.9210636615753174 41.004417419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932415008544922 3.8528900146484375 40.3221435546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908596992492676 3.046053171157837 32.25139236450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932451963424683 3.1232919692993164 33.02616500854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930582761764526 3.168524980545044 33.47830581665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929078340530396 3.879025936126709 40.583168029785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926331758499146 3.9597482681274414 41.390113830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915536165237427 3.2566521167755127 34.35807418823242
  batch 20 loss: 1.7915536165237427, 3.2566521167755127, 34.35807418823242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929033041000366 3.027087688446045 32.06378173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792891025543213 2.6306729316711426 28.099618911743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927396297454834 2.7997336387634277 29.790077209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933425903320312 3.0607099533081055 32.40044403076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929359674453735 3.20046067237854 33.797542572021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930344343185425 2.7931647300720215 29.724681854248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936242818832397 3.246143341064453 34.25505828857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792248010635376 4.330578804016113 45.0980339050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7944374084472656 3.017514944076538 31.969587326049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 3.48368501663208 36.628440856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930430173873901 3.2732553482055664 34.52559280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913637161254883 2.928178548812866 31.073150634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929625511169434 3.1471357345581055 33.264320373535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931002378463745 2.9427802562713623 31.220903396606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930359840393066 3.3892569541931152 35.68560791015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931510210037231 3.2090871334075928 33.8840217590332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793061375617981 3.454357147216797 36.336631774902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791819453239441 3.202930450439453 33.82112503051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925258874893188 3.481045961380005 36.60298538208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918157577514648 2.9833717346191406 31.625534057617188
  batch 40 loss: 1.7918157577514648, 2.9833717346191406, 31.625534057617188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925794124603271 3.4105324745178223 35.89790344238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792921781539917 3.3301236629486084 35.09415817260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924095392227173 3.0184519290924072 31.9769287109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791771411895752 2.8095409870147705 29.88718032836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793001413345337 3.482407808303833 36.61708068847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 3.266075372695923 34.453285217285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923624515533447 4.105379581451416 42.84615707397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79214608669281 3.4473118782043457 36.265262603759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915377616882324 3.767045259475708 39.46199035644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923364639282227 4.097439289093018 42.766727447509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909057140350342 3.8296828269958496 40.08773422241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792798399925232 3.3730897903442383 35.5236930847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419195175171 3.8360838890075684 40.15325927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928969860076904 3.8153979778289795 39.946876525878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792007327079773 3.1706693172454834 33.49869918823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928276062011719 3.0256714820861816 32.04954147338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933709621429443 2.905182123184204 30.845191955566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932425737380981 3.5767507553100586 37.560752868652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937605381011963 3.7060635089874268 38.85439682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792460322380066 3.0141031742095947 31.933490753173828
  batch 60 loss: 1.792460322380066, 3.0141031742095947, 31.933490753173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933800220489502 3.7476658821105957 39.27003860473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918018102645874 4.094452381134033 42.736324310302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930094003677368 3.3407723903656006 35.20073318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462944984436 3.4786794185638428 36.579254150390625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930816411972046 2.855684280395508 30.349924087524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919485569000244 3.1845858097076416 33.6378059387207
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 3.9356541633605957 41.148441314697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791550636291504 3.5368876457214355 37.16042709350586
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791805624961853 3.2929975986480713 34.72178268432617
Total LOSS train 35.26811825678899 valid 36.6671142578125
CE LOSS train 1.7926259701068585 valid 0.44795140624046326
Contrastive LOSS train 3.347549244073721 valid 0.8232493996620178
EPOCH 119:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391061782837 3.5830674171447754 37.62306594848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916162014007568 3.5200400352478027 36.99201583862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929326295852661 3.08125638961792 32.605499267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79276704788208 3.276390790939331 34.55667495727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792340636253357 3.6084203720092773 37.87654495239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934684753417969 3.1539900302886963 33.33336639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920314073562622 3.519150733947754 36.98353576660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921594381332397 3.457498073577881 36.36714172363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917969226837158 3.2704193592071533 34.49599075317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920312881469727 3.1965625286102295 33.75765609741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932376861572266 2.4480581283569336 26.273818969726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927525043487549 3.276808977127075 34.56084060668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937288284301758 3.3250794410705566 35.04452133178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931028604507446 3.3519489765167236 35.312591552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790870189666748 2.9432623386383057 31.223493576049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793309211730957 3.237245798110962 34.165767669677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79317307472229 3.138523817062378 33.17841339111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931063175201416 3.4630179405212402 36.42328643798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926368713378906 2.6559224128723145 28.35186004638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915806770324707 2.5205078125 26.996658325195312
  batch 20 loss: 1.7915806770324707, 2.5205078125, 26.996658325195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793013095855713 3.3550333976745605 35.34334945678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929677963256836 3.9248111248016357 41.041080474853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926145792007446 2.6032044887542725 27.82465934753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931208610534668 2.6344189643859863 28.137310028076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792660117149353 3.177795171737671 33.570613861083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927677631378174 2.9603052139282227 31.39582061767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935729026794434 3.589501142501831 37.68858337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921911478042603 3.042297601699829 32.21516799926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942827939987183 3.308157444000244 34.8758544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914478778839111 3.212644100189209 33.91788864135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930184602737427 3.183239459991455 33.62541198730469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913228273391724 2.7358365058898926 29.149686813354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929619550704956 2.8657655715942383 30.45061683654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931876182556152 3.384906053543091 35.64224624633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930495738983154 3.6238884925842285 38.03193283081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931524515151978 2.712507963180542 28.918231964111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929210662841797 3.1701884269714355 33.49480438232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916498184204102 2.905060052871704 30.84225082397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924286127090454 3.415787696838379 35.95030212402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917872667312622 2.5886988639831543 27.67877769470215
  batch 40 loss: 1.7917872667312622, 2.5886988639831543, 27.67877769470215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792448878288269 2.150184392929077 23.294292449951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927331924438477 3.011601448059082 31.908748626708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792178750038147 3.117478847503662 32.96696853637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916948795318604 2.8963520526885986 30.75521469116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929449081420898 3.578537940979004 37.57832336425781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923558950424194 2.7639410495758057 29.431766510009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922817468643188 3.2878730297088623 34.67101287841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920676469802856 3.1378374099731445 33.170440673828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915740013122559 2.9679741859436035 31.471315383911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218565940857 2.8552019596099854 30.34423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908446788787842 3.117968797683716 32.97053146362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927570343017578 2.9950482845306396 31.743240356445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792548656463623 2.8835418224334717 30.627967834472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930043935775757 3.425889015197754 36.0518913269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918856143951416 2.980769634246826 31.59958267211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793043851852417 2.4066591262817383 25.859634399414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79324471950531 2.3768372535705566 25.561616897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933077812194824 3.103889226913452 32.83219909667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938761711120605 2.567941904067993 27.473295211791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924365997314453 1.8905861377716064 20.69829750061035
  batch 60 loss: 1.7924365997314453, 1.8905861377716064, 20.69829750061035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933216094970703 1.907217025756836 20.86549186706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918123006820679 3.0133984088897705 31.925796508789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793043613433838 1.9988101720809937 21.781145095825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79245924949646 2.304887533187866 24.84133529663086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931675910949707 1.5885285139083862 17.67845344543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920193672180176 2.744414806365967 29.236167907714844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918614149093628 2.64699387550354 28.261798858642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915761470794678 2.424257755279541 26.03415298461914
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792032241821289 2.453463077545166 26.326662063598633
Total LOSS train 31.754155907264124 valid 27.4646954536438
CE LOSS train 1.7925908345442552 valid 0.44800806045532227
Contrastive LOSS train 2.9961565274458666 valid 0.6133657693862915
EPOCH 120:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792555570602417 2.2686052322387695 24.478607177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916353940963745 2.420524835586548 25.996883392333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929073572158813 2.4446258544921875 26.239166259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927716970443726 2.4194979667663574 25.98775291442871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922899723052979 2.727083444595337 29.06312370300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934755086898804 3.3158082962036133 34.95155715942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920191287994385 3.051280975341797 32.30482864379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792160153388977 2.89350962638855 30.727256774902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918567657470703 3.600597620010376 37.79783630371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919920682907104 3.15047550201416 33.29674530029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932844161987305 3.049440622329712 32.287689208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927671670913696 3.141972064971924 33.212486267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936307191848755 3.705392360687256 38.847557067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931822538375854 2.629899740219116 28.092180252075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909237146377563 3.1793406009674072 33.584327697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933028936386108 3.1801884174346924 33.59518814086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930060625076294 2.977860450744629 31.571611404418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929050922393799 3.60082745552063 37.801177978515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792634129524231 3.0351409912109375 32.14404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791582703590393 3.3800621032714844 35.59220504760742
  batch 20 loss: 1.791582703590393, 3.3800621032714844, 35.59220504760742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929996252059937 2.9048686027526855 30.841686248779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792911171913147 3.1554627418518066 33.347537994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792572021484375 2.863318681716919 30.425758361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931458950042725 2.8716044425964355 30.509191513061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926932573318481 3.7483088970184326 39.27578353881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928208112716675 3.3574845790863037 35.3676643371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934987545013428 3.1461825370788574 33.25532531738281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792161226272583 2.7278451919555664 29.070613861083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942386865615845 3.2961478233337402 34.75571823120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79150390625 3.5673012733459473 37.464515686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929790019989014 2.685457944869995 28.647558212280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913062572479248 2.8741095066070557 30.532400131225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928695678710938 2.7905266284942627 29.698135375976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930978536605835 2.5809848308563232 27.602947235107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792988896369934 3.3609354496002197 35.40234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931296825408936 3.5785939693450928 37.57906723022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929445505142212 3.188403606414795 33.67698287963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791690707206726 2.6162526607513428 27.9542179107666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792362093925476 3.4003591537475586 35.795955657958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916697263717651 2.5428354740142822 27.22002410888672
  batch 40 loss: 1.7916697263717651, 2.5428354740142822, 27.22002410888672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924425601959229 2.226172924041748 24.05417251586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927542924880981 3.449754238128662 36.29029846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792335033416748 3.360968589782715 35.40202331542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917873859405518 3.008235454559326 31.874143600463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929679155349731 2.4987499713897705 26.780467987060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924747467041016 2.9668586254119873 31.461061477661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792362928390503 3.318962812423706 34.981990814208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921693325042725 3.3127903938293457 34.92007064819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916330099105835 3.041419744491577 32.20582962036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792259693145752 2.614948272705078 27.941741943359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790831446647644 2.240032434463501 24.19115447998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927393913269043 2.1858932971954346 23.65167236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925548553466797 2.7819812297821045 29.612367630004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792993187904358 2.7275264263153076 29.06825828552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919650077819824 2.163017749786377 23.422142028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930142879486084 2.8707692623138428 30.500707626342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793269395828247 2.6861493587493896 28.65476417541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932525873184204 2.824219226837158 30.035444259643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938122749328613 2.9222030639648438 31.01584243774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792406678199768 2.4982523918151855 26.774930953979492
  batch 60 loss: 1.792406678199768, 2.4982523918151855, 26.774930953979492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932902574539185 2.8191912174224854 29.98520278930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918099164962769 2.3540375232696533 25.332183837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929530143737793 2.4045634269714355 25.83858871459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923812866210938 2.342060089111328 25.212982177734375
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931404113769531 2.1870195865631104 23.6633358001709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919328212738037 3.172922372817993 33.521156311035156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918003797531128 2.5890069007873535 27.681867599487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915542125701904 2.6916909217834473 28.70846176147461
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919503450393677 2.196051597595215 23.752466201782227
Total LOSS train 30.905646602924055 valid 28.415987968444824
CE LOSS train 1.7925856516911434 valid 0.4479875862598419
Contrastive LOSS train 2.9113060841193565 valid 0.5490128993988037
EPOCH 121:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924848794937134 3.1636452674865723 33.42893600463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915151119232178 3.7836239337921143 39.62775421142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927613258361816 2.8450851440429688 30.24361228942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926280498504639 2.6615097522735596 28.407724380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921414375305176 2.9803242683410645 31.595382690429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933660745620728 3.262179136276245 34.415157318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919317483901978 3.1029036045074463 32.82096862792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920836210250854 3.1695640087127686 33.48772430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918087244033813 2.419735908508301 25.989168167114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774868965149 2.3885014057159424 25.676788330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793176293373108 3.1591057777404785 33.38423156738281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927848100662231 2.5032641887664795 26.825428009033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935693264007568 3.17899227142334 33.583492279052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931172847747803 2.952293634414673 31.31605339050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908720970153809 2.497559070587158 26.766462326049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932096719741821 2.571995735168457 27.513166427612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930190563201904 2.409205436706543 25.885072708129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928268909454346 2.695582389831543 28.7486515045166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927308082580566 3.5956029891967773 37.74876403808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916755676269531 3.2891790866851807 34.683467864990234
  batch 20 loss: 1.7916755676269531, 3.2891790866851807, 34.683467864990234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928838729858398 3.403418779373169 35.82707214355469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928128242492676 3.559136152267456 37.38417434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927325963974 3.0378763675689697 32.1714973449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932416200637817 3.3181798458099365 34.975040435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927168607711792 3.716284990310669 38.95556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928707599639893 3.758021831512451 39.37308883666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934404611587524 4.108394145965576 42.877384185791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921048402786255 3.585495948791504 37.647064208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794061541557312 3.714216709136963 38.93622589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915054559707642 3.6075804233551025 37.8673095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793018102645874 3.4278078079223633 36.07109451293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913864850997925 2.8557653427124023 30.34903907775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928944826126099 3.7031238079071045 38.824134826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931084632873535 3.3435142040252686 35.22825241088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930340766906738 3.134384870529175 33.13688278198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930161952972412 3.488304376602173 36.67605972290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929751873016357 3.305342674255371 34.846405029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791723370552063 3.569155693054199 37.48328399658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924256324768066 3.5373990535736084 37.16641616821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791846752166748 3.491075038909912 36.702598571777344
  batch 40 loss: 1.791846752166748, 3.491075038909912, 36.702598571777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927358150482178 2.54181170463562 27.210853576660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929224967956543 2.4316306114196777 26.109230041503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925033569335938 3.7520081996917725 39.312583923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917468547821045 3.279290199279785 34.58464813232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929916381835938 3.0305702686309814 32.09869384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925629615783691 3.8356316089630127 40.14888000488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924491167068481 2.938352346420288 31.175973892211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921786308288574 3.4579784870147705 36.37196350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918140888214111 3.4937798976898193 36.729610443115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924463748931885 2.990344762802124 31.695894241333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79098641872406 3.5311741828918457 37.102725982666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928357124328613 3.307541847229004 34.86825180053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 3.28075909614563 34.600120544433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930355072021484 3.890521764755249 40.69824981689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920269966125488 3.498764991760254 36.7796745300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793020248413086 3.197662115097046 33.7696418762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931228876113892 3.505479335784912 36.84791564941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793163537979126 2.874870777130127 30.5418701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938053607940674 3.728637218475342 39.080177307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923728227615356 3.3463616371154785 35.25598907470703
  batch 60 loss: 1.7923728227615356, 3.3463616371154785, 35.25598907470703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932456731796265 3.671038866043091 38.50363540649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916834354400635 3.394521474838257 35.73689651489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79292631149292 2.75270676612854 29.31999397277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924690246582031 2.9992125034332275 31.78459358215332
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793248176574707 2.6991403102874756 28.784652709960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920411825180054 2.9013845920562744 30.80588722229004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918871641159058 2.212674856185913 23.918636322021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916918992996216 2.759113073348999 29.382823944091797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792042851448059 2.318995237350464 24.981996536254883
Total LOSS train 34.05783565227802 valid 27.27233600616455
CE LOSS train 1.7925861688760611 valid 0.44801071286201477
Contrastive LOSS train 3.226524954575759 valid 0.579748809337616
EPOCH 122:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925689220428467 3.0331032276153564 32.123600006103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916555404663086 2.7727243900299072 29.518898010253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929096221923828 3.5638208389282227 37.431114196777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930352687835693 3.4608957767486572 36.40199279785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792429804801941 4.157506465911865 43.367496490478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933379411697388 3.8342957496643066 40.136295318603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919600009918213 3.5835049152374268 37.627010345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792057752609253 3.836700916290283 40.15906524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918555736541748 3.960761070251465 41.39946746826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918803691864014 2.5067241191864014 26.859121322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932393550872803 3.173048496246338 33.52372360229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926970720291138 3.708193778991699 38.874637603759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793789267539978 3.0962822437286377 32.75661087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932422161102295 3.38344144821167 35.62765884399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909576892852783 3.307097911834717 34.8619384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931057214736938 3.304318428039551 34.83628845214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929333448410034 3.1988720893859863 33.781654357910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928025722503662 3.128047227859497 33.073272705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926244735717773 3.0163862705230713 31.956485748291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915914058685303 3.592479944229126 37.716392517089844
  batch 20 loss: 1.7915914058685303, 3.592479944229126, 37.716392517089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793007731437683 3.2510926723480225 34.303932189941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929962873458862 3.0802645683288574 32.59564208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926141023635864 2.948460102081299 31.27721405029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793200135231018 3.255794048309326 34.35114288330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926443815231323 3.4662063121795654 36.454708099365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928918600082397 3.297351121902466 34.76640319824219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934753894805908 3.8233389854431152 40.0268669128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920650243759155 3.6157217025756836 37.949283599853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941296100616455 3.450972557067871 36.303855895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913466691970825 3.2463386058807373 34.25473403930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929798364639282 3.533060312271118 37.12358474731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791368007659912 3.8302652835845947 40.09402084350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929366827011108 3.300154447555542 34.79448318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931127548217773 3.9511067867279053 41.30418014526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930129766464233 3.6841351985931396 38.63436508178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932841777801514 3.319934368133545 34.99263000488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931740283966064 3.561310052871704 37.40627670288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918027639389038 3.4487619400024414 36.2794189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792373538017273 3.4199604988098145 35.99197769165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918225526809692 3.704716920852661 38.838993072509766
  batch 40 loss: 1.7918225526809692, 3.704716920852661, 38.838993072509766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923725843429565 3.2970833778381348 34.763206481933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792636752128601 3.1812806129455566 33.60544204711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792252540588379 3.670208215713501 38.49433517456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915762662887573 3.7264726161956787 39.05630111694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792857050895691 3.4708831310272217 36.50168991088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924727201461792 3.101109743118286 32.80356979370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923192977905273 3.1428425312042236 33.22074508666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792115330696106 3.3704559803009033 35.496673583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791573405265808 3.534179210662842 37.133365631103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792402982711792 3.0243024826049805 32.03542709350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910531759262085 3.626434803009033 38.05540084838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929433584213257 3.4495019912719727 36.287960052490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927104234695435 3.133861780166626 33.13132858276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931379079818726 2.885817766189575 30.651315689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921305894851685 3.4706871509552 36.499000549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930381298065186 2.8819029331207275 30.6120662689209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932701110839844 2.781254291534424 29.605812072753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932063341140747 3.5813698768615723 37.606903076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937403917312622 3.7237119674682617 39.030860900878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792379379272461 3.317425012588501 34.96662902832031
  batch 60 loss: 1.792379379272461, 3.317425012588501, 34.96662902832031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793286681175232 3.7772715091705322 39.566001892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917933464050293 3.8109397888183594 39.90119171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930834293365479 4.02919340133667 42.085018157958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 3.833327054977417 40.12577819824219
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7932050228118896 2.8178746700286865 29.971952438354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920193672180176 4.045105934143066 42.24307632446289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918645143508911 3.496034860610962 36.75221633911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916110754013062 3.404811382293701 35.839725494384766
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.792006254196167 3.1409764289855957 33.2017707824707
Total LOSS train 35.86129866379958 valid 37.00919723510742
CE LOSS train 1.7925996266878568 valid 0.44800156354904175
Contrastive LOSS train 3.4068699029775766 valid 0.7852441072463989
EPOCH 123:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925225496292114 3.2900283336639404 34.692806243896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915664911270142 3.5058951377868652 36.85051727294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928657531738281 3.1672093868255615 33.46495819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792730689048767 2.9372127056121826 31.164857864379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792213797569275 3.1006996631622314 32.79920959472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933473587036133 3.1575872898101807 33.36922073364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919206619262695 2.559171199798584 27.38363265991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920591831207275 3.214038610458374 33.93244552612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917864322662354 2.6071009635925293 27.862796783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919058799743652 2.9966354370117188 31.75826072692871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933614253997803 2.864367961883545 30.437042236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928699254989624 2.904125213623047 30.834121704101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936959266662598 2.551499366760254 27.30868911743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931898832321167 2.551840305328369 27.31159210205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909632921218872 2.5482630729675293 27.273595809936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932908535003662 2.7899556159973145 29.692846298217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930861711502075 3.5160720348358154 36.95380783081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929072380065918 3.2985284328460693 34.77819061279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792790174484253 2.6195602416992188 27.988391876220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917362451553345 2.2543840408325195 24.3355770111084
  batch 20 loss: 1.7917362451553345, 2.2543840408325195, 24.3355770111084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930151224136353 2.4252350330352783 26.045364379882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929255962371826 3.0104525089263916 31.897449493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925946712493896 2.8396029472351074 30.18862533569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932509183883667 3.7745213508605957 39.5384635925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926957607269287 4.073278427124023 42.52547836303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929099798202515 3.3023829460144043 34.816741943359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935034036636353 3.5389795303344727 37.18329620361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920501232147217 3.6753733158111572 38.54578399658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938926219940186 2.899078845977783 30.784679412841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791494607925415 2.852128505706787 30.312780380249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931571006774902 3.107731819152832 32.87047576904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914972305297852 2.9169085025787354 30.960582733154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930171489715576 2.7254533767700195 29.047550201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931936979293823 3.145480155944824 33.24799346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793036937713623 2.935835361480713 31.151390075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793229341506958 3.297380208969116 34.76702880859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930409908294678 2.981199026107788 31.605031967163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917448282241821 3.019498109817505 31.986724853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924131155014038 2.8553762435913086 30.346176147460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791773796081543 3.2263808250427246 34.05558395385742
  batch 40 loss: 1.791773796081543, 3.2263808250427246, 34.05558395385742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441487312317 3.335380792617798 35.1462516784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928287982940674 3.0402615070343018 32.19544219970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923208475112915 3.585878849029541 37.65110778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917227745056152 3.7989490032196045 39.781211853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928327322006226 2.8683321475982666 30.476154327392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792445182800293 2.871267557144165 30.50511932373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923147678375244 2.644667625427246 28.238990783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792163372039795 3.614635944366455 37.93852233886719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916520833969116 3.9999287128448486 41.79093933105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922375202178955 3.5043764114379883 36.83599853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908146381378174 3.3223700523376465 35.0145149230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927203178405762 2.4006409645080566 25.799129486083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924796342849731 2.3407695293426514 25.200176239013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929580211639404 2.9103763103485107 30.89672088623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791943907737732 3.4835939407348633 36.62788009643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929846048355103 2.837695837020874 30.16994285583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932202816009521 3.6151082515716553 37.94430160522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793337106704712 3.3283028602600098 35.07636642456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939491271972656 3.203540563583374 33.82935333251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926186323165894 2.9866955280303955 31.65957260131836
  batch 60 loss: 1.7926186323165894, 2.9866955280303955, 31.65957260131836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934621572494507 3.987769603729248 41.67115783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918330430984497 3.163712978363037 33.42896270751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929413318634033 3.189887046813965 33.691810607910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923589944839478 3.818566083908081 39.97801971435547
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793127417564392 2.338536500930786 25.178491592407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921428680419922 3.0948195457458496 32.74034118652344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920119762420654 2.6390879154205322 28.182889938354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916858196258545 3.1428918838500977 33.220603942871094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791961908340454 2.5470895767211914 27.26285743713379
Total LOSS train 32.75070615915152 valid 30.351673126220703
CE LOSS train 1.7925993185776932 valid 0.4479904770851135
Contrastive LOSS train 3.0958107178027814 valid 0.6367723941802979
EPOCH 124:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925162315368652 3.1572136878967285 33.36465072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916898727416992 3.465054512023926 36.44223403930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928903102874756 2.5862600803375244 27.65549087524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792747974395752 2.9174106121063232 30.966854095458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922306060791016 3.5973410606384277 37.76564025878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935165166854858 3.0943074226379395 32.73659133911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920387983322144 3.3250720500946045 35.04275894165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922234535217285 3.2711398601531982 34.503623962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917097806930542 2.9245004653930664 31.036714553833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918676137924194 3.403130531311035 35.82316970825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930909395217896 3.0649900436401367 32.4429931640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926573753356934 3.6953282356262207 38.74593734741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936080694198608 3.4038262367248535 35.831871032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930858135223389 3.1342475414276123 33.135562896728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908903360366821 2.976947546005249 31.560365676879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932640314102173 2.841893196105957 30.212196350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930212020874023 3.4697835445404053 36.4908561706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792928695678711 3.4159328937530518 35.95225524902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926098108291626 3.136756420135498 33.16017532348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916414737701416 3.5055394172668457 36.8470344543457
  batch 20 loss: 1.7916414737701416, 3.5055394172668457, 36.8470344543457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930641174316406 3.237523078918457 34.16829299926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930827140808105 3.4152891635894775 35.94597625732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79271399974823 2.53117036819458 27.104415893554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932407855987549 2.8819243907928467 30.612485885620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926996946334839 3.5188002586364746 36.9807014465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792781949043274 2.972971200942993 31.522493362426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933846712112427 2.9497005939483643 31.290390014648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920626401901245 2.650618076324463 28.298242568969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7942001819610596 3.0985023975372314 32.77922439575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915544509887695 3.8609962463378906 40.40151596069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930309772491455 3.234013795852661 34.1331672668457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913516759872437 2.940960168838501 31.200952529907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928282022476196 3.1704347133636475 33.49717712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930362224578857 3.5944721698760986 37.73775863647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928706407546997 2.5944011211395264 27.73688316345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793034315109253 3.648817777633667 38.281211853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792981743812561 3.5335302352905273 37.1282844543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917578220367432 2.6632795333862305 28.42455291748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480230331421 2.7739415168762207 29.53189468383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918516397476196 2.869159698486328 30.483448028564453
  batch 40 loss: 1.7918516397476196, 2.869159698486328, 30.483448028564453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792511224746704 3.5739951133728027 37.53246307373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928117513656616 3.0216453075408936 32.0092658996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923645973205566 2.884796142578125 30.64032554626465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.862884521484375 30.420576095581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929043769836426 2.808441162109375 29.877315521240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372703552246 3.6147170066833496 37.939544677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922335863113403 2.948744535446167 31.279680252075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920793294906616 3.1637542247772217 33.429622650146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791515827178955 2.7610840797424316 29.402355194091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923312187194824 3.3099284172058105 34.89161682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909765243530273 2.944586992263794 31.236846923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926620244979858 3.174945592880249 33.542118072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924824953079224 3.1618945598602295 33.41143035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792916178703308 3.4722421169281006 36.51533508300781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920010089874268 3.243558168411255 34.22758102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929332256317139 2.7743890285491943 29.536823272705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793190360069275 2.9852092266082764 31.645282745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932569980621338 3.179680585861206 33.590065002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793826937675476 3.538395643234253 37.17778396606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924884557724 3.336948871612549 35.1619758605957
  batch 60 loss: 1.7924884557724, 3.336948871612549, 35.1619758605957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932931184768677 3.63348650932312 38.12815856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918031215667725 3.558387279510498 37.375675201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930647134780884 3.4768035411834717 36.561100006103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925539016723633 3.0332772731781006 32.125328063964844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7932764291763306 3.296417713165283 34.75745391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919827699661255 3.3148350715637207 34.94033432006836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918436527252197 4.184840679168701 43.64025115966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915921211242676 3.710411787033081 38.89570999145508
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918009757995605 3.4247725009918213 36.039527893066406
Total LOSS train 33.52910414475661 valid 38.37895584106445
CE LOSS train 1.792581820487976 valid 0.44795024394989014
Contrastive LOSS train 3.1736522381122296 valid 0.8561931252479553
EPOCH 125:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480230331421 3.133748769760132 33.129966735839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915747165679932 3.596400499343872 37.75558090209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928681373596191 3.1849260330200195 33.642127990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927683591842651 3.4214987754821777 36.007755279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923446893692017 2.876225709915161 30.554601669311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934633493423462 3.5514092445373535 37.30755615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791930913925171 2.9465444087982178 31.257375717163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920829057693481 3.466287851333618 36.45496368408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917171716690063 2.9434800148010254 31.226516723632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918860912322998 3.0067543983459473 31.85942840576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793226957321167 3.491072654724121 36.703956604003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928109169006348 3.264019012451172 34.43300247192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936471700668335 3.310398817062378 34.89763641357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793029546737671 3.536205768585205 37.155086517333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790891408920288 3.8819901943206787 40.61079025268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932305335998535 3.575221061706543 37.54544448852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930092811584473 3.520975351333618 37.00276565551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929320335388184 3.429875135421753 36.09168243408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925139665603638 3.6370198726654053 38.16271209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915141582489014 3.1644794940948486 33.436309814453125
  batch 20 loss: 1.7915141582489014, 3.1644794940948486, 33.436309814453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927955389022827 3.836698055267334 40.1597785949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929083108901978 3.5609383583068848 37.40229034423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926408052444458 2.7800352573394775 29.592992782592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793159008026123 3.601776599884033 37.8109245300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927627563476562 3.320237636566162 34.995140075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927546501159668 3.627349853515625 38.066253662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934794425964355 2.95286226272583 31.322101593017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920736074447632 2.9775195121765137 31.567270278930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941203117370605 2.808988332748413 29.884004592895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914416790008545 3.51694655418396 36.960906982421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930105924606323 2.7813429832458496 29.606441497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914228439331055 2.8739798069000244 30.531219482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929714918136597 3.4289867877960205 36.08283996582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931125164031982 3.178523540496826 33.57835006713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929545640945435 2.8768105506896973 30.561059951782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931736707687378 3.3396542072296143 35.18971633911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792949914932251 3.3557114601135254 35.35006332397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917512655258179 3.347111940383911 35.26287078857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924164533615112 3.5280792713165283 37.07320785522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918918132781982 3.47843337059021 36.57622528076172
  batch 40 loss: 1.7918918132781982, 3.47843337059021, 36.57622528076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924301624298096 3.3061716556549072 34.854148864746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927166223526 3.1351027488708496 33.14374542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923144102096558 3.4111928939819336 35.90424346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916078567504883 3.4760940074920654 36.552547454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928613424301147 3.3182058334350586 34.97492218017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792495608329773 3.166651487350464 33.45901107788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923442125320435 2.703557014465332 28.82791519165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920681238174438 3.42031192779541 35.99518585205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915241718292236 3.301854372024536 34.81006622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923002243041992 3.218461275100708 33.97691345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909905910491943 3.478835105895996 36.579341888427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927868366241455 3.325594663619995 35.04873275756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925492525100708 3.0505709648132324 32.29825973510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929717302322388 3.298112392425537 34.77409744262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920050621032715 3.3316903114318848 35.10890579223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928930521011353 3.315333843231201 34.946231842041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932208776474 3.133462429046631 33.127845764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931965589523315 3.304793119430542 34.841129302978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937637567520142 3.477971076965332 36.57347106933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924079895019531 3.4706151485443115 36.498558044433594
  batch 60 loss: 1.7924079895019531, 3.4706151485443115, 36.498558044433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79326593875885 2.963085651397705 31.424121856689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791700005531311 3.217369794845581 33.965396881103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928926944732666 2.8355190753936768 30.148082733154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372465133667 3.1618716716766357 33.41109085083008
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931089401245117 2.15763258934021 23.369434356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920886278152466 3.152721643447876 33.319305419921875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792008876800537 2.54595947265625 27.251604080200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916207313537598 2.764270782470703 29.434328079223633
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791922688484192 2.4868595600128174 26.660518646240234
Total LOSS train 34.42188180776743 valid 29.166439056396484
CE LOSS train 1.792561573248643 valid 0.447980672121048
Contrastive LOSS train 3.262932007129376 valid 0.6217148900032043
EPOCH 126:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924151420593262 3.051175355911255 32.304168701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916446924209595 3.361032247543335 35.4019660949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792877197265625 2.998770236968994 31.78057861328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927600145339966 3.0805344581604004 32.59810256958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.9688847064971924 31.48118019104004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793389916419983 3.2358696460723877 34.15208435058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919867038726807 2.9908573627471924 31.700559616088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921338081359863 2.978262424468994 31.574756622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918683290481567 3.1391441822052 33.183311462402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918769121170044 2.664760112762451 28.43947982788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793148159980774 3.28554368019104 34.64858627319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927165031433105 2.925889492034912 31.051612854003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793505072593689 3.190208911895752 33.695594787597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931162118911743 3.0582399368286133 32.37551498413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790942907333374 2.8044066429138184 29.835010528564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931807041168213 2.982884168624878 31.62202262878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929710149765015 2.6626594066619873 28.419565200805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929049730300903 2.301117181777954 24.8040771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792752742767334 3.0136919021606445 31.929672241210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791642427444458 2.459902048110962 26.390663146972656
  batch 20 loss: 1.791642427444458, 2.459902048110962, 26.390663146972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792909860610962 2.582048177719116 27.613391876220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792844295501709 2.923332929611206 31.026174545288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926030158996582 2.23704195022583 24.163021087646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931239604949951 3.1703686714172363 33.49681091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928165197372437 2.8939743041992188 30.732559204101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929761409759521 3.3967485427856445 35.760459899902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936134338378906 3.477834463119507 36.571956634521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922420501708984 2.407416343688965 25.866405487060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7943044900894165 3.0667805671691895 32.46210861206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916232347488403 2.411001682281494 25.901639938354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930659055709839 2.635847330093384 28.151538848876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914358377456665 2.930290937423706 31.094345092773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929356098175049 3.0606887340545654 32.39982223510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930232286453247 3.670363664627075 38.49665832519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928284406661987 3.4675097465515137 36.467926025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793080449104309 2.9311556816101074 31.104639053344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929819822311401 3.3429172039031982 35.22215270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916431427001953 3.174139976501465 33.533042907714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923331260681152 2.6599323749542236 28.39165687561035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 3.309823513031006 34.890010833740234
  batch 40 loss: 1.791774034500122, 3.309823513031006, 34.890010833740234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792479157447815 3.814359426498413 39.936073303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927590608596802 2.890261650085449 30.695375442504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923215627670288 2.6805453300476074 28.597776412963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917331457138062 2.8909213542938232 30.700946807861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792948842048645 2.6248128414154053 28.041078567504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79246985912323 2.704918146133423 28.841650009155273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922921180725098 2.792775869369507 29.720050811767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920746803283691 3.434298038482666 36.13505554199219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914799451828003 3.081780433654785 32.609283447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923405170440674 3.2386672496795654 34.179012298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791002869606018 3.654301404953003 38.33401870727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792691946029663 2.8729796409606934 30.522489547729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924343347549438 3.0943922996520996 32.736358642578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79286789894104 2.8812053203582764 30.604921340942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791939616203308 3.1105051040649414 32.89699172973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929095029830933 1.936798334121704 21.160892486572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931742668151855 3.211878776550293 33.911964416503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793155312538147 2.855919599533081 30.352352142333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937816381454468 3.2189650535583496 33.98343276977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924480438232422 2.7849345207214355 29.641794204711914
  batch 60 loss: 1.7924480438232422, 2.7849345207214355, 29.641794204711914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932701110839844 4.029331684112549 42.086585998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791847586631775 3.504274606704712 36.8345947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929695844650269 3.1939635276794434 33.73260498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792405605316162 3.2802510261535645 34.594913482666016
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793171763420105 2.7326772212982178 29.119943618774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79197096824646 2.8811697959899902 30.603670120239258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918199300765991 2.948556661605835 31.277387619018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915213108062744 2.826896905899048 30.060489654541016
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919517755508423 3.201263904571533 33.804588317871094
Total LOSS train 31.856999852107123 valid 31.43653392791748
CE LOSS train 1.7925737674419697 valid 0.44798794388771057
Contrastive LOSS train 3.0064426055321327 valid 0.8003159761428833
EPOCH 127:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241144657135 2.5726633071899414 27.519044876098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791433334350586 3.3273298740386963 35.064735412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927298545837402 2.949239492416382 31.285125732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926338911056519 2.914637327194214 30.939006805419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792309045791626 3.0256476402282715 32.04878616333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933646440505981 3.559734582901001 37.39071273803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919878959655762 3.6309001445770264 38.100990295410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921090126037598 3.207369089126587 33.86580276489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918424606323242 3.248926877975464 34.28111267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919563055038452 2.2450520992279053 24.242477416992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931965589523315 2.9704229831695557 31.497425079345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79268479347229 2.9397919178009033 31.190603256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934675216674805 2.5196728706359863 26.990196228027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930375337600708 2.451599597930908 26.309032440185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790915846824646 3.0345802307128906 32.13671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931472063064575 3.614490270614624 37.93804931640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928959131240845 2.3895697593688965 25.688594818115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 3.077721118927002 32.57002639770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925759553909302 2.8209176063537598 30.001750946044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915210723876953 1.986546277999878 21.656984329223633
  batch 20 loss: 1.7915210723876953, 1.986546277999878, 21.656984329223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928506135940552 3.0949838161468506 32.74269104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928625345230103 3.735760450363159 39.15046691894531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926185131072998 2.046123743057251 22.253854751586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932169437408447 2.3375768661499023 25.16898536682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926911115646362 3.501986026763916 36.81254959106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928396463394165 3.7061994075775146 38.85483169555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933850288391113 2.8945295810699463 30.738679885864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919528484344482 3.1127941608428955 32.91989517211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939361333847046 3.496971607208252 36.76365280151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791543960571289 3.8290209770202637 40.081756591796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929757833480835 2.4641168117523193 26.434144973754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914155721664429 2.9254465103149414 31.045881271362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792864203453064 3.0577588081359863 32.370452880859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930277585983276 3.0637166500091553 32.43019485473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929258346557617 3.3043134212493896 34.8360595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929234504699707 3.4070816040039062 35.863739013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928234338760376 3.4652183055877686 36.44500732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916145324707031 3.690547227859497 38.697086334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923550605773926 3.5588953495025635 37.381309509277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917498350143433 3.448467493057251 36.276424407958984
  batch 40 loss: 1.7917498350143433, 3.448467493057251, 36.276424407958984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924768924713135 2.8252816200256348 30.045291900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926708459854126 3.8095266819000244 39.887939453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792145848274231 2.794506788253784 29.737215042114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915338277816772 3.2170732021331787 33.96226501464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928248643875122 3.457319974899292 36.366024017333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924405336380005 3.701077938079834 38.80322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923228740692139 3.2829930782318115 34.62225341796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921180725097656 3.439990282058716 36.192020416259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915629148483276 3.3812952041625977 35.60451126098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923030853271484 2.7933998107910156 29.726301193237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910325527191162 3.411241054534912 35.9034423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927347421646118 3.1380996704101562 33.17373275756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924336194992065 2.948336124420166 31.275793075561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928553819656372 2.5949676036834717 27.74253273010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919621467590332 3.2261962890625 34.053924560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930371761322021 2.917106866836548 30.9641056060791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793212890625 3.3723185062408447 35.51639938354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931239604949951 3.2705321311950684 34.49844741821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793717622756958 3.388237953186035 35.67609405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924357652664185 3.1182336807250977 32.97477340698242
  batch 60 loss: 1.7924357652664185, 3.1182336807250977, 32.97477340698242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932608127593994 3.8387703895568848 40.180965423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917588949203491 3.2837016582489014 34.62877655029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792899250984192 2.666736125946045 28.460262298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923706769943237 2.9860222339630127 31.652591705322266
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931078672409058 3.287153482437134 34.664642333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919349670410156 3.370955228805542 35.501487731933594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918438911437988 3.5406928062438965 37.19877243041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520595550537 3.03648042678833 32.15632247924805
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918157577514648 3.337219476699829 35.16401290893555
Total LOSS train 32.989205668522764 valid 35.00514888763428
CE LOSS train 1.7925223607283371 valid 0.4479539394378662
Contrastive LOSS train 3.1196683113391583 valid 0.8343048691749573
EPOCH 128:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923741340637207 3.406810998916626 35.8604850769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915573120117188 3.391911268234253 35.710670471191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928457260131836 3.3338699340820312 35.13154602050781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792763113975525 3.310245990753174 34.89522171020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923122644424438 2.9504621028900146 31.296934127807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934770584106445 3.3473005294799805 35.266483306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791995882987976 3.7069602012634277 38.86159896850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792196273803711 3.1490540504455566 33.282737731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917817831039429 2.9703261852264404 31.495044708251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919105291366577 3.16595721244812 33.45148468017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931733131408691 3.4739866256713867 36.533042907714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927603721618652 3.532275676727295 37.115516662597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935717105865479 4.199025630950928 43.78382873535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930858135223389 3.4963324069976807 36.756412506103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908614873886108 3.0779271125793457 32.570133209228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793142557144165 3.285255193710327 34.645694732666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929236888885498 3.588690757751465 37.679832458496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928712368011475 3.875713348388672 40.55000305175781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792622685432434 3.078908681869507 32.58171081542969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791601538658142 2.838531017303467 30.176912307739258
  batch 20 loss: 1.791601538658142, 2.838531017303467, 30.176912307739258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792895793914795 3.5153017044067383 36.9459114074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792786955833435 2.373473644256592 25.527524948120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925150394439697 2.057711601257324 22.369630813598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931212186813354 2.64640212059021 28.257143020629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792722463607788 2.8197271823883057 29.989994049072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792818307876587 3.2079689502716064 33.87250900268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793470025062561 2.8970367908477783 30.763837814331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921199798583984 3.1752867698669434 33.54499053955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7941670417785645 2.7183313369750977 28.977479934692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791562795639038 3.303974151611328 34.831302642822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929847240447998 3.116163969039917 32.95462417602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913033962249756 2.7091422080993652 28.882726669311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927230596542358 2.991927146911621 31.711994171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929587364196777 2.822584390640259 30.018802642822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929197549819946 2.81386661529541 29.93158531188965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931069135665894 3.179950475692749 33.592613220214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929152250289917 3.151104688644409 33.30396270751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791678547859192 2.5448498725891113 27.240177154541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924237251281738 3.147550106048584 33.26792526245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917919158935547 3.069084644317627 32.482635498046875
  batch 40 loss: 1.7917919158935547, 3.069084644317627, 32.482635498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924189567565918 2.1445233821868896 23.237653732299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 3.491518259048462 36.7078971862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922602891921997 3.8910317420959473 40.70257568359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791719675064087 3.083611249923706 32.62783432006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928760051727295 3.155836820602417 33.35124588012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923617362976074 2.897937536239624 30.771738052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922627925872803 2.7131848335266113 28.924110412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792118787765503 3.3554890155792236 35.347007751464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915735244750977 2.9220757484436035 31.0123291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792278528213501 2.772395372390747 29.516231536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790912389755249 3.5274713039398193 37.06562423706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926753759384155 3.779743194580078 39.59010696411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925773859024048 2.8765182495117188 30.55776023864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929319143295288 2.8019235134124756 29.81216812133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919844388961792 2.8178493976593018 29.970478057861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930494546890259 2.3814098834991455 25.607147216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932809591293335 2.7976133823394775 29.7694149017334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932378053665161 3.455410957336426 36.347347259521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937085628509521 3.4924967288970947 36.71867752075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 2.5729095935821533 27.521514892578125
  batch 60 loss: 1.7924197912216187, 2.5729095935821533, 27.521514892578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932186126708984 3.207482099533081 33.8680419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791852355003357 3.73327898979187 39.12464141845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930388450622559 3.2691245079040527 34.484283447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924436330795288 2.9580209255218506 31.37265396118164
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793127417564392 2.2325503826141357 24.11863136291504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919926643371582 2.9099926948547363 30.891918182373047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918925285339355 2.932309627532959 31.114990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915563583374023 2.9803853034973145 31.595409393310547
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791897177696228 2.2173125743865967 23.965023040771484
Total LOSS train 32.83445816040039 valid 29.39183521270752
CE LOSS train 1.792551645865807 valid 0.447974294424057
Contrastive LOSS train 3.1041906210092396 valid 0.5543281435966492
EPOCH 129:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924128770828247 2.736616611480713 29.158578872680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915509939193726 3.4119179248809814 35.910728454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927820682525635 3.088005781173706 32.6728401184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927663326263428 3.42965030670166 36.08926773071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792342185974121 3.0914602279663086 32.70694351196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934330701828003 3.615593910217285 37.949371337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919665575027466 3.0376691818237305 32.16865921020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921688556671143 2.565720319747925 27.449373245239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917921543121338 3.1155850887298584 32.9476432800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791860580444336 2.890673875808716 30.698598861694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931329011917114 2.6846487522125244 28.639619827270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792716145515442 2.6068177223205566 27.86089324951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935017347335815 2.8065412044525146 29.85891342163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929829359054565 3.6017909049987793 37.81089401245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908483743667603 2.934300661087036 31.133853912353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931276559829712 3.209906816482544 33.89219665527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792891025543213 3.5030717849731445 36.8236083984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928723096847534 3.126472234725952 33.057594299316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792562484741211 3.1088433265686035 32.88099670410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791568636894226 3.6617722511291504 38.4092903137207
  batch 20 loss: 1.791568636894226, 3.6617722511291504, 38.4092903137207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928661108016968 2.559358835220337 27.38645362854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927806377410889 2.842921733856201 30.22199821472168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925031185150146 2.728989839553833 29.082401275634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931066751480103 3.5388619899749756 37.181724548339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 2.6302990913391113 28.095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928098440170288 2.8622167110443115 30.41497802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934582233428955 2.8385627269744873 30.179086685180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920408248901367 2.4952900409698486 26.74494171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940516471862793 2.38425874710083 25.636638641357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915127277374268 3.6682608127593994 38.47412109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930200099945068 2.9348487854003906 31.141508102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913672924041748 2.5625836849212646 27.417203903198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792816162109375 2.6702795028686523 28.4956111907959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79301917552948 2.5218746662139893 27.011764526367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929465770721436 2.8034310340881348 29.827255249023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931392192840576 2.9532885551452637 31.326025009155273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929612398147583 2.6828110218048096 28.621070861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916572093963623 2.8695261478424072 30.48691749572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923401594161987 2.6215896606445312 28.008235931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916796207427979 3.10563063621521 32.84798812866211
  batch 40 loss: 1.7916796207427979, 3.10563063621521, 32.84798812866211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923563718795776 2.733065605163574 29.12301254272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927225828170776 2.668574094772339 28.478464126586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922587394714355 2.688138246536255 28.673641204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791666865348816 2.8178622722625732 29.97028923034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927963733673096 2.088911533355713 22.68191146850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922697067260742 2.920820951461792 31.00048065185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922013998031616 2.8696341514587402 30.488544464111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920719385147095 3.22346568107605 34.026729583740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915167808532715 2.7905080318450928 29.696598052978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923270463943481 2.9306719303131104 31.09904670715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791023850440979 2.6747491359710693 28.538515090942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927439212799072 2.438312530517578 26.17586898803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925719022750854 2.727695941925049 29.069530487060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929713726043701 2.7511274814605713 29.304244995117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792051911354065 3.2516603469848633 34.30865478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929472923278809 2.6526453495025635 28.319400787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931580543518066 2.718733310699463 28.98048973083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931571006774902 3.0818800926208496 32.61195755004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937201261520386 3.2487246990203857 34.280967712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792352318763733 2.7251884937286377 29.04423713684082
  batch 60 loss: 1.792352318763733, 2.7251884937286377, 29.04423713684082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793120265007019 2.4126503467559814 25.91962242126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917468547821045 3.680605888366699 38.597808837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792871356010437 3.0791730880737305 32.58460235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792311429977417 3.37429141998291 35.53522491455078
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930617332458496 2.4088451862335205 25.881513595581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920633554458618 3.3171074390411377 34.963138580322266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791882872581482 2.800414800643921 29.796031951904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791596531867981 3.0315518379211426 32.107112884521484
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920258045196533 3.3107025623321533 34.899051666259766
Total LOSS train 30.9401976952186 valid 32.94133377075195
CE LOSS train 1.792524112187899 valid 0.44800645112991333
Contrastive LOSS train 2.9147673680232122 valid 0.8276756405830383
EPOCH 130:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924991846084595 3.0422322750091553 32.214820861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915551662445068 2.9114785194396973 30.906339645385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927218675613403 3.2504005432128906 34.29672622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792663812637329 3.2059824466705322 33.85248947143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924617528915405 2.918196439743042 30.97442626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933393716812134 4.0999979972839355 42.79331970214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920444011688232 3.111593008041382 32.90797424316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921148538589478 2.8428521156311035 30.22063446044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917243242263794 3.458448886871338 36.37621307373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918951511383057 2.7904882431030273 29.69677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930768728256226 3.1646220684051514 33.43929672241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792698860168457 3.719188928604126 38.984588623046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935771942138672 3.170905828475952 33.50263595581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930289506912231 3.6509711742401123 38.30274200439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908517122268677 3.3612029552459717 35.40288162231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930796146392822 3.880913019180298 40.602210998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792871356010437 3.509624719619751 36.88911819458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927604913711548 3.6080453395843506 37.87321090698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925363779067993 2.9834365844726562 31.626901626586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791555404663086 2.86283278465271 30.419883728027344
  batch 20 loss: 1.791555404663086, 2.86283278465271, 30.419883728027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928773164749146 3.379002571105957 35.58290100097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927160263061523 2.680384397506714 28.596561431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924593687057495 3.385874032974243 35.65119934082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930948734283447 3.321979284286499 35.01288604736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927314043045044 2.9315309524536133 31.108041763305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928361892700195 3.559743642807007 37.3902702331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934550046920776 3.127605438232422 33.06950759887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920223474502563 2.3634836673736572 25.42685890197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940095663070679 2.498347759246826 26.777488708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914117574691772 2.8647310733795166 30.438722610473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929714918136597 3.509194850921631 36.88492202758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79134202003479 2.7904155254364014 29.695497512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927837371826172 2.6960043907165527 28.75282859802246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929997444152832 3.582655429840088 37.61955261230469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928699254989624 3.722142219543457 39.01428985595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930619716644287 2.7093729972839355 28.88679313659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792915940284729 2.919076442718506 30.983680725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916665077209473 3.7208731174468994 39.00040054321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923309803009033 2.985722303390503 31.649555206298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791772484779358 3.089423894882202 32.686012268066406
  batch 40 loss: 1.791772484779358, 3.089423894882202, 32.686012268066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924232482910156 3.462146282196045 36.41388702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79271399974823 2.4944112300872803 26.736825942993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792262077331543 3.5543570518493652 37.33583450317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916868925094604 3.2102229595184326 33.893917083740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928588390350342 2.483168363571167 26.624542236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923169136047363 2.624149799346924 28.0338134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922509908676147 3.1509153842926025 33.3014030456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792084813117981 2.251284122467041 24.3049259185791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791500449180603 2.6696903705596924 28.4884033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922805547714233 2.841744899749756 30.20973014831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909818887710571 2.6119956970214844 27.910938262939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926543951034546 2.9258713722229004 31.051366806030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924062013626099 3.0819687843322754 32.61209487915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927968502044678 2.51023268699646 26.895124435424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919046878814697 2.837794065475464 30.169845581054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929431200027466 3.6645164489746094 38.438106536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931495904922485 3.3107235431671143 34.90038299560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931252717971802 2.9346649646759033 31.139774322509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936351299285889 3.129810333251953 33.091739654541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923790216445923 3.0882787704467773 32.675167083740234
  batch 60 loss: 1.7923790216445923, 3.0882787704467773, 32.675167083740234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931486368179321 2.655416250228882 28.34731101989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917447090148926 2.99942946434021 31.786039352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929719686508179 2.792517900466919 29.718151092529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924089431762695 3.111480712890625 32.9072151184082
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931233644485474 2.164433479309082 23.437458038330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792019248008728 3.2338759899139404 34.13077926635742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918699979782104 3.0031182765960693 31.82305335998535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791561245918274 3.1440038681030273 33.23160171508789
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791902780532837 3.3122191429138184 34.91409683227539
Total LOSS train 32.55284861051119 valid 33.524882793426514
CE LOSS train 1.7925098144091092 valid 0.44797569513320923
Contrastive LOSS train 3.0760338893303505 valid 0.8280547857284546
EPOCH 131:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923909425735474 3.1274118423461914 33.06650924682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915360927581787 2.9879958629608154 31.67149543762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792741298675537 2.8539819717407227 30.332561492919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927268743515015 3.6066982746124268 37.859710693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923520803451538 3.1565771102905273 33.358123779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793480396270752 2.777803659439087 29.571516036987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792002558708191 2.906775951385498 30.85976219177246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792140007019043 2.873180627822876 30.523944854736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917710542678833 3.944242000579834 41.23419189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918932437896729 2.9267284870147705 31.05917739868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931673526763916 3.2748782634735107 34.54195022583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927515506744385 3.5855705738067627 37.64845657348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79352605342865 3.1842260360717773 33.63578796386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930582761764526 2.743828058242798 29.231338500976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909505367279053 2.829808473587036 30.089035034179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793209195137024 3.165710926055908 33.4503173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929680347442627 3.0750770568847656 32.543739318847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928285598754883 2.996703863143921 31.759868621826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513370513916 2.4478261470794678 26.270774841308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914847135543823 2.8378801345825195 30.170286178588867
  batch 20 loss: 1.7914847135543823, 2.8378801345825195, 30.170286178588867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927602529525757 2.7958478927612305 29.751239776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927604913711548 2.966413736343384 31.456897735595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924861907958984 2.094111204147339 22.733598709106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931090593338013 3.5279428958892822 37.072540283203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926850318908691 2.832136869430542 30.11405372619629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792789340019226 2.9055917263031006 30.84870719909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793364405632019 2.5155410766601562 26.948774337768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920337915420532 2.4842159748077393 26.634193420410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940640449523926 2.7911314964294434 29.705379486083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915515899658203 2.8951289653778076 30.742841720581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929080724716187 3.2620551586151123 34.41345977783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913627624511719 3.0776867866516113 32.56822967529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928129434585571 3.0153191089630127 31.946002960205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930408716201782 2.34317946434021 25.224836349487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929092645645142 3.339773416519165 35.190643310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793022632598877 2.8580117225646973 30.373138427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929095029830933 3.301896095275879 34.811866760253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916704416275024 3.051854133605957 32.310211181640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923810482025146 3.1238789558410645 33.03116989135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917500734329224 3.366631269454956 35.458065032958984
  batch 40 loss: 1.7917500734329224, 3.366631269454956, 35.458065032958984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923699617385864 2.8289530277252197 30.081899642944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927160263061523 2.8931992053985596 30.724708557128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792253017425537 3.3811802864074707 35.60405349731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791682481765747 3.3021459579467773 34.81314468383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928402423858643 3.069209337234497 32.48493194580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923948764801025 2.9860012531280518 31.652406692504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792261004447937 3.518369436264038 36.9759521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921212911605835 2.9849138259887695 31.641260147094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915048599243164 2.7476308345794678 29.26781463623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922073602676392 2.677053213119507 28.562740325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909249067306519 2.322291374206543 25.013837814331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926526069641113 2.4272663593292236 26.06531524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924776077270508 2.552389621734619 27.31637191772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928743362426758 2.7248284816741943 29.041160583496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918838262557983 3.128415822982788 33.07604217529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928777933120728 2.5003912448883057 26.796789169311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931021451950073 3.6242828369140625 38.03593063354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931339740753174 2.8594212532043457 30.387346267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936912775039673 4.0999531745910645 42.79322052001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923446893692017 2.7989935874938965 29.78228187561035
  batch 60 loss: 1.7923446893692017, 2.7989935874938965, 29.78228187561035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931013107299805 2.290600061416626 24.699100494384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917219400405884 2.4513611793518066 26.30533218383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929106950759888 2.939995765686035 31.192869186401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792379379272461 2.4635634422302246 26.428014755249023
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79306960105896 3.0697195529937744 32.490264892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791931390762329 3.171360492706299 33.50553512573242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917695045471191 3.1352121829986572 33.143890380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915146350860596 3.55033278465271 37.29484176635742
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919524908065796 2.879021644592285 30.582168579101562
Total LOSS train 31.406879806518553 valid 33.631608963012695
CE LOSS train 1.7925132494706375 valid 0.4479881227016449
Contrastive LOSS train 2.9614366678091195 valid 0.7197554111480713
EPOCH 132:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924091815948486 3.1008026599884033 32.80043411254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914727926254272 2.5268728733062744 27.06020164489746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926750183105469 2.497714042663574 26.76981544494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926279306411743 2.9426143169403076 31.21877098083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792238473892212 2.701568841934204 28.807926177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933436632156372 2.75893235206604 29.382667541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919679880142212 3.359023094177246 35.3822021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920701503753662 2.746951103210449 29.261581420898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918261289596558 2.871119260787964 30.503019332885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791778564453125 3.2926740646362305 34.71852111816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931112051010132 2.812490224838257 29.918014526367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792709469795227 3.0619618892669678 32.412330627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935574054718018 3.0287868976593018 32.081424713134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930974960327148 3.172013521194458 33.51323318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908871173858643 2.8957035541534424 30.747922897338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930852174758911 3.039867639541626 32.1917610168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928907871246338 2.5540173053741455 27.33306312561035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927038669586182 3.4879989624023438 36.67269515991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792539358139038 3.6940343379974365 38.732879638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915284633636475 3.9198009967803955 40.98953628540039
  batch 20 loss: 1.7915284633636475, 3.9198009967803955, 40.98953628540039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925331592559814 4.301220417022705 44.80473709106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792859673500061 3.5605692863464355 37.39855194091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924132347106934 3.257500410079956 34.36741638183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930363416671753 3.4074456691741943 35.86749267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926054000854492 3.06034779548645 32.39608383178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927998304367065 4.0513482093811035 42.30628204345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932908535003662 4.1106438636779785 42.89972686767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919729948043823 4.1042022705078125 42.8339958190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938604354858398 3.716531991958618 38.95918273925781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914472818374634 2.8764941692352295 30.55638885498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793013334274292 3.5511159896850586 37.304176330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914204597473145 2.785167932510376 29.643098831176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927803993225098 3.7974863052368164 39.767642974853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929054498672485 3.6616971492767334 38.409873962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929402589797974 3.6457748413085938 38.25069046020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929918766021729 3.2871525287628174 34.66451644897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929764986038208 3.625472068786621 38.047698974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791696310043335 3.1762800216674805 33.55449676513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792264461517334 3.0196752548217773 31.989017486572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916196584701538 3.049875020980835 32.29037094116211
  batch 40 loss: 1.7916196584701538, 3.049875020980835, 32.29037094116211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792368769645691 2.8504605293273926 30.296972274780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925736904144287 2.9220407009124756 31.012981414794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792279839515686 2.8980138301849365 30.772418975830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791651964187622 3.4366884231567383 36.15853500366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928866147994995 2.628203868865967 28.074926376342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924736738204956 3.1338577270507812 33.13105010986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922669649124146 2.9771125316619873 31.563392639160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920283079147339 3.360736608505249 35.399391174316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791715383529663 3.099792957305908 32.789642333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792353630065918 3.3511404991149902 35.30376052856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910021543502808 3.5051281452178955 36.842281341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926987409591675 2.89392352104187 30.73193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924343347549438 3.229259729385376 34.08503341674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792872428894043 3.3004872798919678 34.79774475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919416427612305 3.164646863937378 33.438411712646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929158210754395 3.1446001529693604 33.23891830444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931475639343262 3.570507764816284 37.498226165771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931181192398071 3.313084125518799 34.92395782470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936530113220215 3.4855825901031494 36.649478912353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922636270523071 3.2610690593719482 34.4029541015625
  batch 60 loss: 1.7922636270523071, 3.2610690593719482, 34.4029541015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930954694747925 4.081605911254883 42.609153747558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917215824127197 3.4662790298461914 36.45450973510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792822241783142 2.9071686267852783 30.8645076751709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924059629440308 2.5428695678710938 27.221101760864258
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7931112051010132 2.570917844772339 27.502290725708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919615507125854 2.9829373359680176 31.621334075927734
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918434143066406 2.8368654251098633 30.160497665405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791635513305664 3.1854665279388428 33.64630126953125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791959524154663 2.5669608116149902 27.46156883239746
Total LOSS train 34.03958487877479 valid 30.72242546081543
CE LOSS train 1.7924884759462796 valid 0.44798988103866577
Contrastive LOSS train 3.2247096465184137 valid 0.6417402029037476
EPOCH 133:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924588918685913 3.0330686569213867 32.123146057128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915596961975098 3.2407150268554688 34.19871139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792876958847046 2.9222655296325684 31.015533447265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927263975143433 2.779149293899536 29.584218978881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922357320785522 3.4568262100219727 36.360496520996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933048009872437 3.6523072719573975 38.316375732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919644117355347 3.6670799255371094 38.462764739990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920113801956177 3.167832374572754 33.4703369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 3.264765977859497 34.43949890136719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918287515640259 3.2233071327209473 34.024898529052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930928468704224 3.2021875381469727 33.81496810913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925688028335571 3.3115508556365967 34.908077239990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935110330581665 2.736356019973755 29.15707015991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930673360824585 3.169842004776001 33.491485595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909369468688965 2.7506332397460938 29.297269821166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931525707244873 3.39150333404541 35.70818328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930103540420532 3.753535747528076 39.328369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927943468093872 3.6087875366210938 37.88066864013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925910949707031 2.7321438789367676 29.114028930664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915512323379517 3.1299784183502197 33.09133529663086
  batch 20 loss: 1.7915512323379517, 3.1299784183502197, 33.09133529663086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792840838432312 3.207660436630249 33.869441986083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928906679153442 3.6059494018554688 37.852386474609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927006483078003 2.755129337310791 29.343992233276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932261228561401 2.9152116775512695 30.945343017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 3.8092548847198486 39.88525390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928047180175781 3.2620444297790527 34.41324996948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932507991790771 3.209686756134033 33.89011764526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919437885284424 3.3688511848449707 35.48045349121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793838620185852 2.9504153728485107 31.297992706298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913411855697632 3.0496957302093506 32.288299560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792855143547058 3.3238015174865723 35.03086853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913153171539307 2.9202721118927 30.994035720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702555656433 2.600285053253174 27.795551300048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792904257774353 3.407440185546875 35.867305755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792940616607666 2.8804595470428467 30.597536087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931023836135864 3.1712467670440674 33.50556945800781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929097414016724 3.092036247253418 32.71327209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791670560836792 2.768596649169922 29.477636337280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923640012741089 2.4527599811553955 26.319963455200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917107343673706 3.0411758422851562 32.203468322753906
  batch 40 loss: 1.7917107343673706, 3.0411758422851562, 32.203468322753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925198078155518 2.476181983947754 26.554340362548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792618751525879 3.839585065841675 40.18846893310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792204737663269 3.143048048019409 33.222686767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915866374969482 4.265665054321289 44.448238372802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928719520568848 2.9850621223449707 31.643491744995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924379110336304 3.4229965209960938 36.022403717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921723127365112 3.487746477127075 36.66963577270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920068502426147 3.885903835296631 40.65104675292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916311025619507 3.3876614570617676 35.668243408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922486066818237 4.244720458984375 44.23945236206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910215854644775 4.422831058502197 46.01933288574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926663160324097 3.488626718521118 36.67893600463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391300201416 3.8311917781829834 40.10430908203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792969822883606 3.624366521835327 38.0366325378418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919877767562866 3.7577052116394043 39.369041442871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930337190628052 4.4172210693359375 45.96524429321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932124137878418 3.8552305698394775 40.34552001953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931240797042847 3.4481542110443115 36.27466583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793595314025879 3.4326634407043457 36.1202278137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922017574310303 3.896738290786743 40.759586334228516
  batch 60 loss: 1.7922017574310303, 3.896738290786743, 40.759586334228516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930071353912354 3.175126075744629 33.54426956176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915438413619995 2.9182565212249756 30.974109649658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927289009094238 3.2226128578186035 34.018856048583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924432754516602 3.1663243770599365 33.4556884765625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930864095687866 2.891733407974243 30.710420608520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919564247131348 3.795898675918579 39.750946044921875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918089628219604 3.780444622039795 39.596256256103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916209697723389 3.56459379196167 37.43756103515625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919495105743408 3.3221490383148193 35.0134391784668
Total LOSS train 34.81913883502667 valid 37.94955062866211
CE LOSS train 1.792498700435345 valid 0.4479873776435852
Contrastive LOSS train 3.3026640341832088 valid 0.8305372595787048
EPOCH 134:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924187183380127 3.9579274654388428 41.3716926574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915242910385132 2.897210121154785 30.763626098632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928905487060547 3.0440244674682617 32.23313522338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926915884017944 2.6083991527557373 27.876684188842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923434972763062 3.2460756301879883 34.25309753417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932928800582886 3.374579429626465 35.53908920288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79220712184906 3.504429817199707 36.83650207519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921279668807983 3.452409267425537 36.31622314453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919586896896362 3.984788179397583 41.63983917236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919386625289917 3.409975290298462 35.891693115234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793016791343689 3.8648438453674316 40.44145584106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925233840942383 3.0883517265319824 32.67604064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933214902877808 3.2680418491363525 34.47373962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927972078323364 3.5430026054382324 37.22282409667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79092276096344 3.97849702835083 41.57589340209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929508686065674 3.9712862968444824 41.50581359863281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928913831710815 3.4343132972717285 36.136024475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927806377410889 3.170980930328369 33.50259017944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792611837387085 3.6015939712524414 37.80854797363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 3.999764919281006 41.789276123046875
  batch 20 loss: 1.7916241884231567, 3.999764919281006, 41.789276123046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927707433700562 3.254757881164551 34.34034729003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926504611968994 3.2389750480651855 34.182403564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924275398254395 3.0697975158691406 32.49040222167969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929720878601074 3.398451328277588 35.77748489379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926459312438965 3.610765218734741 37.90029525756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792922854423523 3.5022799968719482 36.81572341918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793434500694275 3.6381478309631348 38.17491149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79217529296875 3.7779951095581055 39.57212829589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7940342426300049 3.5574193000793457 37.36822509765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914959192276 3.8400285243988037 40.19178009033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929813861846924 3.1068289279937744 32.861270904541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914206981658936 3.3546571731567383 35.337989807128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928173542022705 3.460043430328369 36.39324951171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929975986480713 3.5606868267059326 37.39986801147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929593324661255 3.328040599822998 35.073368072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930341958999634 3.6618223190307617 38.411258697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929017543792725 3.5753793716430664 37.54669189453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915958166122437 3.1259260177612305 33.05085754394531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921844720840454 3.514967679977417 36.94186019897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916395664215088 3.332200527191162 35.1136474609375
  batch 40 loss: 1.7916395664215088, 3.332200527191162, 35.1136474609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924000024795532 2.811568021774292 29.9080810546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926652431488037 3.411710023880005 35.90976333618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 3.129028797149658 33.08264923095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 3.460864305496216 36.40029525756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792866826057434 3.2004573345184326 33.79743957519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924420833587646 3.5673396587371826 37.46583938598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922149896621704 3.11214017868042 32.91361618041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919988632202148 4.191506862640381 43.707069396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916806936264038 2.9546875953674316 31.33855628967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922937870025635 3.736412286758423 39.15641784667969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909345626831055 3.6918997764587402 38.70993423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792698860168457 2.9107887744903564 30.900585174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924240827560425 2.773437261581421 29.526796340942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929531335830688 3.399075984954834 35.783714294433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792062759399414 3.504067897796631 36.832740783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929970026016235 3.788478136062622 39.67777633666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793100118637085 3.1822118759155273 33.61521911621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793104887008667 2.9217846393585205 31.010950088500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793710708618164 3.529496908187866 37.08867645263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922914028167725 2.988457679748535 31.676868438720703
  batch 60 loss: 1.7922914028167725, 2.988457679748535, 31.676868438720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793127417564392 3.4205658435821533 35.99878692626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79169762134552 3.186443328857422 33.656131744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927800416946411 2.7253386974334717 29.046167373657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923855781555176 3.1667628288269043 33.46001434326172
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930622100830078 2.8975229263305664 30.768291473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919398546218872 3.005993127822876 31.851871490478516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918035984039307 3.341291904449463 35.2047233581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791602611541748 2.869640588760376 30.488008499145508
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919671535491943 2.91689133644104 30.960880279541016
Total LOSS train 35.634306658231296 valid 32.126370906829834
CE LOSS train 1.79250472508944 valid 0.4479917883872986
Contrastive LOSS train 3.384180208352896 valid 0.72922283411026
EPOCH 135:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924388647079468 2.9252841472625732 31.04528045654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915834188461304 3.2155160903930664 33.94674301147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928743362426758 3.3968310356140137 35.76118469238281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927210330963135 3.981372594833374 41.6064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922121286392212 3.8426928520202637 40.21914291381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932488918304443 3.517025947570801 36.963504791259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919650077819824 3.353153944015503 35.32350540161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921494245529175 3.1754560470581055 33.54671096801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791943073272705 3.432933807373047 36.121280670166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919375896453857 3.4756381511688232 36.54832077026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79313063621521 2.9742300510406494 31.535430908203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926620244979858 3.220902919769287 34.00169372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935105562210083 3.849804639816284 40.29155731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79310941696167 3.3934082984924316 35.72719192504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909748554229736 3.193883180618286 33.72980499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930903434753418 3.055711030960083 32.35020065307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929731607437134 2.9375815391540527 31.16878890991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792715311050415 3.7923800945281982 39.71651840209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925631999969482 3.7449488639831543 39.2420539855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915561199188232 3.4585373401641846 36.376930236816406
  batch 20 loss: 1.7915561199188232, 3.4585373401641846, 36.376930236816406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928812503814697 3.0000669956207275 31.793550491333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928521633148193 3.9605531692504883 41.398380279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926195859909058 3.0103049278259277 31.89566993713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931047677993774 2.8032944202423096 29.826047897338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926392555236816 3.073373317718506 32.52637481689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927989959716797 2.9760854244232178 31.553653717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934041023254395 2.5781641006469727 27.575044631958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921717166900635 2.4686784744262695 26.47895622253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793929100036621 3.692748785018921 38.72141647338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915380001068115 3.5070605278015137 36.862144470214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929359674453735 3.503793478012085 36.830867767333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791305422782898 3.6108317375183105 37.89962387084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792769193649292 3.728297233581543 39.07574462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930262088775635 3.990809202194214 41.70111846923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928396463394165 3.572282552719116 37.515663146972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930136919021606 3.078747034072876 32.580482482910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928714752197266 3.5649309158325195 37.442176818847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917011976242065 3.04221248626709 32.21382522583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923802137374878 3.254424571990967 34.33662796020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919033765792847 3.5468356609344482 37.26026153564453
  batch 40 loss: 1.7919033765792847, 3.5468356609344482, 37.26026153564453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924786806106567 3.0402140617370605 32.19462203979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927476167678833 2.916137218475342 30.954120635986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923189401626587 3.0097622871398926 31.88994026184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791645884513855 3.8501150608062744 40.29279708862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792841911315918 3.243757724761963 34.23041915893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924132347106934 3.468459367752075 36.47700500488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922542095184326 2.9239423274993896 31.03167724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919737100601196 3.9374160766601562 41.166133880615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791587471961975 3.813652753829956 39.92811584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922234535217285 4.100159645080566 42.793819427490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910321950912476 3.84438419342041 40.23487091064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927570343017578 4.070398807525635 42.496742248535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926928997039795 3.4404664039611816 36.197357177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79301917552948 3.41340970993042 35.92711639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792008399963379 2.952409505844116 31.316104888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927756309509277 3.242570638656616 34.21847915649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931891679763794 3.5414960384368896 37.20814895629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793136477470398 3.329909324645996 35.09223175048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793589472770691 3.493311643600464 36.726707458496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922258377075195 3.5966174602508545 37.758399963378906
  batch 60 loss: 1.7922258377075195, 3.5966174602508545, 37.758399963378906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793095350265503 3.523465394973755 37.027748107910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915656566619873 3.476698160171509 36.55854797363281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929635047912598 2.7836709022521973 29.629671096801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925268411636353 3.1642086505889893 33.43461227416992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793247938156128 3.328575611114502 35.079002380371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791825771331787 2.94427227973938 31.234548568725586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791717529296875 3.059149742126465 32.383216857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79146409034729 2.9191925525665283 30.983388900756836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917935848236084 2.859567880630493 30.38747215270996
Total LOSS train 35.547297110924355 valid 31.247156620025635
CE LOSS train 1.7925285449394812 valid 0.4479483962059021
Contrastive LOSS train 3.375476870170006 valid 0.7148919701576233
EPOCH 136:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923030853271484 2.907106876373291 30.863370895385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914761304855347 3.054412603378296 32.335601806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928096055984497 3.132892370223999 33.121734619140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925764322280884 3.046351909637451 32.25609588623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 3.2861666679382324 34.65407943725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931740283966064 3.0561015605926514 32.354190826416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792066216468811 3.030803680419922 32.100101470947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919926643371582 3.546644449234009 37.25843811035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918009757995605 2.7766520977020264 29.55832290649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918661832809448 2.62943696975708 28.08623504638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930313348770142 2.714873790740967 28.941770553588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926102876663208 3.103663206100464 32.82924270629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935051918029785 2.8637382984161377 30.43088722229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930362224578857 3.7741570472717285 39.53460693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909456491470337 3.890533924102783 40.696285247802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930643558502197 3.426032543182373 36.05339050292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928880453109741 3.0268452167510986 32.06134033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927258014678955 3.68803334236145 38.673057556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925792932510376 3.7043774127960205 38.83635330200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916316986083984 3.280979633331299 34.60142517089844
  batch 20 loss: 1.7916316986083984, 3.280979633331299, 34.60142517089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792805552482605 2.8353190422058105 30.14599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929075956344604 2.922585964202881 31.018768310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927147150039673 3.6625022888183594 38.4177360534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932060956954956 3.0936899185180664 32.730106353759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792750597000122 2.937075614929199 31.16350746154785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792953610420227 3.2637109756469727 34.43006134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933340072631836 3.040541172027588 32.19874572753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919999361038208 3.4719111919403076 36.511112213134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793826937675476 2.7524595260620117 29.318422317504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791319489479065 3.1337990760803223 33.129310607910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928094863891602 2.8550634384155273 30.34344482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913461923599243 2.993150472640991 31.722850799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928433418273926 3.1754934787750244 33.54777908325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930574417114258 3.0290565490722656 32.083621978759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929104566574097 3.251034736633301 34.30325698852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930917739868164 3.046294689178467 32.256038665771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928932905197144 2.8928439617156982 30.721332550048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 3.0415596961975098 32.20732498168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923579216003418 2.8434805870056152 30.22716522216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918825149536133 3.5193138122558594 36.98501968383789
  batch 40 loss: 1.7918825149536133, 3.5193138122558594, 36.98501968383789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 3.18615984916687 33.65397262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 3.0870463848114014 32.6630744934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921823263168335 3.1907708644866943 33.69989013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915008068084717 3.697218418121338 38.7636833190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927719354629517 2.9932730197906494 31.725502014160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924861907958984 3.04465651512146 32.239051818847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792309284210205 3.278367280960083 34.57598114013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920622825622559 3.6507208347320557 38.29927062988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916016578674316 3.3429365158081055 35.220970153808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922041416168213 2.9065301418304443 30.857505798339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910051345825195 3.146355628967285 33.25456237792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927356958389282 3.134780168533325 33.14053726196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925662994384766 3.4036178588867188 35.82874298095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792923927307129 3.083294630050659 32.62586975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919535636901855 3.372545003890991 35.51740264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927883863449097 2.872894287109375 30.521730422973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930785417556763 2.8537211418151855 30.330291748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79304039478302 3.091860771179199 32.711647033691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936007976531982 3.620666742324829 38.00027084350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923798561096191 3.729062795639038 39.0830078125
  batch 60 loss: 1.7923798561096191, 3.729062795639038, 39.0830078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931910753250122 3.3381447792053223 35.17463684082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791691780090332 3.617043972015381 37.96213150024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928048372268677 3.4676601886749268 36.46940612792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923755645751953 2.8721673488616943 30.514049530029297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930846214294434 2.6799001693725586 28.592086791992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792022943496704 3.2484257221221924 34.27627944946289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791893482208252 2.736212968826294 29.154022216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916069030761719 3.368041753768921 35.472023010253906
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918905019760132 3.11081862449646 32.90007781982422
Total LOSS train 33.54051396296575 valid 32.95060062408447
CE LOSS train 1.7925009250640869 valid 0.4479726254940033
Contrastive LOSS train 3.1748013092921332 valid 0.777704656124115
EPOCH 137:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372703552246 3.742600917816162 39.2183837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915456295013428 3.657912492752075 38.370670318603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792820930480957 3.235797882080078 34.15079879760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926130294799805 3.842811107635498 40.220726013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792278528213501 3.1705751419067383 33.49803161621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932261228561401 3.692826747894287 38.721492767333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920434474945068 3.992243766784668 41.714481353759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920697927474976 2.9525630474090576 31.31770133972168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917907238006592 3.2572991847991943 34.36478042602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791967511177063 3.297837972640991 34.770347595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793021559715271 3.276676654815674 34.55978775024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925746440887451 3.4672341346740723 36.46491622924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934406995773315 3.0257649421691895 32.051090240478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928766012191772 3.336002826690674 35.15290451049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908778190612793 3.0522186756134033 32.31306457519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930693626403809 3.106489658355713 32.85796356201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792884349822998 2.48612380027771 26.654123306274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928094863891602 3.3460168838500977 35.25297546386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924985885620117 3.0140039920806885 31.932537078857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915922403335571 2.5445950031280518 27.23754119873047
  batch 20 loss: 1.7915922403335571, 2.5445950031280518, 27.23754119873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928410768508911 2.8565292358398438 30.35813331604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927640676498413 3.0660815238952637 32.45357894897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792572021484375 2.973292589187622 31.525497436523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930392026901245 3.2615127563476562 34.408164978027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926158905029297 3.0311942100524902 32.10456085205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927244901657104 3.202906847000122 33.82179260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793302059173584 3.2844552993774414 34.63785171508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920809984207153 3.5886785984039307 37.67886734008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.794013500213623 2.756375312805176 29.35776710510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791438102722168 2.741290330886841 29.204341888427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792860746383667 3.0468244552612305 32.261104583740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913192510604858 3.066061496734619 32.451934814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927465438842773 2.6784963607788086 28.577709197998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929476499557495 2.847947597503662 30.272424697875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928575277328491 3.3047049045562744 34.839908599853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793062686920166 2.901282548904419 30.80588722229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928884029388428 3.6550498008728027 38.343387603759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917273044586182 3.179121255874634 33.58293914794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 2.9235196113586426 31.027559280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 3.093592882156372 32.72776794433594
  batch 40 loss: 1.7918387651443481, 3.093592882156372, 32.72776794433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923310995101929 3.3771650791168213 35.56398391723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925947904586792 2.759884834289551 29.391443252563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922722101211548 2.488306760787964 26.67534065246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916104793548584 2.6627652645111084 28.41926383972168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928375005722046 2.882154703140259 30.614383697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480707168579 2.759686231613159 29.38934326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79225492477417 2.68583083152771 28.650564193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920187711715698 2.803431272506714 29.826332092285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914988994598389 3.0286865234375 32.078365325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921638488769531 3.0535082817077637 32.327247619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909690141677856 3.0365803241729736 32.15677261352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926825284957886 3.7912580966949463 39.705265045166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924987077713013 2.9666972160339355 31.45947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792887806892395 3.1869022846221924 33.66191101074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919721603393555 2.9698646068573 31.490619659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928199768066406 2.6102561950683594 27.895381927490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931607961654663 3.5453267097473145 37.24642562866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930991649627686 2.9855704307556152 31.6488037109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793583869934082 3.5120458602905273 36.91404342651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923192977905273 2.500023365020752 26.792552947998047
  batch 60 loss: 1.7923192977905273, 2.500023365020752, 26.792552947998047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931299209594727 3.010892152786255 31.902050018310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916723489761353 3.4154551029205322 35.946224212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928379774093628 2.9011647701263428 30.804485321044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923645973205566 3.281853199005127 34.610897064208984
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7929959297180176 2.4160525798797607 25.953521728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919209003448486 3.4079976081848145 35.87189483642578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79178786277771 3.245810031890869 34.24988555908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915171384811401 2.8632566928863525 30.424083709716797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918020486831665 2.862957239151001 30.42137336730957
Total LOSS train 32.806002954336314 valid 32.741809368133545
CE LOSS train 1.7924836195432223 valid 0.4479505121707916
Contrastive LOSS train 3.1013519250429593 valid 0.7157393097877502
EPOCH 138:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922754287719727 2.950805187225342 31.30032730102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914438247680664 2.6209113597869873 28.00055694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927554845809937 2.6825168132781982 28.617923736572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926579713821411 3.0094761848449707 31.887418746948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923465967178345 2.8178136348724365 29.970483779907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932215929031372 2.852485418319702 30.318077087402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.3997962474823 25.789997100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791996955871582 2.8005285263061523 29.797283172607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791919469833374 2.5339272022247314 27.13119125366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920318841934204 2.6906282901763916 28.698314666748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931164503097534 3.1291539669036865 33.08465576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926480770111084 3.141174077987671 33.20438766479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934272289276123 3.589486598968506 37.68829345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929301261901855 2.998466730117798 31.777597427368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908905744552612 3.1557724475860596 33.34861373901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930490970611572 3.6362063884735107 38.155113220214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928670644760132 3.220238208770752 33.99524688720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926217317581177 3.549708366394043 37.28970718383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925083637237549 3.9643354415893555 41.43586349487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791702389717102 3.419287919998169 35.984580993652344
  batch 20 loss: 1.791702389717102, 3.419287919998169, 35.984580993652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930340766906738 3.444969654083252 36.24272918701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925969362258911 3.5349414348602295 37.14201354980469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924835681915283 2.6014351844787598 27.806835174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793030858039856 3.149960517883301 33.29263687133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927498817443848 3.3648555278778076 35.441307067871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929974794387817 3.864288091659546 40.43587875366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934672832489014 3.6963706016540527 38.75717544555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792075753211975 3.831531524658203 40.107391357421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939441204071045 3.8929190635681152 40.72313690185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913175821304321 3.67360782623291 38.52739334106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927364110946655 3.374962568283081 35.542362213134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913286685943604 3.437854528427124 36.16987228393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926182746887207 3.4116921424865723 35.90953826904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793006420135498 3.617875337600708 37.97175979614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929915189743042 3.393735647201538 35.7303466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932296991348267 3.5624945163726807 37.418174743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931450605392456 3.2963249683380127 34.75639343261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919201850891113 3.3889822959899902 35.68174362182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 3.36545729637146 35.44710922241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920448780059814 3.833021402359009 40.12226104736328
  batch 40 loss: 1.7920448780059814, 3.833021402359009, 40.12226104736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923827171325684 3.3931326866149902 35.72370910644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926009893417358 3.9461264610290527 41.253868103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920644283294678 3.7044389247894287 38.83645248413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913845777511597 3.2956838607788086 34.748226165771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792562484741211 3.471259593963623 36.505157470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924562692642212 3.467857837677002 36.47103500366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921783924102783 3.9053261280059814 40.84543991088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918630838394165 4.131750583648682 43.10936737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791771411895752 3.894993782043457 40.74170684814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921122312545776 3.618635892868042 37.97846984863281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791028618812561 3.901700735092163 40.80803298950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792731761932373 3.920884132385254 41.00157165527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926028966903687 3.5622358322143555 37.41496276855469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929785251617432 3.4865529537200928 36.65850830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920653820037842 3.5963423252105713 37.755489349365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792837142944336 4.294363021850586 44.73646545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932617664337158 4.28164005279541 44.60966110229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930101156234741 3.88803768157959 40.67338943481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934015989303589 3.729078769683838 39.08418655395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922075986862183 3.1374082565307617 33.166290283203125
  batch 60 loss: 1.7922075986862183, 3.1374082565307617, 33.166290283203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79306161403656 3.3199286460876465 34.992347717285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791562557220459 3.4958975315093994 36.75053787231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792803406715393 3.6190359592437744 37.9831657409668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925057411193848 3.6285977363586426 38.07848358154297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7932671308517456 2.792290687561035 29.71617317199707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921698093414307 3.3981900215148926 35.774070739746094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921262979507446 2.9335596561431885 31.127721786499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918012142181396 3.086362600326538 32.655426025390625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7920817136764526 2.780266284942627 29.594743728637695
Total LOSS train 36.005298321063705 valid 32.28799057006836
CE LOSS train 1.7924985005305363 valid 0.44802042841911316
Contrastive LOSS train 3.4212799879220817 valid 0.6950665712356567
EPOCH 139:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925236225128174 3.0302491188049316 32.09501266479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916464805603027 3.0011706352233887 31.803354263305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928763628005981 3.629148244857788 38.08435821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926377058029175 3.7237725257873535 39.03036117553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922335863113403 3.2448196411132812 34.24042892456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79319167137146 2.7703003883361816 29.49619483947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921638488769531 3.0412700176239014 32.204864501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792111873626709 3.1580801010131836 33.3729133605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791805624961853 3.44946551322937 36.286460876464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919373512268066 3.1333303451538086 33.125240325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792917251586914 3.4798784255981445 36.591697692871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523741722107 3.775134325027466 39.54386520385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793458342552185 3.1596598625183105 33.39005661010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929502725601196 2.942521095275879 31.21816062927246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908941507339478 3.110553741455078 32.89643096923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930296659469604 3.5199358463287354 36.99238967895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927926778793335 3.414325475692749 35.9360466003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927155494689941 2.8222341537475586 30.015056610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792453408241272 3.5606439113616943 37.39889144897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916256189346313 2.9707069396972656 31.498695373535156
  batch 20 loss: 1.7916256189346313, 2.9707069396972656, 31.498695373535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928985357284546 2.729032278060913 29.083221435546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927114963531494 3.4421472549438477 36.21418380737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925693988800049 2.7537472248077393 29.330041885375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79305899143219 2.5905961990356445 27.699020385742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926819324493408 3.163383722305298 33.426517486572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792757272720337 3.117732286453247 32.9700813293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933343648910522 3.534026861190796 37.133602142333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792055606842041 3.1011202335357666 32.80325698852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939265966415405 2.688737630844116 28.681303024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913811206817627 2.969053030014038 31.48191261291504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928122282028198 3.4698808193206787 36.49161911010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913504838943481 3.2834761142730713 34.62611389160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 2.974853754043579 31.54135513305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929855585098267 3.0912580490112305 32.70556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928390502929688 3.416111707687378 35.953956604003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930153608322144 3.050666093826294 32.29967498779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928298711776733 2.853273630142212 30.325565338134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916847467422485 2.989173173904419 31.68341636657715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79234778881073 3.4211347103118896 36.003692626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918680906295776 3.308009624481201 34.87196350097656
  batch 40 loss: 1.7918680906295776, 3.308009624481201, 34.87196350097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923895120620728 3.034301280975342 32.13540267944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926054000854492 2.5866949558258057 27.65955352783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922616004943848 2.933288335800171 31.125144958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916033267974854 3.023449182510376 32.02609634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927767038345337 2.6258914470672607 28.05169105529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924299240112305 3.1710383892059326 33.50281524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922693490982056 2.3520872592926025 25.313140869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920328378677368 3.012796640396118 31.919998168945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915563583374023 3.2677719593048096 34.469276428222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922030687332153 3.03104305267334 32.10263442993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791032075881958 3.4643237590789795 36.434268951416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926836013793945 3.333798885345459 35.130672454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462944984436 2.53989839553833 27.19144630432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927874326705933 2.7604405879974365 29.397193908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919487953186035 2.8643593788146973 30.4355411529541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928091287612915 2.9575133323669434 31.367942810058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931468486785889 2.89328670501709 30.72601318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930748462677002 2.992795467376709 31.721031188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793559193611145 3.278698682785034 34.58054733276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923411130905151 3.2124476432800293 33.91681671142578
  batch 60 loss: 1.7923411130905151, 3.2124476432800293, 33.91681671142578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793113112449646 2.678436517715454 28.577478408813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916362285614014 2.9111013412475586 30.90264892578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927627563476562 3.207892417907715 33.87168884277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922759056091309 2.682492733001709 28.617204666137695
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793026089668274 3.1012210845947266 32.80523681640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920023202896118 2.901900291442871 30.811004638671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918833494186401 2.9146320819854736 30.938203811645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915799617767334 2.904712677001953 30.838706970214844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918504476547241 2.589024782180786 27.682098388671875
Total LOSS train 32.685046621469354 valid 30.067503452301025
CE LOSS train 1.7924800414305466 valid 0.44796261191368103
Contrastive LOSS train 3.089256679094755 valid 0.6472561955451965
EPOCH 140:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923424243927002 2.9457740783691406 31.250083923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791512370109558 3.3018109798431396 34.80961990356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926461696624756 2.9260473251342773 31.053119659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925561666488647 3.275963068008423 34.552188873291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923002243041992 2.9318575859069824 31.110877990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932400703430176 2.4970920085906982 26.76416015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920451164245605 3.5199785232543945 36.99182891845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921370267868042 2.6948652267456055 28.74078941345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918869256973267 2.8691093921661377 30.482980728149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919989824295044 2.4475595951080322 26.267595291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793041706085205 2.54103684425354 27.20340919494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792564868927002 2.6557388305664062 28.349952697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793334722518921 2.432450771331787 26.117843627929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929069995880127 2.7282395362854004 29.075302124023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909162044525146 2.9176862239837646 30.9677791595459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930078506469727 3.43892765045166 36.182281494140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928297519683838 3.2516682147979736 34.309513092041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792715072631836 3.2790424823760986 34.58313751220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792496919631958 3.4644691944122314 36.43718719482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791595220565796 3.561250925064087 37.40410614013672
  batch 20 loss: 1.791595220565796, 3.561250925064087, 37.40410614013672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928701639175415 2.3355050086975098 25.147918701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792785406112671 3.281630754470825 34.609092712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792633056640625 2.3176066875457764 24.968700408935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931708097457886 2.142772674560547 23.220897674560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927658557891846 3.6150600910186768 37.94336700439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928153276443481 3.3758225440979004 35.55104064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932955026626587 3.0977227687835693 32.77052307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792000412940979 3.191746711730957 33.709468841552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938342094421387 2.9242048263549805 31.0358829498291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791407585144043 2.8891704082489014 30.68311309814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927827835083008 3.2458083629608154 34.2508659362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913458347320557 3.707746982574463 38.86881637573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79275643825531 3.4531009197235107 36.32376480102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929552793502808 2.9563262462615967 31.356218338012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 3.6102468967437744 37.895286560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929794788360596 3.1753485202789307 33.54646301269531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928344011306763 3.1708855628967285 33.50168991088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 3.5708765983581543 37.500450134277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923040390014648 3.151054620742798 33.30284881591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918097972869873 3.352848529815674 35.32029342651367
  batch 40 loss: 1.7918097972869873, 3.352848529815674, 35.32029342651367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923775911331177 3.2146072387695312 33.93844985961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792624592781067 3.5965752601623535 37.75837707519531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922766208648682 3.819880247116089 39.99108123779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916204929351807 3.4136805534362793 35.928428649902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927240133285522 3.4229912757873535 36.02263641357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792360782623291 2.9679503440856934 31.471864700317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921783924102783 3.1346611976623535 33.138790130615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919530868530273 3.7823424339294434 39.615379333496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791571021080017 3.261714220046997 34.408714294433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921833992004395 3.7481675148010254 39.27385711669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910077571868896 3.8146207332611084 39.93721389770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926628589630127 3.1426773071289062 33.21943664550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924054861068726 3.5460755825042725 37.25315856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928800582885742 3.4543488025665283 36.336368560791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920070886611938 3.41650390625 35.95704650878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927968502044678 3.7786428928375244 39.579227447509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930470705032349 3.4900686740875244 36.6937370300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792942762374878 3.7497293949127197 39.29023742675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934563159942627 3.019603967666626 31.9894962310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921812534332275 3.021921396255493 32.01139450073242
  batch 60 loss: 1.7921812534332275, 3.021921396255493, 32.01139450073242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930845022201538 3.2134463787078857 33.927547454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915769815444946 3.805947780609131 39.85105514526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927496433258057 3.1997854709625244 33.79060363769531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79236900806427 3.583857297897339 37.630943298339844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.793075442314148 2.385589361190796 25.648969650268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919706106185913 3.275595188140869 34.54792022705078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918496131896973 3.332545518875122 35.117305755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79159414768219 3.6602368354797363 38.39396286010742
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791917324066162 3.2384073734283447 34.17599105834961
Total LOSS train 33.67422268207257 valid 35.55879497528076
CE LOSS train 1.7924620775076059 valid 0.4479793310165405
Contrastive LOSS train 3.188176052386944 valid 0.8096018433570862
EPOCH 141:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923396825790405 3.2470362186431885 34.26270294189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915153503417969 3.3021671772003174 34.81318664550781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927829027175903 3.3152592182159424 34.94537353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925715446472168 3.148663282394409 33.279205322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922186851501465 3.2319366931915283 34.1115837097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930033206939697 3.276024103164673 34.553245544433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920210361480713 3.441457986831665 36.20660400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792043685913086 2.9552571773529053 31.344615936279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917925119400024 3.7551662921905518 39.34345626831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791957139968872 3.624506950378418 38.03702926635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792870044708252 2.9868967533111572 31.661836624145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924277782440186 3.3138954639434814 34.93138122558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933155298233032 3.0168697834014893 31.962013244628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928694486618042 3.0931997299194336 32.7248649597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7908661365509033 3.3053319454193115 34.84418487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930126190185547 3.249246120452881 34.28547668457031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928755283355713 3.2354652881622314 34.14752960205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928745746612549 2.5571112632751465 27.363988876342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792643427848816 2.3496346473693848 25.28898811340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916104793548584 2.325774669647217 25.049358367919922
  batch 20 loss: 1.7916104793548584, 2.325774669647217, 25.049358367919922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926826477050781 3.193065643310547 33.72333908081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926334142684937 3.9314966201782227 41.10759735107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924774885177612 3.1841635704040527 33.63411331176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792949914932251 2.6015474796295166 27.80842399597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925649881362915 3.6534643173217773 38.32720947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927852869033813 3.0746989250183105 32.53977584838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932666540145874 2.984722375869751 31.64048957824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919981479644775 3.6584649085998535 38.37664794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938464879989624 3.602536201477051 37.81920623779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913775444030762 3.1028547286987305 32.819923400878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927722930908203 3.146880626678467 33.26158142089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912966012954712 3.087444305419922 32.665740966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925903797149658 2.617110252380371 27.963693618774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792833685874939 3.2882094383239746 34.674930572509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928717136383057 3.037257194519043 32.165443420410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930082082748413 3.4039266109466553 35.832271575927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929097414016724 3.354149341583252 35.33440399169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916334867477417 3.364332914352417 35.43496322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921890020370483 3.3050336837768555 34.842529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918099164962769 3.333805561065674 35.12986373901367
  batch 40 loss: 1.7918099164962769, 3.333805561065674, 35.12986373901367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922489643096924 2.5156140327453613 26.948389053344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792547583580017 3.338793992996216 35.18048858642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792250394821167 3.111011266708374 32.90236282348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914990186691284 2.789743661880493 29.688934326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928247451782227 3.77585768699646 39.55139923095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923845052719116 3.3129115104675293 34.92150115966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921653985977173 2.621490478515625 28.007070541381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919663190841675 3.148669719696045 33.278663635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916432619094849 3.500765800476074 36.7993049621582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792150616645813 3.261363983154297 34.405792236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910295724868774 2.6855859756469727 28.646888732910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79256272315979 3.012366533279419 31.916227340698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792270541191101 3.1178812980651855 32.97108459472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792871356010437 3.3203630447387695 34.996498107910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917468547821045 3.9062278270721436 40.854026794433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925657033920288 3.8933265209198 40.725830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930402755737305 4.045841693878174 42.25145721435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930457592010498 3.5112690925598145 36.90573501586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793491005897522 3.4182944297790527 35.976436614990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792283296585083 3.574662685394287 37.538909912109375
  batch 60 loss: 1.792283296585083, 3.574662685394287, 37.538909912109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793126106262207 3.4414479732513428 36.207603454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916557788848877 3.2915453910827637 34.70711135864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927062511444092 3.4114954471588135 35.90766143798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922896146774292 3.2174506187438965 33.966796875
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7929978370666504 2.6850996017456055 28.643993377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918672561645508 3.672870635986328 38.520572662353516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917821407318115 3.6772727966308594 38.564510345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915226221084595 3.780916690826416 39.60068893432617
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918193340301514 2.866223096847534 30.454050064086914
Total LOSS train 34.03321448106032 valid 36.7849555015564
CE LOSS train 1.7924226540785568 valid 0.44795483350753784
Contrastive LOSS train 3.224079165091881 valid 0.7165557742118835
EPOCH 142:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922813892364502 3.513613700866699 36.92842102050781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914507389068604 3.4159326553344727 35.950775146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927311658859253 2.9259729385375977 31.052459716796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 3.0772156715393066 32.5646858215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922554016113281 3.1475303173065186 33.26755905151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793143630027771 3.234760046005249 34.140743255615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920527458190918 3.395827054977417 35.75032424926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920781373977661 3.4239227771759033 36.031307220458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918407917022705 2.9456820487976074 31.2486629486084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920029163360596 2.9325830936431885 31.117834091186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793047308921814 3.4770312309265137 36.56336212158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724609375 3.3754661083221436 35.547386169433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935484647750854 3.266742706298828 34.460975646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792994499206543 3.4183738231658936 35.97673416137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909239530563354 3.343200922012329 35.22293472290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930556535720825 3.284949779510498 34.642555236816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927876710891724 2.9426655769348145 31.21944236755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 3.229081630706787 34.08350372314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792404294013977 2.964980125427246 31.44220542907715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915658950805664 2.9035961627960205 30.827526092529297
  batch 20 loss: 1.7915658950805664, 2.9035961627960205, 30.827526092529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928451299667358 2.8606245517730713 30.399089813232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927491664886475 3.071995735168457 32.5127067565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925883531570435 2.4414725303649902 26.20731544494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930747270584106 2.955199718475342 31.345073699951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792654037475586 3.2615365982055664 34.40802001953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927486896514893 3.5585365295410156 37.37811279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932710647583008 3.2621350288391113 34.41461944580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 3.5782134532928467 37.57418441772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7939616441726685 3.310593366622925 34.89989471435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914170026779175 2.857079267501831 30.36220932006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792859435081482 3.136680841445923 33.15966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913565635681152 2.995326519012451 31.7446231842041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927113771438599 3.113121747970581 32.92393112182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928996086120605 2.7490837574005127 29.283737182617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927510738372803 3.2732772827148438 34.5255241394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930344343185425 3.19789719581604 33.77200698852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928295135498047 3.3383915424346924 35.17674255371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917375564575195 2.788621664047241 29.677955627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923845052719116 3.924057960510254 41.032962799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918972969055176 3.6816697120666504 38.60859298706055
  batch 40 loss: 1.7918972969055176, 3.6816697120666504, 38.60859298706055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923529148101807 3.027101755142212 32.06336975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925963401794434 3.3027610778808594 34.82020568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923017740249634 3.458568811416626 36.37799072265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916096448898315 3.0979015827178955 32.770626068115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927097082138062 3.213243007659912 33.925140380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 2.984992742538452 31.642292022705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792176365852356 2.879603862762451 30.58821678161621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920125722885132 2.988223075866699 31.674243927001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791728138923645 2.8392860889434814 30.184589385986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922521829605103 2.525007486343384 27.042325973510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911590337753296 2.624558210372925 28.036741256713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927768230438232 3.044555425643921 32.23833084106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924699783325195 2.6122727394104004 27.91519546508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928659915924072 3.2432243824005127 34.2251091003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919729948043823 2.9010894298553467 30.802867889404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927486896514893 2.8024842739105225 29.81759262084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79305100440979 3.255568504333496 34.34873962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930188179016113 3.069274663925171 32.48576736450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934398651123047 3.2892849445343018 34.68628692626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216181755066 2.974616050720215 31.538375854492188
  batch 60 loss: 1.792216181755066, 2.974616050720215, 31.538375854492188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930504083633423 3.5195021629333496 36.98807144165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79166579246521 3.288823127746582 34.679893493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927756309509277 3.3586885929107666 35.379661560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923243045806885 3.307084560394287 34.8631706237793
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928965091705322 2.815275192260742 29.945648193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919553518295288 3.2561194896698 34.3531494140625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918330430984497 4.052847862243652 42.3203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915623188018799 3.512779951095581 36.91936111450195
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918351888656616 3.6279590129852295 38.07142639160156
Total LOSS train 33.176720340435324 valid 37.916062355041504
CE LOSS train 1.7924686193466186 valid 0.4479587972164154
Contrastive LOSS train 3.1384251557863676 valid 0.9069897532463074
EPOCH 143:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792271614074707 3.3476474285125732 35.26874542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915018796920776 3.4135429859161377 35.92692947387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927052974700928 3.308452606201172 34.87723159790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925463914871216 3.4309792518615723 36.10233688354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922639846801758 2.9416041374206543 31.20830535888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793195366859436 3.3002538681030273 34.79573440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920682430267334 3.929964542388916 41.091712951660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920762300491333 3.420819044113159 36.000267028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916862964630127 2.870351552963257 30.495203018188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918727397918701 3.499505043029785 36.786922454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927534580230713 3.1994285583496094 33.78704071044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792375922203064 3.2375690937042236 34.168067932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931582927703857 3.2293708324432373 34.08686828613281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792765498161316 3.2225637435913086 34.01840591430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.790910005569458 3.226717233657837 34.058082580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930752038955688 3.092067241668701 32.713748931884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929903268814087 2.8079235553741455 29.872224807739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792824149131775 2.8680920600891113 30.473743438720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792559266090393 3.649254322052002 38.28510284423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916030883789062 3.421969175338745 36.011295318603516
  batch 20 loss: 1.7916030883789062, 3.421969175338745, 36.011295318603516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927401065826416 2.441605567932129 26.20879554748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702078819275 3.4469668865203857 36.26237106323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792496919631958 3.473600149154663 36.52849578857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928775548934937 2.949787139892578 31.290748596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792578101158142 3.3575563430786133 35.368141174316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927340269088745 2.8265140056610107 30.05787467956543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933355569839478 3.356462240219116 35.35795593261719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792244791984558 3.354200601577759 35.334251403808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793941855430603 3.0358073711395264 32.152015686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791383147239685 3.026547431945801 32.05685806274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927367687225342 3.302373170852661 34.81646728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913100719451904 3.0751259326934814 32.54256820678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926236391067505 2.939486503601074 31.187488555908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928677797317505 3.1531012058258057 33.32387924194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792864203453064 2.9589602947235107 31.38246726989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793007731437683 2.6627185344696045 28.42019271850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928192615509033 3.4124081134796143 35.916900634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791733741760254 3.4015376567840576 35.80710983276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923465967178345 3.1955208778381348 33.747554779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918380498886108 2.925400972366333 31.045846939086914
  batch 40 loss: 1.7918380498886108, 2.925400972366333, 31.045846939086914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922606468200684 2.8109240531921387 29.90150260925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924752235412598 2.355140447616577 25.34387969970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921817302703857 3.065188407897949 32.44406509399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915337085723877 2.977421522140503 31.565750122070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792720913887024 3.3482954502105713 35.27567672729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924156188964844 2.9151968955993652 30.944385528564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922000885009766 3.044344425201416 32.23564147949219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919764518737793 3.7581050395965576 39.37302780151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791538953781128 2.9566824436187744 31.358362197875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921380996704102 3.2543904781341553 34.33604049682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910077571868896 2.63381290435791 28.12913703918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925851345062256 2.7973403930664062 29.765989303588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924511432647705 3.0940327644348145 32.7327766418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928284406661987 3.472548246383667 36.518310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919504642486572 3.4832417964935303 36.624366760253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927186489105225 3.1942481994628906 33.735198974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793028712272644 2.8535077571868896 30.328105926513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930021286010742 3.126420497894287 33.05720901489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793485403060913 3.1453070640563965 33.24655532836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922999858856201 3.759995937347412 39.39226150512695
  batch 60 loss: 1.7922999858856201, 3.759995937347412, 39.39226150512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930564880371094 3.82858943939209 40.07895278930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916311025619507 3.2925004959106445 34.71663284301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927085161209106 3.1177899837493896 32.97060775756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923527956008911 3.193530559539795 33.7276611328125
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7929922342300415 2.8011341094970703 29.804332733154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919543981552124 2.960833787918091 31.400291442871094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791811466217041 2.6370840072631836 28.16265106201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791596531867981 2.4274232387542725 26.06583023071289
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791884422302246 2.259697437286377 24.388858795166016
Total LOSS train 33.483728966346156 valid 27.50440788269043
CE LOSS train 1.7924296250710121 valid 0.4479711055755615
Contrastive LOSS train 3.1691299475156343 valid 0.5649243593215942
EPOCH 144:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923380136489868 2.9967379570007324 31.75971794128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915246486663818 3.3663549423217773 35.455074310302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792761206626892 2.3974111080169678 25.76687240600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925843000411987 2.906913995742798 30.861722946166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922285795211792 3.389364719390869 35.685874938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931127548217773 2.86496639251709 30.44277572631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920128107070923 2.5498552322387695 27.290565490722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921366691589355 3.14558482170105 33.24798583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918353080749512 3.3160650730133057 34.95248794555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919403314590454 3.4322738647460938 36.11467742919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929227352142334 3.2542030811309814 34.33495330810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925409078598022 3.2598907947540283 34.391448974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933377027511597 3.3273489475250244 35.066829681396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928701639175415 2.812800168991089 29.92087173461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909166812896729 2.9517886638641357 31.30880355834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929753065109253 3.4165799617767334 35.95877456665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792786717414856 3.5982484817504883 37.7752685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927250862121582 3.2838945388793945 34.63166809082031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924288511276245 3.2963685989379883 34.75611114501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916098833084106 3.237374782562256 34.16535949707031
  batch 20 loss: 1.7916098833084106, 3.237374782562256, 34.16535949707031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792844533920288 2.677053928375244 28.563383102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927058935165405 3.7626216411590576 39.418922424316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925316095352173 2.7999391555786133 29.79192352294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929890155792236 3.664614200592041 38.43912887573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926822900772095 3.5654637813568115 37.44731903076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928165197372437 3.640176296234131 38.194580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932562828063965 3.597546339035034 37.76871871948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920629978179932 3.5991709232330322 37.783775329589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938779592514038 2.877795934677124 30.57183837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79142427444458 2.7693583965301514 29.485008239746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928239107131958 2.9702601432800293 31.495426177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914605140686035 3.074092149734497 32.53238296508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792734146118164 2.9751806259155273 31.544540405273438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929457426071167 3.128040075302124 33.07334518432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928152084350586 2.6615803241729736 28.408618927001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930235862731934 3.2767903804779053 34.5609245300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929859161376953 2.9875142574310303 31.668128967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791772723197937 3.8204164505004883 39.995933532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922643423080444 3.348534345626831 35.27760696411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 3.5883116722106934 37.67498779296875
  batch 40 loss: 1.7918701171875, 3.5883116722106934, 37.67498779296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922700643539429 3.5636484622955322 37.42875671386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925251722335815 3.2385523319244385 34.17805099487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921862602233887 3.070485830307007 32.49704360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915908098220825 3.4308078289031982 36.09967041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927067279815674 3.292213201522827 34.71483612060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923848628997803 3.215327501296997 33.94565963745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922477722167969 2.7940545082092285 29.732791900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919942140579224 2.9438600540161133 31.230594635009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916209697723389 3.256704807281494 34.35866928100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921593189239502 2.9524362087249756 31.3165225982666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910549640655518 2.9163053035736084 30.95410919189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926242351531982 3.346576452255249 35.25838851928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924283742904663 3.053074836730957 32.323177337646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928237915039062 2.6431725025177 28.22454833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919572591781616 3.4247162342071533 36.039119720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927207946777344 2.942805528640747 31.220775604248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929527759552002 3.276956081390381 34.56251525878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929680347442627 3.2436201572418213 34.22917175292969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933977842330933 3.185661554336548 33.6500129699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792271614074707 3.3491058349609375 35.283329010009766
  batch 60 loss: 1.792271614074707, 3.3491058349609375, 35.283329010009766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930443286895752 2.9912848472595215 31.705894470214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916858196258545 3.3435792922973633 35.22747802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927416563034058 3.215041160583496 33.943153381347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922661304473877 3.0014872550964355 31.807140350341797
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79285728931427 2.3879051208496094 25.67190933227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918893098831177 2.9894707202911377 31.686595916748047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791763424873352 2.581057548522949 27.602338790893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915029525756836 2.6523313522338867 28.314815521240234
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917739152908325 2.097398042678833 22.76575469970703
Total LOSS train 33.433656252347504 valid 27.592376232147217
CE LOSS train 1.792445496412424 valid 0.44794347882270813
Contrastive LOSS train 3.164121077610896 valid 0.5243495106697083
EPOCH 145:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922059297561646 2.281043291091919 24.602638244628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914695739746094 2.6232728958129883 28.024198532104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926596403121948 2.569060802459717 27.48326873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925347089767456 3.4130940437316895 35.9234733581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923150062561035 3.08917498588562 32.68406677246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793159008026123 3.0321877002716064 32.11503601074219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920340299606323 3.333961009979248 35.13164520263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792028546333313 2.961677312850952 31.408802032470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918035984039307 3.0104403495788574 31.896207809448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918827533721924 3.1418473720550537 33.210357666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929307222366333 2.79828143119812 29.775745391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925679683685303 3.1466119289398193 33.25868606567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933586835861206 3.179145336151123 33.58481216430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792884349822998 3.573272228240967 37.52560806274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909355163574219 2.890087127685547 30.69180679321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929688692092896 3.0121521949768066 31.91448974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792770266532898 2.5675735473632812 27.468505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927495241165161 3.076464891433716 32.55739974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923798561096191 2.7850277423858643 29.642656326293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791536808013916 3.4994425773620605 36.78596496582031
  batch 20 loss: 1.791536808013916, 3.4994425773620605, 36.78596496582031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927134037017822 2.6079704761505127 27.872417449951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926021814346313 3.16817045211792 33.474308013916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924904823303223 3.339238166809082 35.184871673583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929717302322388 3.388897657394409 35.681949615478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926156520843506 3.611286163330078 37.90547561645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792725920677185 3.1573262214660645 33.36598587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793252944946289 2.993837594985962 31.73162841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920972108840942 2.9244048595428467 31.03614616394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7938706874847412 2.778775215148926 29.581623077392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914025783538818 2.8501996994018555 30.293399810791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792715072631836 3.0832223892211914 32.62493896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913644313812256 3.068342685699463 32.474788665771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927170991897583 3.0082662105560303 31.87537956237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792899250984192 2.8737704753875732 30.53060531616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724609375 2.69439435005188 28.73666763305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928926944732666 3.2167601585388184 33.96049499511719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927402257919312 2.996173858642578 31.754478454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916947603225708 3.31298565864563 34.92155075073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923035621643066 3.393026828765869 35.722572326660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791857123374939 2.7381396293640137 29.173254013061523
  batch 40 loss: 1.791857123374939, 2.7381396293640137, 29.173254013061523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792305588722229 3.2697854042053223 34.49015808105469
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792554259300232 2.7767984867095947 29.56053924560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922091484069824 2.786515235900879 29.65736198425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916053533554077 3.3117425441741943 34.90903091430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792694091796875 3.203687906265259 33.82957458496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923729419708252 3.458895444869995 36.38132858276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792169213294983 3.4050590991973877 35.84275817871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919892072677612 3.7034010887145996 38.82600021362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915871143341064 3.4867608547210693 36.65919494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921663522720337 3.01314640045166 31.923629760742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910624742507935 3.6445765495300293 38.2368278503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925465106964111 3.1517117023468018 33.309661865234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924113273620605 3.348649501800537 35.278907775878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928019762039185 3.163245439529419 33.425254821777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791987657546997 2.7258260250091553 29.050249099731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927545309066772 3.15568470954895 33.34960174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930301427841187 3.1074297428131104 32.86732864379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930020093917847 3.7487893104553223 39.2808952331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793477177619934 3.30381178855896 34.83159255981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792271614074707 3.2906956672668457 34.69922637939453
  batch 60 loss: 1.792271614074707, 3.2906956672668457, 34.69922637939453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929999828338623 3.221801996231079 34.01102066040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916616201400757 2.8783535957336426 30.575197219848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792732834815979 3.3359453678131104 35.15218734741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923156023025513 3.4742543697357178 36.53485870361328
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792899250984192 2.8878602981567383 30.6715030670166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919483184814453 3.1268961429595947 33.060909271240234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918457984924316 3.2796285152435303 34.588130950927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569471359253 2.748375415802002 29.27532196044922
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791846752166748 2.3665525913238525 25.457372665405273
Total LOSS train 32.87602762075571 valid 30.595433712005615
CE LOSS train 1.7924220763719998 valid 0.447961688041687
Contrastive LOSS train 3.1083605546217696 valid 0.5916381478309631
EPOCH 146:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79226815700531 2.8846867084503174 30.639135360717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915501594543457 3.6126163005828857 37.9177131652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927021980285645 2.6831729412078857 28.624431610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.535144329071045 27.143999099731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922587394714355 3.2789807319641113 34.58206558227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931078672409058 3.1986260414123535 33.77936553955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920271158218384 3.299656391143799 34.78858947753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920565605163574 2.931001901626587 31.102075576782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917860746383667 2.755793571472168 29.349721908569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919172048568726 3.0690228939056396 32.482147216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792895793914795 3.3515636920928955 35.30853271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513370513916 3.3288166522979736 35.08068084716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932815551757812 3.226497173309326 34.05825424194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79286789894104 3.558985471725464 37.38272476196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7909777164459229 3.074152946472168 32.532508850097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929959297180176 3.1165153980255127 32.95814895629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927868366241455 3.4483580589294434 36.2763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678952217102 3.2523343563079834 34.31602096557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923840284347534 2.611537218093872 27.90775489807129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915626764297485 2.5270369052886963 27.061931610107422
  batch 20 loss: 1.7915626764297485, 2.5270369052886963, 27.061931610107422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792715311050415 2.5723326206207275 27.516040802001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926347255706787 3.5363929271698 37.15656280517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925001382827759 2.795696496963501 29.74946403503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929939031600952 2.913104772567749 30.924041748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792610764503479 3.074770212173462 32.540313720703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927497625350952 2.601719379425049 27.8099422454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793258547782898 2.8401832580566406 30.195091247558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792094349861145 2.292637348175049 24.718467712402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7937504053115845 3.0218684673309326 32.01243591308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914526462554932 2.7704508304595947 29.495960235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792763113975525 3.0824015140533447 32.61677932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914007902145386 3.5383617877960205 37.175018310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926418781280518 3.881927728652954 40.61191940307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929350137710571 3.539794921875 37.19088363647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927920818328857 3.8563010692596436 40.355804443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792845368385315 3.2761545181274414 34.55438995361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926958799362183 3.432584047317505 36.118534088134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917083501815796 3.5855116844177246 37.646827697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923052310943604 3.4036874771118164 35.82917785644531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918905019760132 3.4848825931549072 36.640716552734375
  batch 40 loss: 1.7918905019760132, 3.4848825931549072, 36.640716552734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923659086227417 2.9675557613372803 31.467924118041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925736904144287 3.3675272464752197 35.46784591674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922194004058838 3.7238988876342773 39.031211853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915186882019043 2.9737179279327393 31.528697967529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926183938980103 3.3376972675323486 35.16958999633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923855781555176 3.44104266166687 36.20281219482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136311531067 2.4821932315826416 26.61406898498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920608520507812 3.125231981277466 33.04438018798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917543649673462 3.4170262813568115 35.96201705932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922950983047485 2.710365056991577 28.895946502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912304401397705 2.9629406929016113 31.420637130737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927253246307373 2.9920973777770996 31.713699340820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924877405166626 3.7540407180786133 39.33289337158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927992343902588 2.742818593978882 29.220985412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919249534606934 2.9146127700805664 30.938053131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926050424575806 2.3400025367736816 25.192628860473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792927622795105 3.469533681869507 36.48826217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792952299118042 3.456202983856201 36.354984283447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934259176254272 3.86604380607605 40.45386505126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922167778015137 3.1265928745269775 33.058143615722656
  batch 60 loss: 1.7922167778015137, 3.1265928745269775, 33.058143615722656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929359674453735 2.9661285877227783 31.454221725463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916723489761353 4.3096208572387695 44.88787841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926623821258545 2.634565591812134 28.138317108154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923481464385986 3.3338358402252197 35.130706787109375
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928962707519531 2.693153142929077 28.724428176879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 3.5066018104553223 36.857879638671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917518615722656 3.0090978145599365 31.88273048400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915771007537842 3.7925186157226562 39.71676254272461
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918553352355957 3.098947286605835 32.78133010864258
Total LOSS train 33.23145754887508 valid 35.30967569351196
CE LOSS train 1.7924254013941838 valid 0.4479638338088989
Contrastive LOSS train 3.1439032261188213 valid 0.7747368216514587
EPOCH 147:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922600507736206 3.2547695636749268 34.3399543762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914797067642212 3.2782256603240967 34.57373809814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926663160324097 3.0051536560058594 31.844202041625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792518138885498 2.7011702060699463 28.80422019958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921570539474487 3.070192575454712 32.494083404541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793074607849121 3.1925055980682373 33.71813201904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920315265655518 3.3107056617736816 34.899085998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919849157333374 2.857369899749756 30.365684509277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791857123374939 3.1378159523010254 33.17001724243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866660118103 3.143779754638672 33.22966384887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928791046142578 3.5453391075134277 37.24626922607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924373149871826 3.4065728187561035 35.8581657409668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931952476501465 2.973558187484741 31.528778076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792814016342163 2.7799408435821533 29.592222213745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910093069076538 3.033271551132202 32.12372589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929314374923706 3.522365093231201 37.01658248901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928272485733032 3.4696404933929443 36.48923110961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792642593383789 3.102320432662964 32.81584930419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924937009811401 2.530181646347046 27.094310760498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916675806045532 3.3081369400024414 34.8730354309082
  batch 20 loss: 1.7916675806045532, 3.3081369400024414, 34.8730354309082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926585674285889 3.588465452194214 37.67731475830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925634384155273 2.5509369373321533 27.30193328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923786640167236 2.3142335414886475 24.934715270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928005456924438 2.3897299766540527 25.690101623535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924737930297852 2.846280813217163 30.25528335571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926356792449951 2.9537575244903564 31.330209732055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931458950042725 2.941288948059082 31.206035614013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79202401638031 3.988914966583252 41.68117141723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936687469482422 2.6048617362976074 27.842287063598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913923263549805 3.129828691482544 33.08967971801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927677631378174 3.372796058654785 35.52072525024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914085388183594 3.039659023284912 32.1879997253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926733493804932 2.423142194747925 26.02409553527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928990125656128 3.6989524364471436 38.78242492675781
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926759719848633 3.1142449378967285 32.935123443603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792784333229065 2.7465708255767822 29.25849151611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926990985870361 2.546884775161743 27.261547088623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916522026062012 3.1793601512908936 33.58525466918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792227864265442 3.2342886924743652 34.13511657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791745901107788 3.209636926651001 33.88811492919922
  batch 40 loss: 1.791745901107788, 3.209636926651001, 33.88811492919922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923412322998047 2.853816509246826 30.330507278442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925970554351807 3.78594708442688 39.652069091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921398878097534 3.5574331283569336 37.36647415161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915563583374023 3.5508737564086914 37.30029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925965785980225 2.7665908336639404 29.458505630493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792352318763733 3.2417197227478027 34.20954895019531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921528816223145 3.8980424404144287 40.77257537841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919760942459106 2.818643093109131 29.978408813476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916908264160156 3.1545093059539795 33.33678436279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922320365905762 3.442390203475952 36.21613311767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910531759262085 2.9964871406555176 31.755924224853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925785779953003 3.1854586601257324 33.64716720581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924463748931885 3.478555917739868 36.578006744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928427457809448 3.460573673248291 36.39857864379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918559312820435 3.840095281600952 40.192806243896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925512790679932 3.2412831783294678 34.20538330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792834997177124 3.5024261474609375 36.81709671020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929199934005737 3.0816197395324707 32.60911560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933881282806396 3.046215772628784 32.25554656982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922077178955078 2.9376628398895264 31.16883659362793
  batch 60 loss: 1.7922077178955078, 2.9376628398895264, 31.16883659362793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928987741470337 3.731992483139038 39.112823486328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916085720062256 4.139612674713135 43.1877326965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925642728805542 3.764007568359375 39.432640075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792155385017395 3.453991651535034 36.33207321166992
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927290201187134 2.8937158584594727 30.729887008666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791768193244934 3.323047637939453 35.02224349975586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916642427444458 3.1332993507385254 33.124656677246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791468858718872 3.3230795860290527 35.02226638793945
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917834520339966 3.0246500968933105 32.03828430175781
Total LOSS train 33.533992239145135 valid 33.801862716674805
CE LOSS train 1.792374455011808 valid 0.44794586300849915
Contrastive LOSS train 3.174161767959595 valid 0.7561625242233276
EPOCH 148:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921853065490723 3.3800272941589355 35.59246063232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914376258850098 3.405689001083374 35.84832763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926115989685059 3.1091833114624023 32.88444519042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925134897232056 3.314565896987915 34.938175201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922533750534058 3.3897595405578613 35.68984603881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793081283569336 3.649613618850708 38.289215087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920840978622437 3.606515407562256 37.85723876953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791948914527893 2.919149875640869 30.98344612121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791764736175537 3.617114543914795 37.96290969848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917929887771606 3.9551124572753906 41.342918395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792834758758545 4.135649681091309 43.14933395385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924593687057495 3.974442720413208 41.53688430786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932852506637573 4.054656028747559 42.339847564697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928903102874756 3.7454442977905273 39.24733352661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910890579223633 3.2696168422698975 34.48725509643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930479049682617 3.3281702995300293 35.07475280761719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928088903427124 3.1891305446624756 33.68411636352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926565408706665 2.9922280311584473 31.714935302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924039363861084 3.4909262657165527 36.70166778564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916053533554077 3.5128936767578125 36.9205436706543
  batch 20 loss: 1.7916053533554077, 3.5128936767578125, 36.9205436706543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926616668701172 3.1020138263702393 32.81279754638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926239967346191 4.009220123291016 41.88482666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792555570602417 2.6724538803100586 28.517093658447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929550409317017 3.392432689666748 35.717281341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925280332565308 3.531946897506714 37.111995697021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926527261734009 3.1679844856262207 33.472496032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930632829666138 3.266223192214966 34.45529556274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920196056365967 2.5571553707122803 27.36357307434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793674349784851 3.53313946723938 37.12506866455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791394591331482 3.556255340576172 37.353946685791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792741060256958 3.359626054763794 35.388999938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913641929626465 3.033568859100342 32.127052307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926156520843506 2.8304221630096436 30.096837997436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928216457366943 3.2413992881774902 34.20681381225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792733073234558 3.406670331954956 35.85943603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792850136756897 3.493891716003418 36.73176956176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927862405776978 3.7196781635284424 38.989566802978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791703462600708 3.4696266651153564 36.48796844482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922168970108032 3.3529372215270996 35.321590423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918070554733276 2.925112247467041 31.04292869567871
  batch 40 loss: 1.7918070554733276, 2.925112247467041, 31.04292869567871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923482656478882 2.7837350368499756 29.62969970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925536632537842 3.4590747356414795 36.38330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922141551971436 3.6636862754821777 38.4290771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915217876434326 3.337291955947876 35.16444396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926379442214966 3.618194103240967 37.974578857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924220561981201 3.6616015434265137 38.40843963623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921974658966064 3.5984067916870117 37.776268005371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919355630874634 3.5041778087615967 36.83371353149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916991710662842 3.656999349594116 38.361690521240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921521663665771 2.9318692684173584 31.1108455657959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911267280578613 3.8148436546325684 39.9395637512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926439046859741 3.4402782917022705 36.19542694091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924525737762451 3.200871706008911 33.801170349121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792798638343811 3.0482189655303955 32.274986267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919869422912598 3.3465781211853027 35.25777053833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926783561706543 3.3264994621276855 35.057674407958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929179668426514 3.5225536823272705 37.018455505371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928948402404785 2.9682118892669678 31.475013732910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933282852172852 2.6962454319000244 28.755783081054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922760248184204 3.869267702102661 40.48495101928711
  batch 60 loss: 1.7922760248184204, 3.869267702102661, 40.48495101928711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930055856704712 3.2685248851776123 34.47825622558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791636347770691 3.1282052993774414 33.07368850708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927334308624268 2.953465700149536 31.327390670776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925114631652832 2.825828790664673 30.050798416137695
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7930067777633667 2.906406879425049 30.857074737548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791806697845459 3.213963508605957 33.93143844604492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917475700378418 3.4967219829559326 36.758968353271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915724515914917 3.398883819580078 35.78041076660156
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7917615175247192 2.720390558242798 28.99566650390625
Total LOSS train 35.36050852262057 valid 33.866621017456055
CE LOSS train 1.7924031257629394 valid 0.4479403793811798
Contrastive LOSS train 3.356810533083402 valid 0.6800976395606995
EPOCH 149:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921857833862305 3.296816110610962 34.76034927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791420340538025 3.455028533935547 36.341705322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925869226455688 3.556152820587158 37.3541145324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513132095337 2.887619733810425 30.668710708618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923357486724854 2.905674934387207 30.849084854125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931091785430908 3.0011086463928223 31.804195404052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921472787857056 2.796499013900757 29.757137298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919763326644897 2.6244935989379883 28.03691291809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918500900268555 3.235332489013672 34.14517593383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791890263557434 3.2021219730377197 33.8131103515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927902936935425 2.786506175994873 29.657852172851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923593521118164 3.3592689037323 35.385047912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793294072151184 3.3802313804626465 35.59560775756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929198741912842 2.9592442512512207 31.385360717773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910817861557007 2.718881130218506 28.979894638061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929596900939941 2.9386353492736816 31.179311752319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927329540252686 3.233548879623413 34.12821960449219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925407886505127 3.0700395107269287 32.49293518066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792353868484497 2.8808047771453857 30.60040283203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915911674499512 3.0085129737854004 31.876720428466797
  batch 20 loss: 1.7915911674499512, 3.0085129737854004, 31.876720428466797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926957607269287 3.3370184898376465 35.162879943847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925912141799927 3.46645450592041 36.45713424682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924026250839233 2.7954487800598145 29.746889114379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928261756896973 3.6654162406921387 38.446990966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925047874450684 3.6054131984710693 37.84663391113281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927019596099854 3.7270100116729736 39.062801361083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931655645370483 4.166531085968018 43.45847702026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920689582824707 4.114112377166748 42.93319320678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7936127185821533 4.01857852935791 41.97939682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912980318069458 2.8446707725524902 30.238006591796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792551040649414 2.6713099479675293 28.505651473999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7912981510162354 2.4579482078552246 26.37078094482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925500869750977 2.8565549850463867 30.35810089111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927477359771729 2.8742802143096924 30.53554916381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927064895629883 3.1428141593933105 33.220848083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929216623306274 3.2048020362854004 33.8409423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928433418273926 3.3084843158721924 34.877685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917877435684204 3.0334999561309814 32.12678527832031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792248249053955 3.877171754837036 40.56396484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918239831924438 3.189039468765259 33.68221664428711
  batch 40 loss: 1.7918239831924438, 3.189039468765259, 33.68221664428711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922831773757935 2.9634735584259033 31.427019119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923848628997803 2.718702554702759 28.97941017150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920610904693604 3.254504442214966 34.33710479736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914700508117676 3.4720299243927 36.51176834106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926440238952637 2.8259801864624023 30.052446365356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923623323440552 2.9622385501861572 31.41474723815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922719717025757 3.708296537399292 38.87523651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920923233032227 4.098872184753418 42.78081512451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917823791503906 4.071672439575195 42.508506774902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921613454818726 3.5848581790924072 37.640743255615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911534309387207 3.824887275695801 40.04002380371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792689561843872 3.4161527156829834 35.95421600341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924848794937134 3.4524495601654053 36.316978454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927911281585693 3.4351394176483154 36.14418411254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919808626174927 3.436506509780884 36.157047271728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926137447357178 3.075809955596924 32.55071258544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928621768951416 3.417607545852661 35.968936920166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928093671798706 3.5598371028900146 37.391178131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932300567626953 3.688189744949341 38.67512512207031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920691967010498 3.8687756061553955 40.47982406616211
  batch 60 loss: 1.7920691967010498, 3.8687756061553955, 40.47982406616211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927587032318115 3.926461935043335 41.057376861572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915555238723755 3.055385112762451 32.34540939331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925286293029785 3.0849297046661377 32.64182662963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923434972763062 3.137812614440918 33.17047119140625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7929338216781616 2.746675968170166 29.259693145751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919267416000366 3.218144178390503 33.97336959838867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918027639389038 3.363774299621582 35.429542541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916371822357178 3.337554454803467 35.16718292236328
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918431758880615 3.078585147857666 32.577693939208984
Total LOSS train 34.475501221876875 valid 34.28694725036621
CE LOSS train 1.7923738974791306 valid 0.4479607939720154
Contrastive LOSS train 3.2683127623337964 valid 0.7696462869644165
EPOCH 150:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922176122665405 3.2922251224517822 34.71446990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791495680809021 3.268986225128174 34.48135757446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925848960876465 3.237617015838623 34.16875457763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924033403396606 3.236476182937622 34.15716552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921435832977295 3.680396556854248 38.59611129760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928738594055176 3.1398842334747314 33.191715240478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792087197303772 3.172776222229004 33.51984786987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920340299606323 2.6236653327941895 28.0286865234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919354438781738 2.6663625240325928 28.4555606842041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920022010803223 2.8980824947357178 30.7728271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928612232208252 2.805659055709839 29.84945297241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79243803024292 3.325753688812256 35.04997634887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932262420654297 2.889681339263916 30.690038681030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927680015563965 3.774533987045288 39.53810501098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910125255584717 3.612091541290283 37.91192626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928649187088013 3.4523165225982666 36.31603240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927569150924683 3.071674346923828 32.509498596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925933599472046 3.902339458465576 40.81599044799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924296855926514 3.255486488342285 34.347293853759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916710376739502 2.5813496112823486 27.605167388916016
  batch 20 loss: 1.7916710376739502, 2.5813496112823486, 27.605167388916016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926571369171143 2.760568857192993 29.398345947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925523519515991 3.6382899284362793 38.175453186035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924737930297852 3.3612966537475586 35.40544128417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792861819267273 4.099496364593506 42.78782653808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79252028465271 3.6854805946350098 38.64732360839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926446199417114 3.290480136871338 34.697444915771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793048620223999 3.3672845363616943 35.46589279174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919844388961792 2.985585927963257 31.647844314575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793545126914978 3.0212655067443848 32.00619888305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914361953735352 3.6568386554718018 38.35982131958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926676273345947 2.8727076053619385 30.519742965698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913967370986938 3.63430118560791 38.13440704345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925351858139038 3.1433019638061523 33.225555419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927272319793701 3.609832525253296 37.89105224609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927137613296509 3.2958476543426514 34.751190185546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928403615951538 3.081068515777588 32.60352325439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792746901512146 2.926069736480713 31.053443908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917366027832031 3.1875901222229004 33.66763687133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922767400741577 3.47257137298584 36.51799392700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791825771331787 3.010460615158081 31.896432876586914
  batch 40 loss: 1.791825771331787, 3.010460615158081, 31.896432876586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923426628112793 3.019263744354248 31.984981536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924798727035522 3.8323349952697754 40.11582946777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921817302703857 3.6967403888702393 38.759586334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791528582572937 3.4796440601348877 36.58796691894531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925533056259155 3.43121075630188 36.10466003417969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923392057418823 3.602796792984009 37.820308685302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921524047851562 3.445897340774536 36.25112533569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919851541519165 3.67258620262146 38.517845153808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917377948760986 3.357243299484253 35.36417007446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921639680862427 3.1143510341644287 32.93567657470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911006212234497 3.1433188915252686 33.22428894042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925468683242798 3.345479726791382 35.24734115600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923603057861328 3.462634325027466 36.418701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792717456817627 2.7372097969055176 29.164813995361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919247150421143 3.080670118331909 32.59862518310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926349639892578 3.787750482559204 39.670143127441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928494215011597 2.904693603515625 30.839784622192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928507328033447 3.173532009124756 33.52817153930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932671308517456 3.3925657272338867 35.71892547607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922159433364868 3.6060562133789062 37.852779388427734
  batch 60 loss: 1.7922159433364868, 3.6060562133789062, 37.852779388427734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792894959449768 2.9168152809143066 30.96104621887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916563749313354 3.325637102127075 35.04802703857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926393747329712 3.446274757385254 36.25538635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792267084121704 3.194647789001465 33.738746643066406
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928022146224976 2.2495625019073486 24.288427352905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918636798858643 3.3749589920043945 35.54145050048828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791768193244934 3.6094510555267334 37.88627624511719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915875911712646 3.209225654602051 33.88384246826172
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918298244476318 3.0993692874908447 32.7855224609375
Total LOSS train 34.470306279109074 valid 35.02427291870117
CE LOSS train 1.7923659067887527 valid 0.44795745611190796
Contrastive LOSS train 3.267794051537147 valid 0.7748423218727112
EPOCH 151:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922083139419556 3.611912727355957 37.91133499145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914645671844482 3.8211312294006348 40.002777099609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925821542739868 3.5129754543304443 36.92233657836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924280166625977 3.1593079566955566 33.38550567626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921371459960938 3.3676717281341553 35.46885299682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928876876831055 3.9704203605651855 41.497093200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919877767562866 3.8862955570220947 40.654945373535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919167280197144 3.821320056915283 40.0051155090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918152809143066 3.5351815223693848 37.14363098144531
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918870449066162 3.7681970596313477 39.473854064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928335666656494 3.397088050842285 35.76371383666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924609184265137 3.73880934715271 39.1805534362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79320228099823 3.784362554550171 39.63682556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927803993225098 3.4252736568450928 36.04551696777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910465002059937 3.285273551940918 34.64378356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928361892700195 3.438870429992676 36.18153762817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926777601242065 3.462129831314087 36.413978576660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925461530685425 3.701284170150757 38.80538558959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923909425735474 3.2317378520965576 34.109771728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 3.4322569370269775 36.11417770385742
  batch 20 loss: 1.791605830192566, 3.4322569370269775, 36.11417770385742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926528453826904 2.8226215839385986 30.01886749267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925763130187988 3.8486876487731934 40.27945327758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924046516418457 2.8232905864715576 30.025310516357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792838215827942 2.9982309341430664 31.775148391723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924706935882568 3.5048673152923584 36.84114074707031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926199436187744 3.464513063430786 36.43775177001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930537462234497 2.908856153488159 30.881616592407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919962406158447 2.9487035274505615 31.27903175354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935746908187866 3.7911410331726074 39.704986572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914245128631592 3.288215160369873 34.67357635498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792658805847168 2.8909692764282227 30.702350616455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913967370986938 3.134597063064575 33.137367248535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792570948600769 3.553719997406006 37.32977294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927433252334595 3.4063339233398438 35.856082916259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 3.1403632164001465 33.196285247802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927794456481934 2.6570708751678467 28.363489151000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926923036575317 2.759425640106201 29.38694953918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916736602783203 2.9380924701690674 31.172597885131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922011613845825 3.038372755050659 32.17593002319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791764736175537 3.1791462898254395 33.58322525024414
  batch 40 loss: 1.791764736175537, 3.1791462898254395, 33.58322525024414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922518253326416 3.4915108680725098 36.707359313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924612760543823 2.9142913818359375 30.935375213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921507358551025 3.1474103927612305 33.26625442504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 3.734255313873291 39.13412094116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925699949264526 2.504302740097046 26.83559799194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923039197921753 2.4651827812194824 26.44413185119629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921324968338013 3.028526782989502 32.07740020751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919567823410034 2.978968858718872 31.58164405822754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916998863220215 2.5840985774993896 27.632686614990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921168804168701 2.990867853164673 31.700794219970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910902500152588 3.2854654788970947 34.645748138427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925118207931519 2.6925666332244873 28.718177795410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792336106300354 3.4039361476898193 35.831695556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926836013793945 2.726158380508423 29.05426788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919176816940308 3.678349256515503 38.575408935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926331758499146 3.31298828125 34.922515869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928838729858398 3.6479620933532715 38.27250671386719
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928626537322998 3.4236221313476562 36.029083251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932978868484497 3.6969544887542725 38.762840270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921899557113647 4.139238357543945 43.184574127197266
  batch 60 loss: 1.7921899557113647, 4.139238357543945, 43.184574127197266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928922176361084 3.4384329319000244 36.177223205566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916501760482788 3.475703477859497 36.548683166503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926236391067505 3.323417901992798 35.02680587768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922585010528564 2.876329183578491 30.555551528930664
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928433418273926 3.2724039554595947 34.516883850097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918884754180908 3.673142433166504 38.523311614990234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791804552078247 3.4722306728363037 36.51411056518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 3.049211025238037 32.28371810913086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918721437454224 3.5125579833984375 36.91745376586914
Total LOSS train 34.82029163654034 valid 36.059648513793945
CE LOSS train 1.7923435321220984 valid 0.4479680359363556
Contrastive LOSS train 3.3027948122758133 valid 0.8781394958496094
EPOCH 152:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922438383102417 3.5538394451141357 37.33063888549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79152250289917 3.666069984436035 38.45222091674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792582392692566 2.836177349090576 30.154356002807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 2.493122100830078 26.72365951538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921557426452637 3.061701774597168 32.40917205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929377555847168 3.416593074798584 35.95886993408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919758558273315 3.1783387660980225 33.57536315917969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79194974899292 3.262418508529663 34.416133880615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791806936264038 2.7722322940826416 29.514129638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918766736984253 2.6606550216674805 28.398426055908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792823314666748 2.781857490539551 29.611398696899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924383878707886 2.9900717735290527 31.693157196044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931896448135376 3.0632717609405518 32.425907135009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927789688110352 3.4883995056152344 36.67677307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791035532951355 3.0401415824890137 32.19245147705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792832612991333 2.6207237243652344 28.000070571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792655348777771 2.293327808380127 24.725933074951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925505638122559 2.916529417037964 30.95784568786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923816442489624 2.9024484157562256 30.816865921020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915970087051392 3.0351104736328125 32.1427001953125
  batch 20 loss: 1.7915970087051392, 3.0351104736328125, 32.1427001953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926326990127563 2.4713399410247803 26.506032943725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925671339035034 3.2646772861480713 34.4393424987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923988103866577 3.2567784786224365 34.36018371582031
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928321361541748 2.5555830001831055 27.348661422729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924683094024658 2.8130745887756348 29.923213958740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926284074783325 3.5002198219299316 36.79482650756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793045163154602 2.804933786392212 29.842382431030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919909954071045 2.610703706741333 27.89902687072754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935515642166138 3.109614849090576 32.88970184326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791425108909607 2.8592329025268555 30.38375473022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926523685455322 2.9445958137512207 31.238609313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914021015167236 3.754136800765991 39.332767486572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792573094367981 2.6504645347595215 28.29722023010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792744755744934 2.3292291164398193 25.085037231445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926526069641113 2.445760488510132 26.25025749206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792779803276062 2.6509976387023926 28.30275535583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792675256729126 3.022568941116333 32.01836395263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916595935821533 2.470252513885498 26.494186401367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921969890594482 3.0986781120300293 32.77898025512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917624711990356 2.7213478088378906 29.00524139404297
  batch 40 loss: 1.7917624711990356, 2.7213478088378906, 29.00524139404297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922508716583252 2.5370678901672363 27.16292953491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 2.4571073055267334 26.36354637145996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921580076217651 3.010490894317627 31.897066116333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791579246520996 2.8655920028686523 30.447498321533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925692796707153 2.4260919094085693 26.053489685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922970056533813 2.313513994216919 24.92743682861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921273708343506 2.2470719814300537 24.262847900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919493913650513 2.26953387260437 24.487289428710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916873693466187 2.9359047412872314 31.150733947753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921077013015747 3.175858974456787 33.550697326660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910834550857544 2.5597336292266846 27.38842010498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925084829330444 2.3424389362335205 25.21689796447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79232919216156 2.8602511882781982 30.39484214782715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926719188690186 2.809288740158081 29.88555908203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918972969055176 2.5421319007873535 27.213214874267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792615532875061 2.604280948638916 27.835424423217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928643226623535 3.5585124492645264 37.37799072265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928483486175537 2.9555294513702393 31.348142623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793292760848999 3.4475622177124023 36.268917083740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921940088272095 2.923227548599243 31.02446937561035
  batch 60 loss: 1.7921940088272095, 2.923227548599243, 31.02446937561035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929006814956665 2.607555389404297 27.868453979492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916702032089233 2.8091824054718018 29.883493423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926456928253174 3.373124599456787 35.52389144897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922720909118652 2.777324676513672 29.565519332885742
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928643226623535 2.9867351055145264 31.660215377807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791894793510437 3.1309397220611572 33.10129165649414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918174266815186 2.7267537117004395 29.05935287475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916046380996704 2.87423038482666 30.53390884399414
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791886568069458 2.5339910984039307 27.131797790527344
Total LOSS train 30.52500933133639 valid 29.95658779144287
CE LOSS train 1.7923426187955416 valid 0.4479716420173645
Contrastive LOSS train 2.8732666639181286 valid 0.6334977746009827
EPOCH 153:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922574281692505 2.79463267326355 29.738584518432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915260791778564 2.6190390586853027 27.981918334960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925727367401123 2.6784467697143555 28.57703971862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79242742061615 3.321525812149048 35.007686614990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921411991119385 3.66465163230896 38.438655853271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79293692111969 3.126678228378296 33.05971908569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791962742805481 3.3430192470550537 35.22215270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791946291923523 2.7978122234344482 29.770069122314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918037176132202 2.499873638153076 26.79054069519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791865587234497 2.244659185409546 24.23845863342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928178310394287 2.683990478515625 28.632722854614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.6505188941955566 28.29762077331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931746244430542 2.3622262477874756 25.415437698364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927753925323486 2.754863739013672 29.341413497924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791038155555725 2.96244740486145 31.415512084960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928376197814941 2.9609525203704834 31.402362823486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926609516143799 2.975820779800415 31.55086898803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556881904602 2.535658597946167 27.14914321899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923835515975952 2.329141855239868 25.08380126953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915961742401123 1.9371280670166016 21.16287612915039
  batch 20 loss: 1.7915961742401123, 1.9371280670166016, 21.16287612915039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926236391067505 2.186957359313965 23.66219711303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925604581832886 2.5533111095428467 27.325672149658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792386531829834 3.106489896774292 32.85728454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792820930480957 2.8751518726348877 30.54433822631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924590110778809 2.6387720108032227 28.180179595947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926117181777954 3.046739101409912 32.26000213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930322885513306 2.7916247844696045 29.709280014038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919737100601196 2.298245429992676 24.77442741394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935442924499512 2.5727617740631104 27.521162033081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914302349090576 3.1068975925445557 32.86040496826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926602363586426 3.1311075687408447 33.103736877441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79140305519104 2.577712059020996 27.568523406982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925610542297363 2.7576370239257812 29.36893081665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792730450630188 2.2423675060272217 24.216405868530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926446199417114 2.6617724895477295 28.410369873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927775382995605 2.9439871311187744 31.232648849487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926852703094482 2.586834669113159 27.66103172302246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916700839996338 2.8084003925323486 29.875673294067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792202115058899 2.738273859024048 29.17494010925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917615175247192 2.350595712661743 25.297718048095703
  batch 40 loss: 1.7917615175247192, 2.350595712661743, 25.297718048095703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922449111938477 2.79064679145813 29.698711395263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924612760543823 2.685108184814453 28.643543243408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921448945999146 2.9345624446868896 31.13776969909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791571021080017 3.20780873298645 33.869659423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925604581832886 3.1088953018188477 32.88151168823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922892570495605 2.2099697589874268 23.891986846923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792120099067688 2.884803295135498 30.640153884887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791948914527893 3.504645824432373 36.838409423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916878461837769 3.131762742996216 33.10931396484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921069860458374 2.764427423477173 29.43638038635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 2.928507089614868 31.076156616210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925019264221191 2.483504056930542 26.62754249572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923213243484497 2.2751476764678955 24.543798446655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792669415473938 2.868734836578369 30.480016708374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919023036956787 2.4247922897338867 26.039825439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926161289215088 2.4251723289489746 26.044340133666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928705215454102 2.983431339263916 31.627182006835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928516864776611 2.5597503185272217 27.390356063842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932978868484497 3.0741117000579834 32.53441619873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921900749206543 2.3757855892181396 25.550046920776367
  batch 60 loss: 1.7921900749206543, 2.3757855892181396, 25.550046920776367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928955554962158 2.8896901607513428 30.68979835510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791663646697998 2.5867807865142822 27.65947151184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926424741744995 2.440263032913208 26.19527244567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922767400741577 3.147003650665283 33.26231384277344
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928721904754639 2.6189188957214355 27.9820613861084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919056415557861 3.0248422622680664 32.04032897949219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918250560760498 2.7963900566101074 29.755725860595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916111946105957 2.54292368888855 27.220848083496094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918930053710938 2.371070623397827 25.502599716186523
Total LOSS train 29.25740846487192 valid 28.629875659942627
CE LOSS train 1.7923391580581665 valid 0.44797325134277344
Contrastive LOSS train 2.746506933065561 valid 0.5927676558494568
EPOCH 154:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922636270523071 2.4348347187042236 26.140609741210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915363311767578 3.076887845993042 32.56041717529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925776243209839 3.1444592475891113 33.23716735839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924329042434692 2.300266981124878 24.795103073120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921468019485474 3.13744854927063 33.16663360595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929308414459229 2.645249605178833 28.245426177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791948676109314 2.485680341720581 26.648752212524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919256687164307 3.4125826358795166 35.917755126953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917871475219727 2.64846134185791 28.27640151977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918545007705688 3.131587505340576 33.107730865478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928118705749512 3.791943073272705 39.712242126464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79242742061615 3.023073434829712 32.023162841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931604385375977 3.267469882965088 34.467857360839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927587032318115 2.770777463912964 29.500534057617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910287380218506 3.2425549030303955 34.216575622558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928353548049927 3.095665216445923 32.749488830566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926632165908813 2.8869428634643555 30.662092208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925574779510498 2.8629276752471924 30.421833038330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923831939697266 2.655162811279297 28.344011306762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791584849357605 3.1920247077941895 33.711830139160156
  batch 20 loss: 1.791584849357605, 3.1920247077941895, 33.711830139160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792605996131897 2.1692514419555664 23.48512077331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925410270690918 2.563896417617798 27.43150520324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923663854599 3.213406801223755 33.92643356323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928037643432617 2.5399742126464844 27.192546844482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792449951171875 2.77081298828125 29.500579833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926145792007446 2.590567111968994 27.698284149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930421829223633 2.764446258544922 29.437503814697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919847965240479 2.830753803253174 30.09952163696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935481071472168 2.6210525035858154 28.004074096679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791429877281189 2.339440107345581 25.18583106994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926464080810547 2.4323267936706543 26.115915298461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913893461227417 3.253408670425415 34.325477600097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 3.234046459197998 34.13300704956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927038669586182 3.0989415645599365 32.78211975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79262375831604 3.1391377449035645 33.18400192260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927584648132324 2.458592176437378 26.378681182861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926692962646484 2.837590456008911 30.1685733795166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916597127914429 3.1021926403045654 32.8135871887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921963930130005 3.3069839477539062 34.862037658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917412519454956 3.238452196121216 34.17626190185547
  batch 40 loss: 1.7917412519454956, 3.238452196121216, 34.17626190185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922320365905762 2.896117687225342 30.75341033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79244863986969 3.541438341140747 37.20683288574219
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792125940322876 2.6347038745880127 28.139163970947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 2.4911651611328125 26.70322036743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925527095794678 2.1945011615753174 23.737564086914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922660112380981 3.073892831802368 32.531192779541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792105793952942 2.7933571338653564 29.725677490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919352054595947 2.297320604324341 24.765140533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916715145111084 2.3712668418884277 25.50434112548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921069860458374 2.6626462936401367 28.418569564819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910887002944946 2.3309717178344727 25.100805282592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925156354904175 2.3939192295074463 25.731706619262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923377752304077 1.934633493423462 21.138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926826477050781 1.9086586236953735 20.879268646240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791907787322998 2.42383074760437 26.030216217041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926207780838013 2.1277246475219727 23.069868087768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928701639175415 2.5027318000793457 26.820186614990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928497791290283 2.127479314804077 23.067644119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793290376663208 2.848910331726074 30.282394409179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921805381774902 2.269623041152954 24.48841094970703
  batch 60 loss: 1.7921805381774902, 2.269623041152954, 24.48841094970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928743362426758 2.30503511428833 24.843223571777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791649341583252 2.6934831142425537 28.72648048400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926197052001953 2.9465243816375732 31.257863998413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792242169380188 2.3453938961029053 25.24618148803711
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928438186645508 2.4488019943237305 26.280864715576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918974161148071 2.8092334270477295 29.884231567382812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918130159378052 2.7385048866271973 29.176860809326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915946245193481 2.3187448978424072 24.97904396057129
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791890263557434 2.197120189666748 23.763093948364258
Total LOSS train 29.21931674663837 valid 26.950807571411133
CE LOSS train 1.7923310793363132 valid 0.4479725658893585
Contrastive LOSS train 2.7426985612282384 valid 0.549280047416687
EPOCH 155:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922579050064087 2.979687213897705 31.589128494262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915281057357788 2.8367230892181396 30.15876007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925719022750854 2.7124979496002197 28.917551040649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792424201965332 2.445047616958618 26.242900848388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921361923217773 2.9389407634735107 31.18154525756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929341793060303 3.2523040771484375 34.315975189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919460535049438 2.7027809619903564 28.81975555419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919341325759888 2.5654067993164062 27.446002960205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791787028312683 2.639037609100342 28.18216323852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918415069580078 3.493884563446045 36.730690002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928035259246826 2.829361915588379 30.086421966552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924238443374634 2.8867673873901367 30.660097122192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931538820266724 3.289717197418213 34.69032669067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927522659301758 2.3381264209747314 25.174015045166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910226583480835 2.473932981491089 26.530353546142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928224802017212 2.8872082233428955 30.66490364074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792650818824768 2.0337014198303223 22.129663467407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925524711608887 2.3970632553100586 25.763185501098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923896312713623 2.1317601203918457 23.109989166259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916007041931152 3.04292631149292 32.220863342285156
  batch 20 loss: 1.7916007041931152, 3.04292631149292, 32.220863342285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926195859909058 2.852828025817871 30.320899963378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792553424835205 2.3814315795898438 25.606868743896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923773527145386 1.9390841722488403 21.18321990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928162813186646 2.627472162246704 28.067537307739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924567461013794 3.036257028579712 32.155025482177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926112413406372 2.5391833782196045 27.184446334838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793031930923462 2.451188087463379 26.304912567138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791969656944275 2.9837048053741455 31.629016876220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935335636138916 2.8019495010375977 29.81302833557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914289236068726 2.9115407466888428 30.906837463378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926448583602905 2.917067766189575 30.963321685791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913938760757446 2.5997629165649414 27.78902244567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925491333007812 2.559431552886963 27.386863708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 2.6203713417053223 27.99642562866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926294803619385 2.5448882579803467 27.241512298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927610874176025 3.0211105346679688 32.003868103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926675081253052 2.8001503944396973 29.794170379638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916580438613892 2.5302746295928955 27.094404220581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921972274780273 2.561648368835449 27.408679962158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917490005493164 2.787729501724243 29.669044494628906
  batch 40 loss: 1.7917490005493164, 2.787729501724243, 29.669044494628906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922369241714478 2.770012617111206 29.49236297607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924580574035645 2.51039981842041 26.896455764770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921425104141235 2.598907470703125 27.781217575073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791576862335205 2.412461280822754 25.916189193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925591468811035 2.4877121448516846 26.669679641723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922778129577637 2.502052068710327 26.81279945373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921149730682373 3.5395772457122803 37.18788528442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919425964355469 2.904418468475342 30.83612823486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916728258132935 3.4707109928131104 36.498783111572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920960187911987 2.8258416652679443 30.050512313842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791069507598877 2.8639981746673584 30.43105125427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792486310005188 2.4620625972747803 26.41311264038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923109531402588 2.1611781120300293 23.40409278869629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926629781723022 2.6931169033050537 28.723833084106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 2.408900737762451 25.88090705871582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926136255264282 2.1699459552764893 23.49207305908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928651571273804 2.5683786869049072 27.476652145385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928444147109985 3.1355948448181152 33.1487922668457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793285608291626 2.543941020965576 27.232696533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921810150146484 2.2423794269561768 24.215974807739258
  batch 60 loss: 1.7921810150146484, 2.2423794269561768, 24.215974807739258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792878270149231 2.458007574081421 26.372955322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916611433029175 2.1561789512634277 23.353450775146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792631983757019 2.195953369140625 23.752164840698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922496795654297 2.5938591957092285 27.7308406829834
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792845606803894 2.4121968746185303 25.914813995361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918941974639893 2.7749431133270264 29.54132652282715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791812539100647 2.960197925567627 31.39379119873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915947437286377 3.0153965950012207 31.945560455322266
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791890025138855 2.6368677616119385 28.160566329956055
Total LOSS train 28.474135736318736 valid 30.260311126708984
CE LOSS train 1.792330131163964 valid 0.44797250628471375
Contrastive LOSS train 2.6681805665676412 valid 0.6592169404029846
EPOCH 156:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922582626342773 2.481097459793091 26.603233337402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915242910385132 2.979763984680176 31.58916473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925643920898438 2.9656667709350586 31.44923210144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792415738105774 2.88217830657959 30.614198684692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921273708343506 3.541003942489624 37.20216369628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929470539093018 2.816452741622925 29.957475662231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919282913208008 2.662492275238037 28.416851043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919319868087769 2.4285056591033936 26.076988220214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917884588241577 2.1628823280334473 23.420610427856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918518781661987 2.360764741897583 25.399497985839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928193807601929 2.7558016777038574 29.35083770751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924386262893677 2.70760178565979 28.86845588684082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931653261184692 2.5423316955566406 27.216482162475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927625179290771 2.163672924041748 23.429492950439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910373210906982 2.205268621444702 23.84372329711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928342819213867 2.527488946914673 27.06772232055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792663335800171 2.6251847743988037 28.044511795043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925529479980469 2.9921813011169434 31.714366912841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923872470855713 2.103506565093994 22.827451705932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915875911712646 2.846482753753662 30.25641632080078
  batch 20 loss: 1.7915875911712646, 2.846482753753662, 30.25641632080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925933599472046 2.3246989250183105 25.039583206176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925324440002441 2.6695704460144043 28.488237380981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923510074615479 2.6361238956451416 28.153589248657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927993535995483 3.0075271129608154 31.868070602416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924479246139526 3.2661921977996826 34.454368591308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926009893417358 2.6487069129943848 28.2796688079834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793026089668274 2.616070032119751 27.953725814819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919695377349854 2.212632417678833 23.918292999267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79353666305542 2.8593859672546387 30.38739776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791433572769165 3.0557854175567627 32.34928894042969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792641043663025 3.0356903076171875 32.14954376220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791382908821106 2.417846202850342 25.969846725463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.7132086753845215 28.924617767333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926933765411377 2.4007198810577393 25.79989242553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926127910614014 2.69826602935791 28.775272369384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927442789077759 3.049466371536255 32.28740692138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792656421661377 2.414788007736206 25.940536499023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.315682888031006 24.948484420776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921925783157349 2.6180102825164795 27.9722957611084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739583015442 2.5860753059387207 27.65249252319336
  batch 40 loss: 1.791739583015442, 2.5860753059387207, 27.65249252319336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792230248451233 2.6482975482940674 28.275205612182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924450635910034 2.4068589210510254 25.861032485961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921220064163208 3.1222259998321533 33.014381408691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915606498718262 2.7414751052856445 29.20631217956543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792546033859253 1.9304293394088745 21.096837997436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922619581222534 2.183818817138672 23.630449295043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921048402786255 2.7770636081695557 29.562740325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919373512268066 3.074615716934204 32.53809356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916674613952637 3.169464111328125 33.48630905151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920998334884644 2.564164876937866 27.433748245239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910791635513306 2.6232101917266846 28.02318000793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924973964691162 2.6723945140838623 28.516443252563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923216819763184 2.594655752182007 27.738880157470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792664647102356 2.6325900554656982 28.118566513061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918928861618042 2.4778807163238525 26.57069969177246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926105260849 2.9229393005371094 31.022003173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792851448059082 2.717158317565918 28.964435577392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928282022476196 2.8820912837982178 30.613740921020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793271780014038 2.825906991958618 30.05234146118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921710014343262 2.7208147048950195 29.00031852722168
  batch 60 loss: 1.7921710014343262, 2.7208147048950195, 29.00031852722168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928664684295654 2.2831759452819824 24.62462615966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916526794433594 3.1901729106903076 33.693382263183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792616844177246 2.7415802478790283 29.208419799804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922307252883911 2.651172161102295 28.303953170776367
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928309440612793 2.058852195739746 22.3813533782959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918885946273804 3.4317541122436523 36.109432220458984
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918063402175903 2.807758092880249 29.869388580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915921211242676 2.8300986289978027 30.092578887939453
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791885256767273 2.480257987976074 26.594465255737305
Total LOSS train 28.3938299032358 valid 30.666466236114502
CE LOSS train 1.7923244127860436 valid 0.44797131419181824
Contrastive LOSS train 2.6601505517959594 valid 0.6200644969940186
EPOCH 157:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922533750534058 3.0471932888031006 32.26418685913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79152512550354 2.854762554168701 30.33915138244629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792562484741211 2.9140255451202393 30.932817459106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924214601516724 3.286217451095581 34.654598236083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921286821365356 2.5922281742095947 27.71441078186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929280996322632 2.535975456237793 27.15268325805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919377088546753 3.1741244792938232 33.53318405151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919321060180664 2.620262861251831 27.99456024169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791786789894104 2.8239119052886963 30.03090476989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918415069580078 2.7878403663635254 29.670244216918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928062677383423 2.892681121826172 30.71961784362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924267053604126 2.780510902404785 29.597536087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931456565856934 3.0991299152374268 32.78444290161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927488088607788 2.671778440475464 28.510534286499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910284996032715 2.5205752849578857 26.996782302856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792827844619751 2.7672078609466553 29.464906692504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926558256149292 2.6612396240234375 28.405052185058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925469875335693 2.718688488006592 28.979433059692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923797369003296 2.494166135787964 26.734041213989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791583776473999 2.4215950965881348 26.00753402709961
  batch 20 loss: 1.791583776473999, 2.4215950965881348, 26.00753402709961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792590618133545 2.6451711654663086 28.24430274963379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925341129302979 3.478222370147705 36.57475662231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923564910888672 3.595322370529175 37.745582580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928014993667603 2.520223379135132 26.99503517150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792444109916687 2.751131534576416 29.30375862121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792596697807312 2.148562431335449 23.278221130371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930189371109009 2.628237009048462 28.075387954711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79196298122406 1.9291893243789673 21.0838565826416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935329675674438 2.346652030944824 25.260053634643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914434671401978 3.031111240386963 32.10255432128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926568984985352 2.350782632827759 25.30048370361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914035320281982 2.6221089363098145 28.01249122619629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925493717193604 2.504678249359131 26.839332580566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927124500274658 2.479128360748291 26.583995819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926325798034668 2.514134645462036 26.933979034423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927610874176025 2.665538787841797 28.448148727416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926695346832275 2.593177318572998 27.724443435668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916616201400757 2.010542869567871 21.897090911865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792195200920105 2.3390815258026123 25.18301010131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.557785987854004 27.369600296020508
  batch 40 loss: 1.7917407751083374, 2.557785987854004, 27.369600296020508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922329902648926 2.377318859100342 25.56542205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924537658691406 2.2610011100769043 24.4024658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921303510665894 2.793360471725464 29.72573471069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791574239730835 2.417649507522583 25.968069076538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925530672073364 2.7113916873931885 28.906469345092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792266845703125 2.790686845779419 29.699134826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921077013015747 2.7500321865081787 29.292430877685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919352054595947 2.8980813026428223 30.772747039794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916600704193115 2.179476261138916 23.586421966552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920924425125122 2.435002326965332 26.14211654663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910710573196411 2.9117047786712646 30.908119201660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924832105636597 2.390162229537964 25.69410514831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923142910003662 2.095541477203369 22.74772834777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926610708236694 2.414184093475342 25.93450355529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791887879371643 2.8265299797058105 30.057188034057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926071882247925 2.7145397663116455 28.938003540039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928504943847656 2.3095197677612305 24.88804817199707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928324937820435 2.4096739292144775 25.889572143554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932827472686768 3.15462589263916 33.339542388916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792185664176941 2.213491916656494 23.92710304260254
  batch 60 loss: 1.792185664176941, 2.213491916656494, 23.92710304260254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928780317306519 2.132601737976074 23.118894577026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916628122329712 2.3413619995117188 25.20528221130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926239967346191 2.2838079929351807 24.63070297241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922395467758179 2.46708083152771 26.463048934936523
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792840838432312 1.8837767839431763 20.6306095123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918981313705444 2.641522169113159 28.207120895385742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918167114257812 2.3161118030548096 24.95293426513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916005849838257 2.6136274337768555 27.927875518798828
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918972969055176 2.009122610092163 21.88312339782715
Total LOSS train 27.875018017108623 valid 25.74276351928711
CE LOSS train 1.7923255443572998 valid 0.4479743242263794
Contrastive LOSS train 2.608269243973952 valid 0.5022806525230408
EPOCH 158:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922637462615967 2.23667311668396 24.158994674682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915334701538086 2.896177053451538 30.75330352783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925684452056885 1.8697319030761719 20.489887237548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924221754074097 2.300029754638672 24.7927188873291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921264171600342 2.4087119102478027 25.87924575805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792932391166687 2.2345492839813232 24.138425827026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919237613677979 2.266265869140625 24.45458221435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919275760650635 2.2424395084381104 24.21632194519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917752265930176 2.2321197986602783 24.112972259521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918277978897095 1.970521092414856 21.497039794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928022146224976 2.44915509223938 26.284353256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792433500289917 2.4580674171447754 26.373106002807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931491136550903 2.2284624576568604 24.077774047851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927590608596802 2.5519039630889893 27.311798095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910387516021729 2.306835889816284 24.859397888183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79282808303833 2.7516212463378906 29.309040069580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792650818824768 2.208900213241577 23.88165283203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 2.745924949645996 29.251789093017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923771142959595 2.1167783737182617 22.960161209106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791590690612793 2.4014604091644287 25.806194305419922
  batch 20 loss: 1.791590690612793, 2.4014604091644287, 25.806194305419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925976514816284 2.130208730697632 23.094684600830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792547345161438 2.2824652194976807 24.617198944091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 2.2075612545013428 23.867977142333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792811393737793 2.51657772064209 26.958587646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924517393112183 3.053394317626953 32.326393127441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925939559936523 2.9684948921203613 31.477542877197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930080890655518 2.0267794132232666 22.060802459716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919445037841797 2.4138922691345215 25.93086814880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79351007938385 2.5652968883514404 27.44647979736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914340496063232 2.7439520359039307 29.230953216552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926441431045532 3.3165838718414307 34.9584846496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913928031921387 3.174408197402954 33.53547286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925364971160889 2.19948410987854 23.787376403808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792694091796875 2.9429540634155273 31.22223472595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926113605499268 3.4448442459106445 36.241050720214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927353382110596 3.206291913986206 33.855655670166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926429510116577 2.5817856788635254 27.610498428344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916420698165894 2.6825926303863525 28.61756706237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921857833862305 2.155022382736206 23.342411041259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917295694351196 2.533463954925537 27.12636947631836
  batch 40 loss: 1.7917295694351196, 2.533463954925537, 27.12636947631836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922316789627075 2.7204878330230713 28.99711036682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924565076828003 1.630132794380188 18.09378433227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792131781578064 2.1400599479675293 23.192731857299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915840148925781 2.244649648666382 24.238080978393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925612926483154 2.1323964595794678 23.116525650024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922719717025757 2.3834660053253174 25.62693214416504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.6172399520874023 27.96451187133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919361591339111 2.5663630962371826 27.455568313598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791658878326416 2.307685613632202 24.868515014648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920931577682495 2.479355812072754 26.585651397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791064977645874 2.5737128257751465 27.528194427490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 2.8120791912078857 29.913265228271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923041582107544 2.1301095485687256 23.093400955200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926568984985352 2.3386785984039307 25.179443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791886329650879 2.5214285850524902 27.00617218017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926102876663208 2.1626124382019043 23.41873550415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928473949432373 2.4508070945739746 26.300918579101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928260564804077 2.364266872406006 25.435495376586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793270230293274 2.9419167041778564 31.21243667602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921732664108276 2.3354809284210205 25.146982192993164
  batch 60 loss: 1.7921732664108276, 2.3354809284210205, 25.146982192993164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928659915924072 2.3856382369995117 25.649248123168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916611433029175 2.386380672454834 25.655467987060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792619228363037 2.652355432510376 28.316173553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922276258468628 2.327988862991333 25.072114944458008
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79282546043396 1.673385500907898 18.52667999267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918893098831177 2.2269392013549805 24.061281204223633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791807770729065 2.0955474376678467 22.747282028198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915890216827393 1.8471490144729614 20.263080596923828
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918959856033325 1.756203293800354 19.35392951965332
Total LOSS train 26.300177060640774 valid 21.606393337249756
CE LOSS train 1.792321493075444 valid 0.44797399640083313
Contrastive LOSS train 2.4507855653762816 valid 0.4390508234500885
Saved best model. Old loss 25.676485061645508 and new best loss 21.606393337249756
EPOCH 159:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922594547271729 1.9905142784118652 21.697402954101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915288209915161 2.626490354537964 28.056432723999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925642728805542 2.60014009475708 27.793964385986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924199104309082 2.789849281311035 29.6909122467041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921216487884521 2.926664113998413 31.05876350402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792934775352478 2.3144748210906982 24.93768310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791916847229004 2.3676114082336426 25.468029022216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791923999786377 3.2309112548828125 34.101036071777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791769027709961 2.2274727821350098 24.066495895385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918192148208618 2.036576509475708 22.157583236694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927970886230469 2.286473274230957 24.657529830932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924308776855469 2.54364275932312 27.228858947753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793134331703186 2.15384578704834 23.331592559814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927452325820923 1.9739006757736206 21.53175163269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910326719284058 1.9162147045135498 20.95318031311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928237915039062 2.2347817420959473 24.140640258789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926493883132935 2.728454828262329 29.077198028564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925329208374023 2.4684560298919678 26.477092742919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923710346221924 1.9297269582748413 21.089641571044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915791273117065 2.2348458766937256 24.140037536621094
  batch 20 loss: 1.7915791273117065, 2.2348458766937256, 24.140037536621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925782203674316 2.1604855060577393 23.397432327270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.6154074668884277 27.946605682373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923418283462524 1.8196576833724976 19.98891830444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927993535995483 2.506509304046631 26.857892990112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924473285675049 2.687129020690918 28.663738250732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792594313621521 2.195876121520996 23.75135612487793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930119037628174 2.0472593307495117 22.265605926513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791948676109314 2.3538920879364014 25.330869674682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793518304824829 2.120593309402466 22.99945068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914464473724365 2.0016870498657227 21.808317184448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926502227783203 2.8920626640319824 30.71327781677246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791398286819458 2.380601167678833 25.597410202026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792543888092041 2.213655948638916 23.929101943969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792701244354248 2.097292184829712 22.765623092651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792622685432434 3.054365873336792 32.336280822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927433252334595 2.6457698345184326 28.250442504882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926435470581055 2.4679887294769287 26.472530364990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916394472122192 2.2455132007598877 24.24677085876465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921836376190186 2.516185998916626 26.954042434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917135953903198 3.1878292560577393 33.670005798339844
  batch 40 loss: 1.7917135953903198, 3.1878292560577393, 33.670005798339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792219877243042 2.7365336418151855 29.157556533813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924473285675049 2.3782312870025635 25.57476043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921221256256104 3.211338758468628 33.90550994873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915836572647095 2.3713295459747314 25.504878997802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925580739974976 2.2161078453063965 23.953638076782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922585010528564 2.229620933532715 24.088468551635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921085357666016 2.3408637046813965 25.200746536254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919355630874634 2.3823606967926025 25.615541458129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916580438613892 2.3945956230163574 25.73761558532715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792096495628357 2.508801221847534 26.880109786987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910726070404053 2.505138635635376 26.842458724975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924844026565552 2.429633855819702 26.088823318481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923173904418945 2.5412838459014893 27.205154418945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792664647102356 2.7341020107269287 29.133686065673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918870449066162 2.4258387088775635 26.050273895263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926113605499268 2.872687339782715 30.519485473632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792842984199524 2.7353997230529785 29.146839141845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928205728530884 1.9665334224700928 21.458154678344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932672500610352 2.483996629714966 26.63323211669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921726703643799 2.0461912155151367 22.254085540771484
  batch 60 loss: 1.7921726703643799, 2.0461912155151367, 22.254085540771484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928576469421387 2.1743083000183105 23.53594207763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916613817214966 2.3711330890655518 25.502992630004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926161289215088 2.4280378818511963 26.072994232177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922163009643555 2.392843723297119 25.720653533935547
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928128242492676 1.9763635396957397 21.556447982788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918838262557983 2.9487788677215576 31.279672622680664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918024063110352 3.0156984329223633 31.948787689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791581630706787 2.2459874153137207 24.251455307006836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918927669525146 2.1933162212371826 23.725055694580078
Total LOSS train 25.891408450786884 valid 27.80124282836914
CE LOSS train 1.792318529349107 valid 0.44797319173812866
Contrastive LOSS train 2.409908991593581 valid 0.5483290553092957
EPOCH 160:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922552824020386 2.5104188919067383 26.89644432067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915234565734863 3.747976541519165 39.27128982543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925554513931274 2.1480472087860107 23.273027420043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924178838729858 2.1273603439331055 23.066020965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921208143234253 2.57108998298645 27.503019332885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929385900497437 2.706786870956421 28.860807418823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919195890426636 2.5313827991485596 27.10574722290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79193115234375 2.1346118450164795 23.138050079345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917758226394653 2.187603235244751 23.667808532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918267250061035 2.3840267658233643 25.63209342956543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928073406219482 2.6433300971984863 28.226106643676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924394607543945 2.7079620361328125 28.872058868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931314706802368 2.2324960231781006 24.118091583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927440404891968 2.60675311088562 27.860275268554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910351753234863 2.5180246829986572 26.971281051635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928239107131958 2.649488925933838 28.28771209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792649745941162 2.4583630561828613 26.376279830932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925282716751099 2.2292113304138184 24.08464241027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923732995986938 1.86846125125885 20.476985931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915842533111572 2.1277151107788086 23.068735122680664
  batch 20 loss: 1.7915842533111572, 2.1277151107788086, 23.068735122680664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792577862739563 2.1804070472717285 23.596647262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925293445587158 2.3956854343414307 25.7493839263916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923352718353271 3.0921084880828857 32.71342086791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927985191345215 2.6307852268218994 28.100650787353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924493551254272 3.265998363494873 34.45243453979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925926446914673 2.2029805183410645 23.822397232055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930103540420532 2.272606372833252 24.519073486328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919477224349976 2.600689649581909 27.798845291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793513536453247 2.769005537033081 29.483570098876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914519309997559 2.3774569034576416 25.566020965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926461696624756 2.1150636672973633 22.943283081054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913954257965088 2.567257881164551 27.463973999023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925362586975098 2.275669574737549 24.549230575561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926894426345825 2.2334251403808594 24.126941680908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 2.480835437774658 26.600963592529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927325963974 2.32206392288208 25.013370513916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926403284072876 2.543210506439209 27.224746704101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916414737701416 2.0385000705718994 22.1766414642334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921884059906006 2.726630210876465 29.058490753173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917119264602661 2.8974173069000244 30.765884399414062
  batch 40 loss: 1.7917119264602661, 2.8974173069000244, 30.765884399414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922232151031494 2.2289559841156006 24.081783294677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924480438232422 2.1171107292175293 22.96355628967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921148538589478 2.407940149307251 25.87151527404785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915841341018677 2.0411722660064697 22.203306198120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925539016723633 1.8034368753433228 19.826923370361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922528982162476 2.330350399017334 25.09575843811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921065092086792 2.849994659423828 30.29205322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919336557388306 2.125020742416382 23.04214096069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791645884513855 2.537644386291504 27.168088912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792087435722351 2.1426970958709717 23.219058990478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910611629486084 2.7891180515289307 29.682241439819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792467474937439 2.480980396270752 26.602270126342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923060655593872 2.1893019676208496 23.685327529907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 2.672443389892578 28.517087936401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918779850006104 2.5169034004211426 26.96091079711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926125526428223 2.336639165878296 25.15900421142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928396463394165 2.861408233642578 30.40692138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792820692062378 2.1991915702819824 23.78473663330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932679653167725 2.6003377437591553 27.796646118164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792177677154541 2.953376054763794 31.325937271118164
  batch 60 loss: 1.792177677154541, 2.953376054763794, 31.325937271118164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928634881973267 2.3770928382873535 25.563791275024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916697263717651 2.32790207862854 25.070690155029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926238775253296 2.4733245372772217 26.525869369506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792227029800415 2.671858787536621 28.510814666748047
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792822241783142 2.150815486907959 23.30097770690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918940782546997 2.947451114654541 31.26640510559082
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918095588684082 2.8036038875579834 29.827848434448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915921211242676 2.766263723373413 29.4542293548584
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791901707649231 2.3621468544006348 25.41337013244629
Total LOSS train 26.29445938697228 valid 28.990463256835938
CE LOSS train 1.7923172583946816 valid 0.44797542691230774
Contrastive LOSS train 2.450214220927312 valid 0.5905367136001587
EPOCH 161:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792264461517334 2.5610992908477783 27.403257369995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915321588516235 2.479973316192627 26.591264724731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925596237182617 2.7033534049987793 28.826095581054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 2.2018840312957764 23.811260223388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921169996261597 2.7945518493652344 29.737634658813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929370403289795 2.219163417816162 23.98457145690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919114828109741 2.3529751300811768 25.32166290283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919254302978516 2.292410135269165 24.716026306152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917659282684326 2.118037223815918 22.972137451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918156385421753 2.1225268840789795 23.0170841217041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928022146224976 2.451256513595581 26.305368423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924399375915527 2.497901201248169 26.771451950073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793129563331604 2.177605390548706 23.569183349609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927439212799072 2.429457187652588 26.08731460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791035771369934 2.1318912506103516 23.109949111938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928210496902466 2.1224257946014404 23.017080307006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926541566848755 2.4045932292938232 25.838586807250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925342321395874 2.6532046794891357 28.324581146240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79237961769104 2.066894292831421 22.461322784423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915915250778198 2.100356340408325 22.795154571533203
  batch 20 loss: 1.7915915250778198, 2.100356340408325, 22.795154571533203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925788164138794 2.3250110149383545 25.04269027709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925314903259277 2.2121996879577637 23.91452980041504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923352718353271 2.3417418003082275 25.209753036499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792797327041626 2.7320899963378906 29.113697052001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924463748931885 2.8376259803771973 30.168704986572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925866842269897 2.4297666549682617 26.090253829956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930026054382324 2.2388994693756104 24.181997299194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919423580169678 2.0539355278015137 22.331298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7935131788253784 2.4984090328216553 26.777603149414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914576530456543 2.3291757106781006 25.083215713500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926435470581055 2.7644097805023193 29.43674087524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791391372680664 2.3282692432403564 25.07408332824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925292253494263 2.3282129764556885 25.07465934753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79268479347229 2.239875316619873 24.191438674926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926101684570312 2.6067938804626465 27.860549926757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927314043045044 2.31589412689209 24.95167350769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926357984542847 2.5261993408203125 27.054628372192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916364669799805 2.115495204925537 22.946590423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921825647354126 2.3849120140075684 25.64130401611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917060852050781 2.7781591415405273 29.57329750061035
  batch 40 loss: 1.7917060852050781, 2.7781591415405273, 29.57329750061035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922179698944092 2.3467774391174316 25.259990692138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792446255683899 2.022045135498047 22.012897491455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792115569114685 2.4479432106018066 26.271547317504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915863990783691 2.3578200340270996 25.369787216186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925541400909424 2.287646770477295 24.669023513793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792250156402588 2.4005167484283447 25.79741668701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921065092086792 1.9099253416061401 20.891359329223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919312715530396 2.078178644180298 22.57371711730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916454076766968 2.1442065238952637 23.23371124267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920869588851929 2.1809232234954834 23.601320266723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910596132278442 2.1783576011657715 23.574636459350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792466640472412 2.7108497619628906 28.900964736938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923099994659424 1.7303507328033447 19.09581756591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926608324050903 1.8483250141143799 20.275911331176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918860912322998 2.5048537254333496 26.840423583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926130294799805 2.7151103019714355 28.94371795654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928352355957031 2.4556875228881836 26.34971046447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928106784820557 2.0627951622009277 22.42076301574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932542562484741 2.678661584854126 28.579870223999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921665906906128 2.259047508239746 24.382640838623047
  batch 60 loss: 1.7921665906906128, 2.259047508239746, 24.382640838623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792845368385315 2.0606484413146973 22.399328231811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916584014892578 2.2707650661468506 24.499309539794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 2.1974363327026367 23.7669734954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922117710113525 2.2813682556152344 24.605894088745117
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928086519241333 2.435532569885254 26.148134231567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791884183883667 2.8488523960113525 30.280406951904297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918004989624023 2.536351442337036 27.155315399169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915809154510498 2.4223949909210205 26.01552963256836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 2.009308099746704 21.88497543334961
Total LOSS train 25.151454837505636 valid 26.334056854248047
CE LOSS train 1.7923147861774151 valid 0.4479736387729645
Contrastive LOSS train 2.3359139864261333 valid 0.502327024936676
EPOCH 162:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79225754737854 2.1369740962982178 23.161998748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791523814201355 2.9668450355529785 31.459972381591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925472259521484 2.4666435718536377 26.458982467651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924137115478516 2.220132350921631 23.993738174438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921136617660522 2.855935573577881 30.351470947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929381132125854 2.98002552986145 31.59319305419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919135093688965 2.4648287296295166 26.440200805664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919301986694336 2.1373507976531982 23.16543960571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917686700820923 2.484543561935425 26.637205123901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918120622634888 2.275355339050293 24.545366287231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927961349487305 2.714341878890991 28.936214447021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792434573173523 2.6137685775756836 27.93012046813965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931184768676758 2.0950608253479004 22.743724822998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792734980583191 2.349642038345337 25.289154052734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910321950912476 2.253157377243042 24.322607040405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928162813186646 2.2698071002960205 24.490886688232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926474809646606 2.0554306507110596 22.346954345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925246953964233 2.169964551925659 23.492170333862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923716306686401 2.555270195007324 27.345073699951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915834188461304 2.551882743835449 27.31041145324707
  batch 20 loss: 1.7915834188461304, 2.551882743835449, 27.31041145324707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925649881362915 1.9417928457260132 21.210493087768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925211191177368 2.481441020965576 26.606931686401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923243045806885 2.8328356742858887 30.120681762695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927913665771484 2.6792266368865967 28.585058212280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924435138702393 2.5448107719421387 27.24055290222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792588472366333 2.2674145698547363 24.466733932495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930030822753906 2.3053858280181885 24.846860885620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791934609413147 2.1697804927825928 23.4897403717041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793500304222107 2.2202908992767334 23.996410369873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914516925811768 2.386577844619751 25.657230377197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79264235496521 2.547576427459717 27.268407821655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791390299797058 2.8752760887145996 30.544151306152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523980140686 2.3658597469329834 25.451122283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926744222640991 2.737316370010376 29.16583824157715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925975322723389 2.887230396270752 30.664899826049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927170991897583 2.292876958847046 24.721487045288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792625069618225 2.1743927001953125 23.53655242919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791634440422058 2.433774709701538 26.12938117980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921863794326782 2.3975088596343994 25.767274856567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917139530181885 2.6385042667388916 28.176755905151367
  batch 40 loss: 1.7917139530181885, 2.6385042667388916, 28.176755905151367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922297716140747 2.3604044914245605 25.396276473999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924550771713257 2.010507106781006 21.89752769470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921192646026611 2.208326578140259 23.875385284423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915886640548706 3.337941884994507 35.17100524902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925503253936768 2.4979755878448486 26.772306442260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922438383102417 2.304656744003296 24.83881187438965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792100191116333 2.2140345573425293 23.93244743347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919282913208008 2.2021892070770264 23.813819885253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916431427001953 2.176893711090088 23.560579299926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920888662338257 2.403076648712158 25.82285499572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910592555999756 2.7748641967773438 29.539701461791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792460560798645 2.373792886734009 25.5303897857666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923003435134888 2.344223737716675 25.234539031982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926502227783203 2.31638240814209 24.95647430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918713092803955 2.431335210800171 26.105224609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926028966903687 1.8943201303482056 20.735803604125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928248643875122 2.452770709991455 26.320531845092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928067445755005 2.168149471282959 23.474302291870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793259620666504 2.4074180126190186 25.86743927001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921764850616455 2.5249948501586914 27.042125701904297
  batch 60 loss: 1.7921764850616455, 2.5249948501586914, 27.042125701904297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928612232208252 2.4038233757019043 25.831096649169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916755676269531 2.5997743606567383 27.789419174194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926262617111206 2.4507696628570557 26.300321578979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792222023010254 2.4314539432525635 26.106761932373047
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928122282028198 1.8069316148757935 19.86212921142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918857336044312 2.750850200653076 29.30038833618164
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917999029159546 2.969113349914551 31.482933044433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915817499160767 2.7089080810546875 28.88066291809082
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918962240219116 2.195888042449951 23.750778198242188
Total LOSS train 25.92982653104342 valid 28.35369062423706
CE LOSS train 1.7923117600954497 valid 0.4479740560054779
Contrastive LOSS train 2.4137514572877152 valid 0.5489720106124878
EPOCH 163:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922582626342773 2.449972629547119 26.29198455810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79152512550354 2.6010007858276367 27.801532745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925485372543335 2.342350959777832 25.2160587310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924168109893799 2.5382187366485596 27.174604415893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921149730682373 2.1915359497070312 23.707473754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929390668869019 2.8238868713378906 30.03180694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919082641601562 2.9925405979156494 31.717313766479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919254302978516 2.625190496444702 28.04383087158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917615175247192 2.249222755432129 24.28398895263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918052673339844 2.178410053253174 23.575904846191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927958965301514 2.5133020877838135 26.92581558227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924370765686035 2.456191062927246 26.354347229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79310941696167 2.608142852783203 27.87453842163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927275896072388 1.880203127861023 20.594758987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910282611846924 2.766862630844116 29.45965576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928110361099243 3.14491605758667 33.241973876953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926431894302368 2.2962772846221924 24.755414962768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925186157226562 2.3310492038726807 25.103010177612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923740148544312 2.1706736087799072 23.499109268188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915867567062378 2.3329806327819824 25.12139320373535
  batch 20 loss: 1.7915867567062378, 2.3329806327819824, 25.12139320373535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925657033920288 2.059236764907837 22.384933471679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925204038619995 2.6802141666412354 28.594661712646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923213243484497 2.2789089679718018 24.581411361694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927919626235962 2.1946613788604736 23.739404678344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792445182800293 2.4396631717681885 26.189075469970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925876379013062 3.0619659423828125 32.41224670410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930001020431519 2.6693625450134277 28.48662567138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919337749481201 2.1130456924438477 22.92238998413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934989929199219 2.442110300064087 26.214601516723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914586067199707 2.310089349746704 24.892351150512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926397323608398 2.6533799171447754 28.326438903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913875579833984 2.878491163253784 30.5762996673584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925186157226562 2.0877819061279297 22.670337677001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79267156124115 2.649249315261841 28.28516387939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926025390625 3.227581262588501 34.068416595458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927261590957642 2.468170166015625 26.474428176879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926356792449951 2.246511936187744 24.257753372192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916419506072998 2.1582493782043457 23.374134063720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792189121246338 2.5752570629119873 27.54475975036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791704773902893 2.2765984535217285 24.557687759399414
  batch 40 loss: 1.791704773902893, 2.2765984535217285, 24.557687759399414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922182083129883 2.3726730346679688 25.51894760131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924472093582153 2.316150188446045 24.953950881958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792109489440918 2.313138484954834 24.92349624633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915886640548706 2.274808406829834 24.5396728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792551875114441 2.1617207527160645 23.409757614135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922406196594238 2.5118329524993896 26.91057014465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921005487442017 2.5834052562713623 27.62615394592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919278144836426 2.4485292434692383 26.277219772338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916405200958252 2.847870349884033 30.270343780517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920833826065063 2.5305001735687256 27.09708595275879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910517454147339 2.534703493118286 27.138086318969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924498319625854 1.8517261743545532 20.309711456298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922950983047485 1.9923936128616333 21.716232299804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792651653289795 2.432537078857422 26.118022918701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918777465820312 1.929219126701355 21.084068298339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926126718521118 1.7542979717254639 19.33559226989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792829155921936 2.212515115737915 23.917980194091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928093671798706 2.1288673877716064 23.08148193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932590246200562 2.8250834941864014 30.04409408569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921744585037231 2.224482297897339 24.036998748779297
  batch 60 loss: 1.7921744585037231, 2.224482297897339, 24.036998748779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928528785705566 2.3561830520629883 25.35468292236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791671633720398 2.070887565612793 22.500547409057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792618989944458 2.1703388690948486 23.496007919311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922130823135376 2.36116099357605 25.403823852539062
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7928035259246826 1.4831598997116089 16.62440299987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918835878372192 2.6532773971557617 28.324657440185547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917993068695068 2.3379621505737305 25.17142105102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915794849395752 2.2361867427825928 24.1534481048584
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918980121612549 2.1934280395507812 23.726179122924805
Total LOSS train 25.738716477614183 valid 25.343926429748535
CE LOSS train 1.7923100874974178 valid 0.4479745030403137
Contrastive LOSS train 2.394640649282015 valid 0.5483570098876953
EPOCH 164:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922582626342773 2.34161114692688 25.208370208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791529893875122 2.701591730117798 28.80744743347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925561666488647 1.8368171453475952 20.160728454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924249172210693 2.0958092212677 22.750516891479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921171188354492 2.405545949935913 25.847576141357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929373979568481 2.2184038162231445 23.97697639465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79189932346344 1.8650901317596436 20.442800521850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919199466705322 2.4413859844207764 26.205780029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917582988739014 2.2393782138824463 24.18553924560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791804313659668 2.2312448024749756 24.104251861572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792798638343811 2.44305157661438 26.22331428527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924424409866333 2.27367901802063 24.529232025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931177616119385 2.195021629333496 23.74333381652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927391529083252 2.3903441429138184 25.696182250976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910408973693848 2.046562671661377 22.25666618347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928190231323242 2.050391912460327 22.296737670898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792649745941162 2.441467046737671 26.207321166992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925208806991577 2.5069901943206787 26.862422943115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 2.1127829551696777 22.92020606994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915891408920288 2.666844606399536 28.46003532409668
  batch 20 loss: 1.7915891408920288, 2.666844606399536, 28.46003532409668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792567253112793 2.2691047191619873 24.48361587524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925245761871338 2.5342695713043213 27.13521957397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923187017440796 2.8667943477630615 30.460262298583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927864789962769 2.6440935134887695 28.233720779418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924387454986572 2.3981692790985107 25.774131774902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925758361816406 2.176771879196167 23.56029510498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929892539978027 2.9321892261505127 31.11488151550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919285297393799 2.2984044551849365 24.77597427368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934995889663696 2.3168206214904785 24.96170425415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791464924812317 2.090791940689087 22.699384689331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926462888717651 2.517315626144409 26.965803146362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79139244556427 2.471719264984131 26.508586883544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925208806991577 2.4296069145202637 26.088590621948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926706075668335 2.1965994834899902 23.7586669921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925970554351807 2.2636172771453857 24.428770065307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927192449569702 2.085523843765259 22.64795684814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926284074783325 2.2555558681488037 24.348188400268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916346788406372 2.527357578277588 27.065210342407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921842336654663 2.9834465980529785 31.626649856567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916994094848633 2.121114492416382 23.002845764160156
  batch 40 loss: 1.7916994094848633, 2.121114492416382, 23.002845764160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792217493057251 2.692493200302124 28.71714973449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792444109916687 2.0390055179595947 22.182498931884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921035289764404 2.5605969429016113 27.3980712890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915815114974976 2.4882051944732666 26.673633575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925406694412231 2.632350206375122 28.116043090820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922346591949463 2.087400197982788 22.666236877441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920987606048584 2.4911065101623535 26.703163146972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791932463645935 2.3038840293884277 24.830774307250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916440963745117 2.0825116634368896 22.61676025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920901775360107 2.217472553253174 23.966814041137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910566329956055 2.4658687114715576 26.449745178222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792449712753296 2.0231308937072754 22.023757934570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792292594909668 2.046135425567627 22.253646850585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926442623138428 2.3892359733581543 25.68500518798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918699979782104 2.6436452865600586 28.228322982788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926082611083984 2.6940438747406006 28.733047485351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792824387550354 2.5899980068206787 27.69280433654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928003072738647 2.223158597946167 24.02438735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932449579238892 2.7188591957092285 28.981836318969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792162537574768 2.3369221687316895 25.1613826751709
  batch 60 loss: 1.792162537574768, 2.3369221687316895, 25.1613826751709
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928340435028076 2.915698289871216 30.94981575012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916566133499146 1.9811049699783325 21.602706909179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792602300643921 2.3248281478881836 25.040884017944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921992540359497 2.6075546741485596 27.867746353149414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792797327041626 1.9724897146224976 21.5176944732666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918825149536133 2.541585683822632 27.207740783691406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917981147766113 2.471038818359375 26.502185821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791577935218811 2.6372838020324707 28.16441535949707
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918986082077026 2.327188014984131 25.063779830932617
Total LOSS train 25.393997339101936 valid 26.734530448913574
CE LOSS train 1.792307945398184 valid 0.44797465205192566
Contrastive LOSS train 2.360168928366441 valid 0.5817970037460327
EPOCH 165:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922570705413818 2.586874485015869 27.661001205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915281057357788 2.5294759273529053 27.086288452148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925519943237305 2.3349053859710693 25.141605377197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792423963546753 2.1249260902404785 23.041683197021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921218872070312 2.2354609966278076 24.146732330322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929476499557495 2.1659739017486572 23.452686309814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919071912765503 2.461846351623535 26.410369873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919251918792725 2.245655059814453 24.248476028442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917543649673462 2.1147685050964355 22.93943977355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917919158935547 2.1142454147338867 22.934246063232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927829027175903 2.613144874572754 27.924232482910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924299240112305 2.9908652305603027 31.70108413696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793094277381897 2.1435446739196777 23.22854232788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927160263061523 2.6217451095581055 28.01016616821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910211086273193 2.5134265422821045 26.9252872467041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927978038787842 2.447716236114502 26.26995849609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926348447799683 2.4411141872406006 26.203777313232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792507529258728 2.7714362144470215 29.50687026977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792363166809082 2.200026273727417 23.792625427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791578769683838 2.2337918281555176 24.12949562072754
  batch 20 loss: 1.791578769683838, 2.2337918281555176, 24.12949562072754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925570011138916 2.5153417587280273 26.945974349975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925164699554443 2.024765729904175 22.04017448425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923146486282349 2.810908317565918 29.901397705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927882671356201 2.301164150238037 24.80443000793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792445421218872 2.7370681762695312 29.163127899169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925832271575928 2.1475210189819336 23.267793655395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929940223693848 2.331005573272705 25.10304832458496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791930913925171 2.1829211711883545 23.621143341064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934939861297607 2.3377773761749268 25.171266555786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914584875106812 2.032226800918579 22.113725662231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926349639892578 2.473141670227051 26.524051666259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913888692855835 2.355234384536743 25.343732833862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925180196762085 2.5911905765533447 27.704423904418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926664352416992 2.6872036457061768 28.664703369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925951480865479 2.519915819168091 26.99175262451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 3.110583543777466 32.8985481262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926253080368042 2.604964256286621 27.842267990112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916340827941895 3.0408239364624023 32.19987487792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921844720840454 2.849517822265625 30.287363052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917011976242065 2.370910406112671 25.500804901123047
  batch 40 loss: 1.7917011976242065, 2.370910406112671, 25.500804901123047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922168970108032 2.3048133850097656 24.840351104736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792445182800293 2.1911368370056152 23.703815460205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921077013015747 2.486938714981079 26.661495208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915898561477661 2.3898544311523438 25.690134048461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925512790679932 2.4040207862854004 25.8327579498291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922428846359253 2.2627761363983154 24.42000389099121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792102575302124 1.9731392860412598 21.523494720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919352054595947 2.531493663787842 27.10687255859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916464805603027 2.2153427600860596 23.9450740814209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920926809310913 1.7330050468444824 19.12214469909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79105806350708 2.3734352588653564 25.525409698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924509048461914 2.4122841358184814 25.91529083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922911643981934 1.8680802583694458 20.473094940185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926411628723145 2.99886155128479 31.7812557220459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918676137924194 2.4638311862945557 26.430179595947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926025390625 2.4326462745666504 26.119064331054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928223609924316 2.6321446895599365 28.114269256591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928019762039185 2.4851653575897217 26.64445686340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793251633644104 2.4476399421691895 26.269649505615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792169213294983 2.3123648166656494 24.915817260742188
  batch 60 loss: 1.792169213294983, 2.3123648166656494, 24.915817260742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792838454246521 2.305727243423462 24.85011100769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916637659072876 2.275291681289673 24.544580459594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926090955734253 2.1412806510925293 23.205415725708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922022342681885 2.2625889778137207 24.4180908203125
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927956581115723 1.8126749992370605 19.919546127319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791879415512085 2.4019861221313477 25.81174087524414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791795015335083 2.245511531829834 24.246912002563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915747165679932 2.2479894161224365 24.271469116210938
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918955087661743 1.8455109596252441 20.24700355529785
Total LOSS train 25.735639220017653 valid 23.6442813873291
CE LOSS train 1.792305797796983 valid 0.4479738771915436
Contrastive LOSS train 2.394333346073444 valid 0.46137773990631104
EPOCH 166:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922534942626953 2.18350887298584 23.627342224121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915229797363281 3.096419334411621 32.755714416503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925426959991455 2.24609637260437 24.253507614135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924151420593262 2.2210564613342285 24.002979278564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921130657196045 2.939220666885376 31.18431854248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929389476776123 2.3921010494232178 25.71394920349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 2.4452619552612305 26.244522094726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919219732284546 2.55997896194458 27.39171028137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917561531066895 1.957949161529541 21.371246337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917956113815308 2.199225425720215 23.78404998779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927899360656738 2.37863826751709 25.579172134399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924388647079468 3.0291872024536133 32.084312438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931065559387207 2.256509304046631 24.358200073242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927285432815552 2.5597152709960938 27.389881134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910337448120117 2.186353921890259 23.654571533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928078174591064 2.6285367012023926 28.078174591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926405668258667 2.3288042545318604 25.0806827545166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925091981887817 2.828871726989746 30.081226348876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923623323440552 1.9007971286773682 20.80033302307129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915762662887573 2.1183362007141113 22.974937438964844
  batch 20 loss: 1.7915762662887573, 2.1183362007141113, 22.974937438964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925517559051514 2.569460391998291 27.487154006958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925111055374146 2.990518093109131 31.69769287109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792309284210205 2.0573182106018066 22.365489959716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927846908569336 2.4525794982910156 26.318580627441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924429178237915 3.3194327354431152 34.98677062988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925840616226196 2.381474733352661 25.607330322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792993426322937 2.4588587284088135 26.381580352783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919310331344604 2.0795814990997314 22.587745666503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934930324554443 2.629988670349121 28.093379974365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79146409034729 2.334340810775757 25.134872436523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792639136314392 2.367263078689575 25.465269088745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913893461227417 2.6334636211395264 28.126026153564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925161123275757 2.5424587726593018 27.217103958129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792667031288147 2.165252923965454 23.4451961517334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792601466178894 2.93947696685791 31.18737030029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792723298072815 2.193727731704712 23.729999542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926311492919922 2.169992208480835 23.4925537109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791635274887085 2.4164183139801025 25.95581817626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921807765960693 2.911245822906494 30.904638290405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916944026947021 2.8590803146362305 30.382497787475586
  batch 40 loss: 1.7916944026947021, 2.8590803146362305, 30.382497787475586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922120094299316 2.3152101039886475 24.944313049316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439579963684 1.9903786182403564 21.696226119995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920994758605957 2.41621470451355 25.954246520996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79158353805542 2.326376438140869 25.055347442626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925410270690918 2.2058496475219727 23.851037979125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922316789627075 2.2468984127044678 24.26121711730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920947074890137 2.4022297859191895 25.81439208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919299602508545 2.101809024810791 22.81001853942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916383743286133 2.335421323776245 25.145851135253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920846939086914 2.1362831592559814 23.15491485595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910524606704712 2.361250638961792 25.4035587310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924456596374512 2.3415884971618652 25.208332061767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922914028167725 2.1113169193267822 22.905460357666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926450967788696 2.263737201690674 24.430015563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918721437454224 2.347162961959839 25.26350212097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926020622253418 2.1127941608428955 22.920543670654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792815089225769 2.4452128410339355 26.244943618774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927926778793335 2.5468966960906982 27.261760711669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932409048080444 2.591980218887329 27.713043212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921619415283203 2.150423526763916 23.296396255493164
  batch 60 loss: 1.7921619415283203, 2.150423526763916, 23.296396255493164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928338050842285 2.174584150314331 23.53867530822754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916613817214966 2.2459373474121094 24.251035690307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926005125045776 2.3180105686187744 24.972705841064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921898365020752 2.3361761569976807 25.15395164489746
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927825450897217 1.5767325162887573 17.560108184814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918773889541626 2.1177196502685547 22.969074249267578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917925119400024 2.0630128383636475 22.421920776367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915730476379395 1.7804532051086426 19.59610366821289
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918949127197266 1.7890610694885254 19.682504653930664
Total LOSS train 25.689069219735952 valid 21.16740083694458
CE LOSS train 1.7923037198873666 valid 0.44797372817993164
Contrastive LOSS train 2.389676569058345 valid 0.44726526737213135
Saved best model. Old loss 21.606393337249756 and new best loss 21.16740083694458
EPOCH 167:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792252540588379 2.0603220462799072 22.39547348022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915282249450684 2.568878650665283 27.480314254760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925524711608887 2.281634569168091 24.608898162841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924261093139648 2.4142861366271973 25.935287475585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921215295791626 2.5793004035949707 27.585124969482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929452657699585 2.5567471981048584 27.36041831970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791902780532837 2.4489099979400635 26.281002044677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919232845306396 2.8792498111724854 30.584421157836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791756272315979 2.1326417922973633 23.118173599243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917943000793457 2.2770156860351562 24.56195068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927886247634888 2.226619243621826 24.058982849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924408912658691 2.4412734508514404 26.205175399780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931005954742432 1.964330792427063 21.43640899658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927219867706299 2.5348284244537354 27.141006469726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910306453704834 2.3715198040008545 25.506229400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928012609481812 2.137415885925293 23.166959762573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926353216171265 2.101370096206665 22.80633544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925080060958862 2.7194101810455322 28.986608505249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79236900806427 2.1200411319732666 22.992780685424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915884256362915 3.029975414276123 32.09134292602539
  batch 20 loss: 1.7915884256362915, 3.029975414276123, 32.09134292602539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925608158111572 2.532738447189331 27.119945526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925201654434204 2.159829616546631 23.390817642211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923177480697632 2.2156662940979004 23.9489803314209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927945852279663 2.578925371170044 27.582048416137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924476861953735 2.6095855236053467 27.888303756713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792584776878357 2.4879672527313232 26.672258377075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929930686950684 2.559647560119629 27.389469146728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791930079460144 2.116422414779663 22.956153869628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934898138046265 1.946432113647461 21.257810592651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914665937423706 2.0647647380828857 22.43911361694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926363945007324 2.802109718322754 29.81373405456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791385531425476 2.235511064529419 24.146495819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925090789794922 1.7948602437973022 19.741111755371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926584482192993 2.301616907119751 24.808826446533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925912141799927 2.6920764446258545 28.713356018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927119731903076 2.909053087234497 30.883241653442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926244735717773 2.115422010421753 22.94684600830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916367053985596 2.257266044616699 24.36429786682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921847105026245 2.608619213104248 27.87837791442871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916945219039917 2.6348767280578613 28.140460968017578
  batch 40 loss: 1.7916945219039917, 2.6348767280578613, 28.140460968017578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922132015228271 2.677873373031616 28.570947647094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924422025680542 2.0858747959136963 22.65118980407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920995950698853 2.441929578781128 26.211395263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915847301483154 2.2486026287078857 24.277610778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925398349761963 2.229144334793091 24.083982467651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922309637069702 2.1727335453033447 23.51956558227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792099118232727 2.79390549659729 29.73115348815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791931390762329 2.605830669403076 27.850238800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916383743286133 2.453209400177002 26.32373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920840978622437 2.2423958778381348 24.216041564941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910544872283936 2.2109711170196533 23.90076446533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792447566986084 2.5471599102020264 27.264047622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922948598861694 2.294452667236328 24.7368221282959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926450967788696 2.7396390438079834 29.189035415649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918661832809448 2.5908799171447754 27.700664520263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926045656204224 2.6877543926239014 28.670148849487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928136587142944 2.3195183277130127 24.98799705505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792793869972229 2.0987234115600586 22.780027389526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932426929473877 2.711336374282837 28.906606674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921655178070068 2.6904289722442627 28.696455001831055
  batch 60 loss: 1.7921655178070068, 2.6904289722442627, 28.696455001831055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928345203399658 2.530722141265869 27.100055694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916665077209473 2.3209075927734375 25.000741958618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926057577133179 2.4059665203094482 25.852272033691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921937704086304 2.8216710090637207 30.00890350341797
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927831411361694 2.4437921047210693 26.23070526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918750047683716 2.342026710510254 25.212142944335938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917898893356323 2.415738582611084 25.949176788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915682792663574 2.1129674911499023 22.92124366760254
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918944358825684 2.0400424003601074 22.192319869995117
Total LOSS train 25.920702274029072 valid 24.068720817565918
CE LOSS train 1.7923046405498797 valid 0.4479736089706421
Contrastive LOSS train 2.4128397648151103 valid 0.5100106000900269
EPOCH 168:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922511100769043 2.469585657119751 26.488107681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915220260620117 2.9148824214935303 30.940345764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925385236740112 1.7827433347702026 19.619970321655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924166917800903 2.1598994731903076 23.39141273498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792114019393921 2.2037596702575684 23.8297119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929438352584839 2.3091769218444824 24.884714126586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791902780532837 2.261612892150879 24.408031463623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919282913208008 2.1120216846466064 22.91214370727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917624711990356 1.9850984811782837 21.64274787902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918035984039307 2.2380568981170654 24.172372817993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927988767623901 2.5605180263519287 27.397979736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792447805404663 2.4256467819213867 26.04891586303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793104887008667 2.2648582458496094 24.441686630249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792728304862976 2.4766900539398193 26.559629440307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910432815551758 2.6462297439575195 28.253341674804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792807698249817 2.4126181602478027 25.918991088867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926421165466309 2.3013598918914795 24.806241989135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79250967502594 2.426783323287964 26.06034278869629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923743724822998 2.385749101638794 25.649864196777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915897369384766 2.0862245559692383 22.65383529663086
  batch 20 loss: 1.7915897369384766, 2.0862245559692383, 22.65383529663086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556643486023 2.6460912227630615 28.253469467163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925173044204712 2.3489861488342285 25.282377243041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792305588722229 2.4830784797668457 26.623088836669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927799224853516 2.236875057220459 24.161531448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924365997314453 2.5426368713378906 27.21880531311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925695180892944 2.409679412841797 25.88936424255371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79297935962677 2.9728822708129883 31.52180290222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919188737869263 2.2424232959747314 24.21615219116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934825420379639 2.6021621227264404 27.81510353088379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914700508117676 2.295438051223755 24.745849609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792640209197998 2.6936585903167725 28.72922706604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913941144943237 2.3125967979431152 24.917362213134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925180196762085 2.2723886966705322 24.51640510559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926639318466187 2.4957308769226074 26.74997329711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925925254821777 2.0999443531036377 22.792036056518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927111387252808 2.2409653663635254 24.202363967895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926182746887207 2.2355308532714844 24.147926330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791630506515503 2.2107386589050293 23.899017333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921799421310425 2.7277700901031494 29.06987953186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916929721832275 2.6048738956451416 27.840431213378906
  batch 40 loss: 1.7916929721832275, 2.6048738956451416, 27.840431213378906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792215347290039 2.477297306060791 26.565187454223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924448251724243 2.2539477348327637 24.33192253112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792100191116333 2.449148654937744 26.283586502075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915918827056885 2.2806484699249268 24.59807586669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 2.5868897438049316 27.66143798828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922269105911255 2.1386795043945312 23.17902183532715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920936346054077 2.0173044204711914 21.965137481689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919256687164307 2.169346332550049 23.485387802124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791637897491455 2.199185609817505 23.783493041992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920867204666138 2.5724120140075684 27.51620864868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791057825088501 2.3183014392852783 24.974071502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924492359161377 2.235405445098877 24.146503448486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923004627227783 2.0031418800354004 21.823719024658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926514148712158 2.673572063446045 28.52837371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918756008148193 2.232219934463501 24.11407470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792611002922058 2.1246750354766846 23.03936004638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792822003364563 2.3916282653808594 25.709104537963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927972078323364 2.5242843627929688 27.035640716552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932418584823608 2.502924919128418 26.822490692138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792165994644165 2.030041217803955 22.09257698059082
  batch 60 loss: 1.792165994644165, 2.030041217803955, 22.09257698059082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928359508514404 2.668888568878174 28.481719970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916675806045532 2.3639883995056152 25.43155288696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926057577133179 2.251593589782715 24.308542251586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921956777572632 2.350701332092285 25.299209594726562
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927864789962769 1.5759028196334839 17.551815032958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79188072681427 2.4022839069366455 25.814720153808594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791793942451477 2.3065812587738037 24.857606887817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915730476379395 2.371730089187622 25.508872985839844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900873184204 1.8814858198165894 20.60675811767578
Total LOSS train 25.25231951200045 valid 24.1969895362854
CE LOSS train 1.7923048881384043 valid 0.447975218296051
Contrastive LOSS train 2.346001469171964 valid 0.47037145495414734
EPOCH 169:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922557592391968 2.161069393157959 23.402950286865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791528344154358 2.549912929534912 27.290658950805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925405502319336 2.427553415298462 26.068073272705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924137115478516 1.8108158111572266 19.900571823120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921063899993896 1.9644129276275635 21.436235427856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929362058639526 3.1342577934265137 33.13551330566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918986082077026 2.1569180488586426 23.3610782623291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791927456855774 2.364231586456299 25.434242248535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917613983154297 2.292722225189209 24.718984603881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917958498001099 2.1156609058380127 22.94840431213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927868366241455 2.447460889816284 26.267396926879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792436122894287 2.542503595352173 27.217472076416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930887937545776 2.7855136394500732 29.648225784301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792716145515442 2.5755422115325928 27.548139572143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910382747650146 2.3603196144104004 25.39423370361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928094863891602 2.683359384536743 28.62640380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926480770111084 2.318706512451172 24.979713439941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925139665603638 2.215327501296997 23.945789337158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923738956451416 2.244600772857666 24.238380432128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915868759155273 2.6199545860290527 27.991134643554688
  batch 20 loss: 1.7915868759155273, 2.6199545860290527, 27.991134643554688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925487756729126 2.092440366744995 22.71695327758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792506456375122 2.526672601699829 27.059232711791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922968864440918 2.025320529937744 22.045501708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927746772766113 2.2178568840026855 23.971343994140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924350500106812 2.903928518295288 30.83172035217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925690412521362 2.0758466720581055 22.551034927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792978048324585 2.148207426071167 23.275053024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919204235076904 2.567054033279419 27.462459564208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934842109680176 2.8852827548980713 30.646310806274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914732694625854 2.615513563156128 27.946609497070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926408052444458 2.8241612911224365 30.03425407409668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791395664215088 2.4704596996307373 26.49599266052246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925153970718384 2.420412302017212 25.99663734436035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926583290100098 2.5548195838928223 27.340852737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925859689712524 2.523582935333252 27.028413772583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927019596099854 2.2950093746185303 24.742795944213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792611002922058 2.307133913040161 24.863948822021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916268110275269 2.1209044456481934 23.00067138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921754121780396 2.830437421798706 30.09654998779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916839122772217 2.2374424934387207 24.166107177734375
  batch 40 loss: 1.7916839122772217, 2.2374424934387207, 24.166107177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79220712184906 2.6905343532562256 28.697551727294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924367189407349 2.0521843433380127 22.314279556274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920938730239868 2.409935235977173 25.89144515991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915892601013184 2.2601730823516846 24.393320083618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 2.5921988487243652 27.71453094482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792227029800415 2.3773813247680664 25.5660400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920936346054077 2.183655261993408 23.628644943237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919248342514038 2.0826892852783203 22.618818283081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916306257247925 2.1763863563537598 23.555492401123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920799255371094 2.2684426307678223 24.476505279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910492420196533 2.3324174880981445 25.115224838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924368381500244 2.412719964981079 25.919635772705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922866344451904 2.123711585998535 23.029401779174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792639136314392 3.0196166038513184 31.988805770874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918623685836792 2.4342613220214844 26.134475708007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926021814346313 2.0118460655212402 21.91106414794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928117513656616 2.5659332275390625 27.452144622802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927913665771484 2.59040904045105 27.696882247924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932440042495728 2.66746187210083 28.46786117553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170524597168 2.7779934406280518 29.572105407714844
  batch 60 loss: 1.792170524597168, 2.7779934406280518, 29.572105407714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928348779678345 2.990504503250122 31.697879791259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916666269302368 2.6000452041625977 27.792118072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926026582717896 2.3327829837799072 25.120431900024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921903133392334 2.772587537765503 29.51806640625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927836179733276 1.8755242824554443 20.54802703857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918848991394043 2.8293397426605225 30.085283279418945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917983531951904 2.6994926929473877 28.786724090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915772199630737 2.4245798587799072 26.03737449645996
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919069528579712 2.514796257019043 26.939868927001953
Total LOSS train 25.948412293654222 valid 27.962312698364258
CE LOSS train 1.7923006424537071 valid 0.4479767382144928
Contrastive LOSS train 2.4156111753903904 valid 0.6286990642547607
EPOCH 170:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922605276107788 2.76940655708313 29.486326217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79153311252594 2.712911367416382 28.92064666748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792546033859253 2.0774614810943604 22.56715965270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417287826538 2.3085198402404785 24.877614974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921053171157837 2.52530837059021 27.045188903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929317951202393 2.4182851314544678 25.975784301757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918847799301147 3.867135524749756 40.46324157714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919126749038696 2.0326578617095947 22.11849021911621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791743516921997 2.3066036701202393 24.85778045654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917802333831787 2.1291463375091553 23.08324432373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927831411361694 2.603567600250244 27.828458786010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79244065284729 2.509114980697632 26.883590698242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930898666381836 2.3137009143829346 24.930099487304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927148342132568 2.7277605533599854 29.07032012939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910341024398804 2.580049753189087 27.59153175354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792798638343811 2.406764507293701 25.860445022583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926346063613892 2.199331521987915 23.78594970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925020456314087 2.4377219676971436 26.169721603393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923660278320312 1.9301427602767944 21.093793869018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915866374969482 2.2432119846343994 24.223705291748047
  batch 20 loss: 1.7915866374969482, 2.2432119846343994, 24.223705291748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925474643707275 2.123180627822876 23.02435302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925128936767578 2.2495110034942627 24.287622451782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923039197921753 2.1263556480407715 23.05586051940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927807569503784 2.252164602279663 24.31442642211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.7498421669006348 29.29085922241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925704717636108 2.271416187286377 24.506731033325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929754257202148 2.6899213790893555 28.692188262939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791911005973816 2.2941958904266357 24.733869552612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79346764087677 2.820659637451172 30.000064849853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914645671844482 2.556867837905884 27.36014175415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926291227340698 2.6476314067840576 28.268943786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913885116577148 2.444404363632202 26.235431671142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925056219100952 2.5646278858184814 27.438783645629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792647361755371 2.159187078475952 23.384517669677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925814390182495 2.930966377258301 31.102245330810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792701244354248 2.268895387649536 24.48165512084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926132678985596 2.1875851154327393 23.66846466064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916308641433716 2.140892267227173 23.20055389404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921807765960693 2.8363051414489746 30.15523338317871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916878461837769 2.830390214920044 30.09558868408203
  batch 40 loss: 1.7916878461837769, 2.830390214920044, 30.09558868408203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206883430481 2.9676003456115723 31.468210220336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924301624298096 1.884112000465393 20.6335506439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920840978622437 2.0037143230438232 21.829227447509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915855646133423 2.2546796798706055 24.338382720947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925382852554321 2.5263426303863525 27.05596351623535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922269105911255 2.2180986404418945 23.97321319580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792093276977539 2.2449560165405273 24.241653442382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919293642044067 2.083441972732544 22.6263484954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916314601898193 2.390996217727661 25.70159339904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920777797698975 2.2421324253082275 24.213401794433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910460233688354 2.6004974842071533 27.7960205078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.5686798095703125 27.479230880737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922834157943726 2.4155499935150146 25.947784423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926360368728638 2.70408296585083 28.833465576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.3399932384490967 25.191797256469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925965785980225 2.714388370513916 28.936479568481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928035259246826 2.9284260272979736 31.077062606811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927809953689575 3.2643215656280518 34.435997009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932316064834595 2.6707370281219482 28.50060272216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921597957611084 2.282865047454834 24.620811462402344
  batch 60 loss: 1.7921597957611084, 2.282865047454834, 24.620811462402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928270101547241 3.0112407207489014 31.905235290527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916626930236816 2.227088689804077 24.062549591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925975322723389 2.3426783084869385 25.219379425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921841144561768 2.53092885017395 27.101472854614258
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927764654159546 1.9804049730300903 21.596826553344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918782234191895 2.6126577854156494 27.918455123901367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917908430099487 2.397563934326172 25.76742935180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915704250335693 2.415341854095459 25.944990158081055
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791901707649231 1.851441502571106 20.306318283081055
Total LOSS train 26.35256752601037 valid 24.98429822921753
CE LOSS train 1.7922967654008133 valid 0.44797542691230774
Contrastive LOSS train 2.4560270804625293 valid 0.4628603756427765
EPOCH 171:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922542095184326 2.0938026905059814 22.73027992248535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915267944335938 2.3078994750976562 24.870521545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925347089767456 2.69667649269104 28.75929832458496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924079895019531 2.065943479537964 22.45184326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792099118232727 1.8183023929595947 19.975122451782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792930006980896 1.9873573780059814 21.66650390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791887640953064 2.813225269317627 29.92413902282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919193506240845 2.302111864089966 24.813037872314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917497158050537 1.9493521451950073 21.28527069091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917823791503906 2.049363136291504 22.28541374206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927793264389038 2.2156922817230225 23.949703216552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924342155456543 2.4824326038360596 26.61676025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930835485458374 2.5765390396118164 27.558473587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927138805389404 2.239567756652832 24.188390731811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910383939743042 2.2643520832061768 24.434558868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792799711227417 2.325329542160034 25.04609489440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926361560821533 2.3553576469421387 25.346214294433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924973964691162 2.2373509407043457 24.166006088256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792358636856079 2.4465863704681396 26.258222579956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791578769683838 2.4080381393432617 25.871959686279297
  batch 20 loss: 1.791578769683838, 2.4080381393432617, 25.871959686279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 2.10394549369812 22.831993103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792506217956543 2.2627665996551514 24.42017364501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922996282577515 2.3376624584198 25.16892433166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927820682525635 2.4569056034088135 26.36183738708496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924396991729736 2.5181353092193604 26.973793029785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792572021484375 2.6224772930145264 28.017345428466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929733991622925 2.7022430896759033 28.81540298461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919094562530518 2.985295534133911 31.644865036010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934629917144775 2.4207563400268555 26.001026153564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914625406265259 2.3171942234039307 24.963403701782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792626976966858 2.5089669227600098 26.882295608520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913901805877686 2.8484041690826416 30.27543067932129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925097942352295 2.1810946464538574 23.603456497192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926547527313232 1.8710715770721436 20.50337028503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925893068313599 2.6976358890533447 28.76894760131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927031517028809 2.188323497772217 23.675939559936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926087379455566 2.452951192855835 26.322120666503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791625738143921 1.9042456150054932 20.834081649780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921748161315918 2.683934450149536 28.631519317626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916864156723022 2.811980724334717 29.911495208740234
  batch 40 loss: 1.7916864156723022, 2.811980724334717, 29.911495208740234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792212963104248 2.226520299911499 24.057416915893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924426794052124 2.1610000133514404 23.402442932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921010255813599 2.6405632495880127 28.19773292541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915971279144287 2.0856528282165527 22.64812660217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542576789856 2.0123989582061768 21.916532516479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922221422195435 2.107560873031616 22.86783218383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920875549316406 2.1315643787384033 23.107730865478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919154167175293 2.248424768447876 24.27616310119629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916204929351807 2.3282172679901123 25.073793411254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920719385147095 2.2402446269989014 24.19451904296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791046380996704 2.524623155593872 27.037277221679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792433261871338 2.6346161365509033 28.138593673706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792287826538086 2.742220401763916 29.21449089050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792637586593628 2.424964427947998 26.042282104492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 2.325814723968506 25.050012588500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925957441329956 2.475600481033325 26.548599243164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928003072738647 2.492154359817505 26.714344024658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927758693695068 2.287747383117676 24.670249938964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932274341583252 2.493377447128296 26.72700309753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792161226272583 2.3313841819763184 25.10600471496582
  batch 60 loss: 1.792161226272583, 2.3313841819763184, 25.10600471496582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928237915039062 2.3082287311553955 24.875110626220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791662573814392 2.951181173324585 31.30347442626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925947904586792 2.0472137928009033 22.264732360839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921823263168335 2.2842862606048584 24.635046005249023
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927730083465576 1.828041911125183 20.073190689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918803691864014 2.419808864593506 25.98996925354004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917922735214233 2.4674196243286133 26.465988159179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915717363357544 2.3576784133911133 25.368356704711914
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919031381607056 2.0439345836639404 22.23124885559082
Total LOSS train 25.306891367985653 valid 25.013890743255615
CE LOSS train 1.7922954981143657 valid 0.4479757845401764
Contrastive LOSS train 2.351459587537325 valid 0.5109836459159851
EPOCH 172:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792255163192749 2.636525869369507 28.157514572143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915301322937012 2.194291114807129 23.73444175720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 2.4398300647735596 26.190837860107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 2.582461357116699 27.61702537536621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921006679534912 1.6414254903793335 18.206356048583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929290533065796 1.7476762533187866 19.269691467285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918888330459595 2.3468713760375977 25.260602951049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919223308563232 2.6800456047058105 28.592378616333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917532920837402 2.154874324798584 23.340497970581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917852401733398 2.7631850242614746 29.42363739013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927799224853516 2.696073055267334 28.753511428833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792434811592102 2.325084924697876 25.043283462524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793082356452942 2.79858136177063 29.77889633178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927131652832031 2.6795859336853027 28.588573455810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910444736480713 2.676373243331909 28.554777145385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792805790901184 2.3480308055877686 25.273115158081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79264497756958 2.4322669506073 26.115314483642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792504906654358 2.640639305114746 28.198898315429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923661470413208 2.322955846786499 25.02192497253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791581392288208 2.8796679973602295 30.5882625579834
  batch 20 loss: 1.791581392288208, 2.8796679973602295, 30.5882625579834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925350666046143 2.2890734672546387 24.683271408081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924997806549072 2.231013059616089 24.102630615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79228937625885 2.2799017429351807 24.591306686401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927708625793457 2.317633628845215 24.969106674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.6836001873016357 28.628435134887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925684452056885 2.2287471294403076 24.080039978027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929723262786865 2.5427441596984863 27.220413208007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919137477874756 2.247610569000244 24.26801872253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793465495109558 2.4317071437835693 26.110536575317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914650440216064 2.4015555381774902 25.807022094726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792620062828064 2.7981038093566895 29.773656845092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913845777511597 2.3476154804229736 25.26753807067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925007343292236 2.4680864810943604 26.473365783691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926464080810547 2.259814977645874 24.390796661376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586326599121 2.275695562362671 24.549541473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927026748657227 2.958113670349121 31.37384033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926090955734253 2.079451322555542 22.587121963500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.1233839988708496 23.02546501159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921724319458008 2.330667018890381 25.09884262084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916806936264038 2.2838947772979736 24.63062858581543
  batch 40 loss: 1.7916806936264038, 2.2838947772979736, 24.63062858581543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922029495239258 2.4004433155059814 25.796634674072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924352884292603 2.0471932888031006 22.264368057250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79209303855896 2.2637715339660645 24.429807662963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915960550308228 1.7739874124526978 19.531469345092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254150390625 1.9577617645263672 21.370159149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792223334312439 2.0233166217803955 22.025388717651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920913696289062 1.7813299894332886 19.605390548706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791919231414795 2.196098804473877 23.752906799316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621208190918 2.3684661388397217 25.47628402709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920724153518677 2.2684924602508545 24.47699737548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910442352294922 2.1770074367523193 23.561119079589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924293279647827 2.409935474395752 25.891782760620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922853231430054 2.5494132041931152 27.286418914794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926383018493652 2.273576498031616 24.528404235839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918654680252075 2.466928243637085 26.461149215698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925996780395508 2.3722519874572754 25.515117645263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928012609481812 2.3862550258636475 25.655351638793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927780151367188 2.1834845542907715 23.62762451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793229103088379 2.6083781719207764 27.877010345458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921662330627441 2.154210329055786 23.33426856994629
  batch 60 loss: 1.7921662330627441, 2.154210329055786, 23.33426856994629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928272485733032 2.367840528488159 25.471233367919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916728258132935 2.511793613433838 26.909608840942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792604684829712 2.1713309288024902 23.50591468811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921890020370483 2.2850286960601807 24.642475128173828
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927747964859009 1.6779626607894897 18.57240104675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918747663497925 2.5286920070648193 27.078794479370117
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917873859405518 2.2505314350128174 24.297101974487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915674448013306 2.4616520404815674 26.4080867767334
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918974161148071 2.3030967712402344 24.822864532470703
Total LOSS train 25.214006247887244 valid 25.65171194076538
CE LOSS train 1.7922955788098849 valid 0.4479743540287018
Contrastive LOSS train 2.342171050952031 valid 0.5757741928100586
EPOCH 173:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792250394821167 2.313292980194092 24.925180435180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 2.6186041831970215 27.977563858032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792526364326477 2.634854793548584 28.141075134277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924057245254517 2.089932441711426 22.691730499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792100429534912 2.1589484214782715 23.3815860748291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929326295852661 2.7587575912475586 29.380508422851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918872833251953 2.686941385269165 28.661300659179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919187545776367 2.2617762088775635 24.409679412841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917516231536865 2.1815741062164307 23.607492446899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917853593826294 2.0030264854431152 21.822052001953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927844524383545 2.780789613723755 29.600679397583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924405336380005 2.3709371089935303 25.501811981201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930814027786255 2.4955902099609375 26.74898338317871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927110195159912 2.5979397296905518 27.77210807800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791041374206543 2.7142348289489746 28.933391571044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927980422973633 2.7001051902770996 28.79384994506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926313877105713 2.486609935760498 26.65873146057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924906015396118 2.606031894683838 27.852807998657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923554182052612 1.896175503730774 20.75411033630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915780544281006 2.1035122871398926 22.82670021057129
  batch 20 loss: 1.7915780544281006, 2.1035122871398926, 22.82670021057129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925351858139038 2.5312750339508057 27.10528564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925035953521729 2.1603434085845947 23.395936965942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792294979095459 1.8279062509536743 20.07135772705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792776346206665 1.9665735960006714 21.458511352539062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924349308013916 2.6638693809509277 28.431129455566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925693988800049 2.3113505840301514 24.906076431274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929742336273193 2.32252836227417 25.018259048461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919174432754517 2.0088775157928467 21.880693435668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934694290161133 2.464276075363159 26.436229705810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914752960205078 2.2877931594848633 24.66940689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926337718963623 2.360438346862793 25.397016525268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913999557495117 2.2835469245910645 24.626869201660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925149202346802 2.1970367431640625 23.762882232666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792657494544983 2.2694990634918213 24.487648010253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925885915756226 2.453941583633423 26.33200454711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702555656433 2.6321539878845215 28.114242553710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926132678985596 2.2589080333709717 24.381694793701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791633129119873 2.844163417816162 30.23326873779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792183518409729 2.509582281112671 26.88800621032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916901111602783 2.2566802501678467 24.35849380493164
  batch 40 loss: 1.7916901111602783, 2.2566802501678467, 24.35849380493164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922117710113525 2.37064528465271 25.49866485595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792436122894287 2.538347005844116 27.175907135009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920829057693481 2.467958927154541 26.47167205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791586995124817 2.325881004333496 25.050397872924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925305366516113 2.4671847820281982 26.464378356933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792217493057251 2.2268481254577637 24.060699462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920912504196167 2.387662649154663 25.668718338012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919281721115112 2.0168960094451904 21.960887908935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916330099105835 2.3140594959259033 24.932228088378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920831441879272 1.6767293214797974 18.559377670288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910526990890503 2.6313254833221436 28.104307174682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924317121505737 2.8046224117279053 29.838655471801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792284607887268 2.122112512588501 23.013408660888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926342487335205 2.2645795345306396 24.438430786132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918673753738403 2.453681707382202 26.328685760498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600393295288 2.1011080741882324 22.803682327270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928030490875244 2.307483434677124 24.867637634277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927805185317993 2.7208893299102783 29.001672744750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932333946228027 2.1227376461029053 23.020610809326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792169213294983 1.9188830852508545 20.981000900268555
  batch 60 loss: 1.792169213294983, 1.9188830852508545, 20.981000900268555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928261756896973 1.9658448696136475 21.451274871826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791669487953186 2.344759464263916 25.2392635345459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925959825515747 2.265296697616577 24.44556427001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921797037124634 2.105775833129883 22.849937438964844
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927696704864502 1.7756431102752686 19.54920196533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918773889541626 2.8751895427703857 30.543773651123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791789174079895 2.1465325355529785 23.25711441040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915693521499634 2.071803569793701 22.509605407714844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791898488998413 1.907748818397522 20.869386672973633
Total LOSS train 25.141117272010217 valid 24.29497003555298
CE LOSS train 1.7922963105715237 valid 0.44797462224960327
Contrastive LOSS train 2.3348820723020114 valid 0.4769372045993805
EPOCH 174:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792250394821167 2.195852041244507 23.750770568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915260791778564 2.9179940223693848 30.971466064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925325632095337 2.5764966011047363 27.557497024536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 2.479323148727417 26.585643768310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921035289764404 2.2883665561676025 24.67576789855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929326295852661 2.3379745483398438 25.172677993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918905019760132 2.848496913909912 30.2768611907959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791926383972168 2.3223373889923096 25.015300750732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791757345199585 2.2784605026245117 24.57636260986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917894124984741 2.0918641090393066 22.710430145263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927831411361694 2.333904981613159 25.131834030151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 2.642789363861084 28.220333099365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930793762207031 2.3548972606658936 25.342052459716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927130460739136 2.6637063026428223 28.42977523803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910462617874146 2.2035415172576904 23.826461791992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792799949645996 1.9900715351104736 21.69351577758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926359176635742 2.4185943603515625 25.978580474853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924970388412476 2.3102023601531982 24.894521713256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923636436462402 2.043581485748291 22.228178024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915852069854736 2.342949151992798 25.22107696533203
  batch 20 loss: 1.7915852069854736, 2.342949151992798, 25.22107696533203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792535424232483 2.9872376918792725 31.664913177490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792500615119934 2.3744590282440186 25.537092208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922894954681396 1.9448599815368652 21.240890502929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927746772766113 2.6120543479919434 27.913318634033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924355268478394 2.8967018127441406 30.75945281982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925695180892944 2.2538857460021973 24.3314266204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79296875 2.351145029067993 25.304418563842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919124364852905 1.9389228820800781 21.181140899658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934620380401611 2.3410730361938477 25.204193115234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79147207736969 2.302914619445801 24.82061767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926206588745117 2.8427462577819824 30.22008514404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913848161697388 2.3331401348114014 25.122787475585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924965620040894 2.3885316848754883 25.677812576293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926433086395264 2.385761022567749 25.650253295898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79258131980896 2.9372143745422363 31.164724349975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926968336105347 2.9533581733703613 31.326276779174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792603850364685 2.1992008686065674 23.78461265563965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791625738143921 1.8895995616912842 20.6876220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79217529296875 2.339651584625244 25.188690185546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.3443753719329834 25.235437393188477
  batch 40 loss: 1.791683554649353, 2.3443753719329834, 25.235437393188477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792209267616272 2.2031984329223633 23.824193954467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924426794052124 2.265178680419922 24.444229125976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920968532562256 2.3239142894744873 25.031240463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916033267974854 2.0893235206604004 22.684837341308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925448417663574 2.0093159675598145 21.885704040527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922279834747314 2.549694299697876 27.28917121887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920938730239868 2.204566478729248 23.837759017944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919248342514038 2.0668790340423584 22.460716247558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916277647018433 2.1890692710876465 23.682321548461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920801639556885 2.007939338684082 21.87147331237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910525798797607 1.9604525566101074 21.395578384399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924295663833618 2.349309206008911 25.285520553588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922801971435547 2.397989511489868 25.772174835205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926307916641235 2.532515287399292 27.11778450012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918577194213867 2.3544304370880127 25.336162567138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925963401794434 2.974900722503662 31.54160499572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927950620651245 2.567646026611328 27.469255447387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927744388580322 2.119605779647827 22.988832473754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932237386703491 3.1305088996887207 33.09831237792969
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921589612960815 2.874934196472168 30.541500091552734
  batch 60 loss: 1.7921589612960815, 2.874934196472168, 30.541500091552734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928144931793213 2.563403367996216 27.426847457885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916672229766846 2.3577773571014404 25.369441986083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925962209701538 2.0693466663360596 22.48606300354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921816110610962 2.635014295578003 28.142324447631836
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927690744400024 1.9051454067230225 20.844223022460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918760776519775 2.3908190727233887 25.7000675201416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917879819869995 2.2605931758880615 24.397720336914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915678024291992 2.001373052597046 21.8052978515625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919018268585205 2.016026735305786 21.95216941833496
Total LOSS train 25.63234070997972 valid 23.46381378173828
CE LOSS train 1.7922950084392841 valid 0.4479754567146301
Contrastive LOSS train 2.3840045598837047 valid 0.5040066838264465
EPOCH 175:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792252540588379 2.274268627166748 24.53493881225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915288209915161 2.527331590652466 27.064844131469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.2757866382598877 24.550395965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924132347106934 2.6752209663391113 28.54462242126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921054363250732 2.001991033554077 21.812015533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929363250732422 2.133558750152588 23.128522872924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918848991394043 2.2585670948028564 24.37755584716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919163703918457 2.0486555099487305 22.278470993041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917437553405762 1.9349225759506226 21.14097023010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917739152908325 1.8274331092834473 20.066104888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927740812301636 2.602405309677124 27.81682777404785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792435646057129 2.3703742027282715 25.496177673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930705547332764 2.3882644176483154 25.67571449279785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927016019821167 2.1466760635375977 23.259462356567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910387516021729 2.5603384971618652 27.394424438476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927886247634888 2.4092466831207275 25.885255813598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926299571990967 2.2092440128326416 23.885068893432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924925088882446 2.5137267112731934 26.929759979248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 2.29583740234375 24.7507381439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915871143341064 2.5656278133392334 27.447866439819336
  batch 20 loss: 1.7915871143341064, 2.5656278133392334, 27.447866439819336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925361394882202 2.169362783432007 23.486164093017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925015687942505 2.3015925884246826 24.808427810668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922899723052979 2.736685276031494 29.159141540527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927744388580322 1.8343390226364136 20.13616371154785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924362421035767 2.4154460430145264 25.946897506713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925677299499512 2.245556354522705 24.248130798339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929668426513672 2.350806951522827 25.301036834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919113636016846 2.1083552837371826 22.875465393066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934536933898926 2.0598697662353516 22.39215087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791468858718872 1.9495372772216797 21.286842346191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926164865493774 2.4820988178253174 26.613603591918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913873195648193 2.326446056365967 25.055849075317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792498230934143 2.1510376930236816 23.302873611450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792642593383789 2.1842474937438965 23.63511848449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580246925354 2.51540470123291 26.946626663208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926939725875854 2.3403666019439697 25.196359634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926052808761597 2.6640822887420654 28.433427810668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791629433631897 2.557589292526245 27.367523193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921764850616455 2.932405710220337 31.116233825683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916796207427979 2.8769524097442627 30.561203002929688
  batch 40 loss: 1.7916796207427979, 2.8769524097442627, 30.561203002929688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206048965454 2.5560739040374756 27.35294532775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792434811592102 2.5767595767974854 27.560029983520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920830249786377 4.095049858093262 42.742584228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915916442871094 2.3341245651245117 25.132837295532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925305366516113 2.4085886478424072 25.878416061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922135591506958 2.368544101715088 25.47765350341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920907735824585 2.139570951461792 23.187801361083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791925311088562 1.6993190050125122 18.78511619567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916276454925537 2.316086769104004 24.952495574951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920762300491333 1.8894684314727783 20.68675994873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910531759262085 2.178546667098999 23.576520919799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924315929412842 2.361053943634033 25.402969360351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922899723052979 2.1702804565429688 23.495094299316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792640209197998 2.7145087718963623 28.937728881835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918709516525269 2.423205614089966 26.02392578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926021814346313 2.0326290130615234 22.118892669677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928022146224976 2.335179328918457 25.144596099853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927789688110352 2.6350831985473633 28.143611907958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932275533676147 2.8614602088928223 30.40782928466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921662330627441 2.250145435333252 24.29361915588379
  batch 60 loss: 1.7921662330627441, 2.250145435333252, 24.29361915588379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928260564804077 2.25984525680542 24.391279220581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916748523712158 2.3628814220428467 25.420490264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925970554351807 2.1152045726776123 22.944643020629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921745777130127 2.759310007095337 29.38527488708496
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792754888534546 1.6129814386367798 17.922569274902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918676137924194 3.3468916416168213 35.26078414916992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791778802871704 2.748821258544922 29.279991149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915593385696411 3.063342809677124 32.42498779296875
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918936014175415 2.550112247467041 27.293014526367188
Total LOSS train 25.281148646428036 valid 31.06469440460205
CE LOSS train 1.7922931524423453 valid 0.4479734003543854
Contrastive LOSS train 2.348885547197782 valid 0.6375280618667603
EPOCH 176:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922439575195312 2.4760446548461914 26.552690505981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915204763412476 2.3982841968536377 25.774362564086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925207614898682 2.349041700363159 25.28293800354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924036979675293 2.0374033451080322 22.16643714904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920992374420166 2.2903215885162354 24.695314407348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929326295852661 2.3153440952301025 24.946372985839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918858528137207 2.5556321144104004 27.34820556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919245958328247 2.039311647415161 22.185041427612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917507886886597 2.183270215988159 23.624452590942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79177725315094 2.0909135341644287 22.700912475585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792772650718689 2.6683971881866455 28.476743698120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924327850341797 2.3322415351867676 25.11484718322754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930629253387451 2.58174729347229 27.61053466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792696237564087 2.5987370014190674 27.780065536499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791040301322937 2.0902740955352783 22.69378089904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927885055541992 2.4663472175598145 26.456260681152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926303148269653 2.351301670074463 25.305646896362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792485237121582 2.298039674758911 24.77288055419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923548221588135 1.939852237701416 21.190876007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915810346603394 2.2726361751556396 24.517942428588867
  batch 20 loss: 1.7915810346603394, 2.2726361751556396, 24.517942428588867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925266027450562 2.628035545349121 28.0728816986084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792496919631958 2.733736753463745 29.129865646362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922797203063965 2.4281764030456543 26.074045181274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927675247192383 3.160409450531006 33.3968620300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924308776855469 2.7359018325805664 29.15144920349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925626039505005 2.6214382648468018 28.00694465637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792961597442627 2.2055861949920654 23.84882354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 2.009174108505249 21.883644104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934489250183105 2.5068812370300293 26.862262725830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914742231369019 2.158878803253174 23.380260467529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926180362701416 2.6882846355438232 28.675464630126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913861274719238 2.5843169689178467 27.63455581665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924927473068237 2.3051304817199707 24.843795776367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926334142684937 2.631666898727417 28.109302520751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925726175308228 2.250354528427124 24.296117782592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926889657974243 2.4939334392547607 26.732023239135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926044464111328 2.835721015930176 30.14981460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916316986083984 2.4493398666381836 26.285030364990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921810150146484 2.6837055683135986 28.629236221313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916802167892456 2.465319871902466 26.44487762451172
  batch 40 loss: 1.7916802167892456, 2.465319871902466, 26.44487762451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922029495239258 2.3251261711120605 25.04346466064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.464757204055786 26.440004348754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792081594467163 2.3160672187805176 24.9527530670166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916003465652466 2.0829734802246094 22.621335983276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925363779067993 2.0336766242980957 22.129301071166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922112941741943 2.3693795204162598 25.486005783081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792087197303772 2.149822235107422 23.29030990600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919164896011353 2.4092302322387695 25.884218215942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791616678237915 2.9644172191619873 31.435789108276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792069435119629 2.046638250350952 22.258453369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910432815551758 2.3299481868743896 25.090526580810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792420506477356 2.329564094543457 25.088062286376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922799587249756 2.1732335090637207 23.524614334106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926305532455444 1.8776615858078003 20.569246292114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918543815612793 2.5062220096588135 26.854074478149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925937175750732 2.563732624053955 27.42991828918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792786717414856 2.3302419185638428 25.09520721435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927652597427368 1.934408187866211 21.1368465423584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932188510894775 2.4166667461395264 25.95988655090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792161464691162 2.4190256595611572 25.982418060302734
  batch 60 loss: 1.792161464691162, 2.4190256595611572, 25.982418060302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928155660629272 2.4599926471710205 26.392742156982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916698455810547 2.1841790676116943 23.633460998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792588710784912 2.205385446548462 23.84644317626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921652793884277 2.3610587120056152 25.402753829956055
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927515506744385 1.8084168434143066 19.87691879272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918696403503418 3.1783647537231445 33.57551574707031
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917803525924683 2.7007267475128174 28.799047470092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791560411453247 2.774531841278076 29.536880493164062
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918977737426758 2.4640867710113525 26.43276596069336
Total LOSS train 25.48043567950909 valid 29.586052417755127
CE LOSS train 1.7922883840707633 valid 0.44797444343566895
Contrastive LOSS train 2.3688147453161386 valid 0.6160216927528381
EPOCH 177:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922465801239014 2.345839500427246 25.250640869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915271520614624 2.825087308883667 30.042400360107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925268411636353 2.2614738941192627 24.407264709472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792409896850586 2.113748550415039 22.929895401000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920985221862793 2.34411358833313 25.233234405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792931079864502 2.1843245029449463 23.63617515563965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918792963027954 2.340756893157959 25.19944953918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791919469833374 2.1238410472869873 23.030330657958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917474508285522 2.1540980339050293 23.33272933959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917754650115967 2.094630479812622 22.738079071044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927751541137695 2.3718271255493164 25.51104736328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924340963363647 2.318256378173828 24.974998474121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930591106414795 2.598466396331787 27.77772331237793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926946878433228 2.2301266193389893 24.09395980834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910410165786743 2.2746689319610596 24.537729263305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927871942520142 3.2903800010681152 34.69658660888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926298379898071 2.3583426475524902 25.376056671142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924854755401611 2.4237289428710938 26.029775619506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792357087135315 2.1493351459503174 23.285707473754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915804386138916 2.4135782718658447 25.9273624420166
  batch 20 loss: 1.7915804386138916, 2.4135782718658447, 25.9273624420166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925207614898682 2.2190120220184326 23.982641220092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924952507019043 2.0627951622009277 22.420448303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922807931900024 2.4467666149139404 26.259946823120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927720546722412 2.1760218143463135 23.552989959716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924342155456543 2.8772895336151123 30.565330505371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925636768341064 3.2429330348968506 34.221893310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929593324661255 3.063326358795166 32.42622375488281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918994426727295 2.641310930252075 28.205007553100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793442726135254 3.164254665374756 33.43598937988281
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914751768112183 2.3540713787078857 25.332189559936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926172018051147 2.5728683471679688 27.52130126953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913894653320312 2.7160372734069824 28.951763153076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924935817718506 2.2097630500793457 23.89012336730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926326990127563 2.242628335952759 24.218915939331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925721406936646 2.436565399169922 26.158226013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926864624023438 2.5640127658843994 27.43281364440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792601227760315 2.3107247352600098 24.89984703063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916285991668701 2.162327289581299 23.414899826049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921737432479858 2.341465473175049 25.20682716369629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916721105575562 1.8110829591751099 19.902502059936523
  batch 40 loss: 1.7916721105575562, 1.8110829591751099, 19.902502059936523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921994924545288 2.2238194942474365 24.0303955078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924296855926514 2.0288243293762207 22.080671310424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920756340026855 2.467449426651001 26.466569900512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593074798584 2.2089779376983643 23.881372451782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925310134887695 2.960576295852661 31.398292541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792209267616272 1.8991259336471558 20.78346824645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920855283737183 1.9271303415298462 21.06338882446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919224500656128 2.301893949508667 24.810861587524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916172742843628 2.1272366046905518 23.063982009887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920728921890259 2.145463466644287 23.246707916259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910486459732056 2.31634521484375 24.954500198364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792421579360962 2.4795920848846436 26.588342666625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922794818878174 2.651517152786255 28.307451248168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926300764083862 2.455396890640259 26.34659767150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791856050491333 2.660731077194214 28.399168014526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925972938537598 2.056687831878662 22.35947608947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927918434143066 2.4503390789031982 26.29618263244629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927708625793457 2.193049430847168 23.723264694213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793217420578003 2.4672164916992188 26.465381622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921591997146606 2.4519524574279785 26.311683654785156
  batch 60 loss: 1.7921591997146606, 2.4519524574279785, 26.311683654785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928135395050049 2.2575008869171143 24.367822647094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916724681854248 2.3873753547668457 25.665424346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925935983657837 2.1436424255371094 23.22901725769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921699285507202 2.2663137912750244 24.455307006835938
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927534580230713 2.0052857398986816 21.845609664916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 2.9573168754577637 31.365039825439453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791780948638916 2.5553789138793945 27.345569610595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915619611740112 2.4311060905456543 26.102622985839844
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918977737426758 2.0597987174987793 22.38988494873047
Total LOSS train 25.540799478384166 valid 26.800779342651367
CE LOSS train 1.7922881273122935 valid 0.44797444343566895
Contrastive LOSS train 2.3748511552810667 valid 0.5149496793746948
EPOCH 178:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792246699333191 2.2864418029785156 24.65666389465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915256023406982 2.2698633670806885 24.490158081054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925223112106323 2.8943095207214355 30.735618591308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924038171768188 2.3547773361206055 25.340177536010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920918464660645 2.215468406677246 23.946775436401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929248809814453 2.794715404510498 29.740079879760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918751239776611 2.719569206237793 28.987567901611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919172048568726 2.093872547149658 22.730642318725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917437553405762 2.042020082473755 22.211944580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791771411895752 2.192216157913208 23.713932037353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792770266532898 2.4110515117645264 25.90328598022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924354076385498 2.5485970973968506 27.278406143188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930577993392944 2.2607243061065674 24.400300979614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926944494247437 2.163564443588257 23.4283390045166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791041374206543 2.1770806312561035 23.561847686767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927852869033813 2.3363287448883057 25.15607261657715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926276922225952 2.265421152114868 24.44683837890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792483925819397 2.504141092300415 26.833894729614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923566102981567 1.9324450492858887 21.11680793762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915838956832886 2.037201166152954 22.16359519958496
  batch 20 loss: 1.7915838956832886, 2.037201166152954, 22.16359519958496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925242185592651 2.1886394023895264 23.678918838500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924976348876953 2.718510627746582 28.977603912353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922784090042114 2.5360467433929443 27.152746200561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792765736579895 2.6765120029449463 28.557886123657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924280166625977 2.7206661701202393 28.999088287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925560474395752 2.769148826599121 29.484045028686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792953372001648 2.7026565074920654 28.81951904296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919011116027832 2.0014264583587646 21.80616569519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934460639953613 2.227358818054199 24.067033767700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914812564849854 2.9123449325561523 30.91493034362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926182746887207 2.2928454875946045 24.721073150634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791390061378479 2.0707554817199707 22.498943328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924935817718506 2.4520771503448486 26.313264846801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792632818222046 2.1247570514678955 23.040203094482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925729751586914 2.4899632930755615 26.69220733642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.2393555641174316 24.186237335205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792596459388733 2.2445521354675293 24.23811912536621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791625738143921 2.104825735092163 22.83988380432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921758890151978 2.6474177837371826 28.266353607177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916755676269531 1.9428985118865967 21.220661163330078
  batch 40 loss: 1.7916755676269531, 1.9428985118865967, 21.220661163330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922035455703735 2.369938850402832 25.491592407226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924325466156006 2.0558393001556396 22.350826263427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920787334442139 2.336127281188965 25.153350830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915964126586914 2.174363136291504 23.535228729248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533040046692 2.0822715759277344 22.615249633789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792210578918457 2.2352325916290283 24.144535064697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792087435722351 2.1747233867645264 23.539321899414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919188737869263 2.2544188499450684 24.336109161376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916139364242554 2.8444602489471436 30.236217498779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792065143585205 2.1255593299865723 23.047657012939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910405397415161 2.9142940044403076 30.93398094177246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924127578735352 2.7107770442962646 28.900184631347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922743558883667 2.001955032348633 21.811824798583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926260232925415 2.190570592880249 23.698331832885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791854977607727 2.304603338241577 24.837888717651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925912141799927 2.240697145462036 24.199562072753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792783498764038 2.441328525543213 26.20606803894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927623987197876 2.374192714691162 25.534690856933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932111024856567 2.8484065532684326 30.27727699279785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792157530784607 2.409626007080078 25.888418197631836
  batch 60 loss: 1.792157530784607, 2.409626007080078, 25.888418197631836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928080558776855 2.1333351135253906 23.12615966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916698455810547 1.9224414825439453 21.016084671020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925869226455688 2.196190357208252 23.75448989868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921669483184814 2.335010528564453 25.14227294921875
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927563190460205 1.9819709062576294 21.61246681213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791878342628479 2.2099287509918213 23.891164779663086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917886972427368 2.2737746238708496 24.5295352935791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 2.0627033710479736 22.418601989746094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919069528579712 2.0441336631774902 22.233243942260742
Total LOSS train 25.21088650043194 valid 23.268136501312256
CE LOSS train 1.7922861356001634 valid 0.4479767382144928
Contrastive LOSS train 2.341860024745648 valid 0.5110334157943726
EPOCH 179:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922539710998535 2.2747793197631836 24.54004669189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791532039642334 2.4811270236968994 26.602802276611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925219535827637 2.3282041549682617 25.07456398010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924048900604248 2.275639057159424 24.54879379272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792091965675354 2.062516689300537 22.417259216308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929292917251587 2.31268048286438 24.91973304748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918754816055298 2.2598869800567627 24.390745162963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791918396949768 1.7829298973083496 19.621217727661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917423248291016 2.177236318588257 23.564105987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917674779891968 1.9131921529769897 20.923688888549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927719354629517 2.407306432723999 25.86583709716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924381494522095 2.3742501735687256 25.534940719604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793054461479187 2.0829017162323 22.622072219848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926890850067139 2.3952057361602783 25.7447452545166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910423278808594 2.4368629455566406 26.159671783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927829027175903 2.47906756401062 26.583459854125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926268577575684 2.3021018505096436 24.81364631652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924797534942627 2.4082894325256348 25.87537384033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923556566238403 2.0032637119293213 21.824993133544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915836572647095 2.0244359970092773 22.03594398498535
  batch 20 loss: 1.7915836572647095, 2.0244359970092773, 22.03594398498535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925214767456055 2.1577842235565186 23.370365142822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792497158050537 2.2475762367248535 24.268259048461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922756671905518 2.0300509929656982 22.09278678894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927666902542114 2.3242111206054688 25.03487777709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792428970336914 2.4112119674682617 25.90454864501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925539016723633 2.2354390621185303 24.14694595336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792950987815857 2.2234458923339844 24.02741050720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.1823432445526123 23.615331649780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934411764144897 2.4481217861175537 26.274660110473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914825677871704 2.3565566539764404 25.3570499420166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926169633865356 2.3738343715667725 25.530961990356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913875579833984 2.2357256412506104 24.148643493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924896478652954 2.3349266052246094 25.141756057739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926284074783325 2.2112441062927246 23.905071258544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925704717636108 2.3295884132385254 25.08845329284668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792681097984314 2.317192316055298 24.964603424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925946712493896 2.161808490753174 23.41067886352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791629433631897 2.141608238220215 23.207712173461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921814918518066 1.9203169345855713 20.995349884033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916792631149292 1.925280213356018 21.04448127746582
  batch 40 loss: 1.7916792631149292, 1.925280213356018, 21.04448127746582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922065258026123 2.1939339637756348 23.731544494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792432188987732 2.1245670318603516 23.038103103637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920728921890259 1.850464940071106 20.296722412109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791591763496399 2.1695539951324463 23.487131118774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925221920013428 2.0769259929656982 22.561782836914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922030687332153 2.0460033416748047 22.25223731994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792086124420166 2.2139878273010254 23.931962966918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919236421585083 2.0809507369995117 22.601430892944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916200160980225 2.219931125640869 23.990930557250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920724153518677 2.0624823570251465 22.41689682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791043996810913 2.3486268520355225 25.277313232421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792407512664795 2.3450331687927246 25.242740631103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922676801681519 2.042916774749756 22.221435546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926201820373535 2.118830680847168 22.980926513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918531894683838 2.1843130588531494 23.63498306274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925970554351807 2.461251974105835 26.40511703491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927911281585693 1.9935052394866943 21.72784423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927683591842651 2.1373281478881836 23.16604995727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793215274810791 2.475295066833496 26.546165466308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921596765518188 2.3410632610321045 25.20279312133789
  batch 60 loss: 1.7921596765518188, 2.3410632610321045, 25.20279312133789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928054332733154 2.267181158065796 24.464616775512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79166579246521 2.145033597946167 23.242002487182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925801277160645 2.2564704418182373 24.357284545898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921571731567383 2.3294806480407715 25.086963653564453
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927416563034058 1.7655630111694336 19.44837188720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918705940246582 2.2682645320892334 24.474515914916992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917814254760742 2.395258903503418 25.744369506835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791561484336853 2.1145076751708984 22.93663787841797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.79189932346344 1.9878958463668823 21.670856475830078
Total LOSS train 23.88472213745117 valid 23.706594944000244
CE LOSS train 1.7922852919651913 valid 0.44797483086586
Contrastive LOSS train 2.2092436698766855 valid 0.4969739615917206
EPOCH 180:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922468185424805 2.439331531524658 26.185562133789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915291786193848 2.3773295879364014 25.5648250579834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925236225128174 2.179392099380493 23.586444854736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79240882396698 2.1433956623077393 23.226364135742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920951843261719 2.092480182647705 22.716896057128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929253578186035 2.1662912368774414 23.45583724975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918689250946045 2.3477718830108643 25.26958656311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919124364852905 2.1440634727478027 23.232547760009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 1.9846168756484985 21.637908935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917687892913818 2.0906567573547363 22.698335647583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927714586257935 2.2833380699157715 24.62615394592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924383878707886 2.364854097366333 25.44097900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930500507354736 2.29374623298645 24.730512619018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926883697509766 2.2580668926239014 24.37335777282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791045069694519 2.361572027206421 25.40676498413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927802801132202 2.098489999771118 22.777679443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926249504089355 2.2600390911102295 24.393016815185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924753427505493 2.227106809616089 24.06354331970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923519611358643 1.965574860572815 21.448101043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915812730789185 1.9257853031158447 21.049434661865234
  batch 20 loss: 1.7915812730789185, 1.9257853031158447, 21.049434661865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925152778625488 2.1529541015625 23.32205581665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924913167953491 2.184617757797241 23.638669967651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922687530517578 2.0705454349517822 22.497722625732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927621603012085 2.255816698074341 24.350929260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924280166625977 2.738689422607422 29.1793212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925529479980469 2.42790150642395 26.07156753540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929469347000122 2.1951091289520264 23.74403953552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791894793510437 2.220057487487793 23.992469787597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934348583221436 1.9156783819198608 20.950218200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914823293685913 2.1974687576293945 23.766170501708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792615294456482 2.2115161418914795 23.907777786254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791391372680664 2.1833691596984863 23.62508201599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924883365631104 2.156276226043701 23.35525131225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792622447013855 2.2759363651275635 24.551984786987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925634384155273 2.5570359230041504 27.36292266845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926769256591797 2.3185508251190186 24.978185653686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925926446914673 2.293029308319092 24.72288703918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916280031204224 2.113847255706787 22.93010139465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921772003173828 2.4112002849578857 25.9041805267334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916722297668457 2.318814992904663 24.979822158813477
  batch 40 loss: 1.7916722297668457, 2.318814992904663, 24.979822158813477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792198657989502 2.04231595993042 22.21535873413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792425274848938 1.9505560398101807 21.297985076904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79207181930542 2.398406505584717 25.776138305664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791598916053772 2.211609363555908 23.907691955566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925313711166382 2.190880298614502 23.70133399963379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922101020812988 2.3093152046203613 24.885360717773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920891046524048 2.1791884899139404 23.583974838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79192316532135 1.9840551614761353 21.632474899291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916147708892822 2.2796666622161865 24.588281631469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920634746551514 2.0577967166900635 22.37002944946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910375595092773 2.2242159843444824 24.033199310302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924021482467651 2.2256314754486084 24.048717498779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922663688659668 2.2415812015533447 24.208078384399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926204204559326 2.1847786903381348 23.640405654907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 2.3877205848693848 25.669063568115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792596459388733 2.2095353603363037 23.887950897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927868366241455 2.429124355316162 26.08403205871582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927621603012085 2.1749284267425537 23.54204750061035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932121753692627 2.396660804748535 25.75982093811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921626567840576 2.213674783706665 23.928909301757812
  batch 60 loss: 1.7921626567840576, 2.213674783706665, 23.928909301757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928109169006348 2.4042015075683594 25.83482551574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791677713394165 2.087186336517334 22.663541793823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792589545249939 2.1639516353607178 23.432106018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921648025512695 2.192981004714966 23.721973419189453
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792746663093567 1.803894281387329 19.831689834594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918709516525269 2.4567229747772217 26.359100341796875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791782259941101 2.416921377182007 25.960996627807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915619611740112 2.098093032836914 22.772491455078125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919009923934937 2.1306746006011963 23.09864616394043
Total LOSS train 23.968618510319637 valid 24.54780864715576
CE LOSS train 1.7922838944655197 valid 0.4479752480983734
Contrastive LOSS train 2.2176334564502422 valid 0.5326686501502991
EPOCH 181:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792248249053955 2.359135627746582 25.383604049682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915258407592773 2.4804635047912598 26.596160888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925153970718384 2.1124470233917236 22.91698455810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792405128479004 2.104914665222168 22.841552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79209566116333 2.090829849243164 22.700393676757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929292917251587 2.2757568359375 24.55049705505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918740510940552 2.3367764949798584 25.159639358520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919166088104248 2.1625094413757324 23.417011260986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917402982711792 1.9929661750793457 21.72140121459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791764736175537 2.201986789703369 23.81163215637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927696704864502 2.581486701965332 27.607637405395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 2.425855875015259 26.050996780395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793051838874817 2.1466381549835205 23.25943374633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926892042160034 2.1030995845794678 22.823684692382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910468578338623 2.1359598636627197 23.150644302368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927829027175903 2.362562656402588 25.41840934753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926297187805176 1.9882365465164185 21.67499542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924786806106567 2.24491024017334 24.241580963134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792354941368103 2.06762957572937 22.468650817871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915825843811035 2.217172384262085 23.963306427001953
  batch 20 loss: 1.7915825843811035, 2.217172384262085, 23.963306427001953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925115823745728 1.9880355596542358 21.672866821289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924890518188477 2.1324360370635986 23.11684799194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922685146331787 2.2009968757629395 23.802236557006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927614450454712 2.1233513355255127 23.026273727416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792424201965332 2.420295476913452 25.995380401611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925472259521484 2.166912794113159 23.4616756439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929414510726929 2.319624900817871 24.98919105529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918944358825684 2.1982734203338623 23.774629592895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934342622756958 2.2900195121765137 24.69363021850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914891242980957 2.184091329574585 23.632402420043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926199436187744 2.2888569831848145 24.681188583374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913930416107178 2.477978229522705 26.57117462158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924867868423462 2.269857168197632 24.491058349609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926225662231445 2.1720540523529053 23.513164520263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925631999969482 2.21266770362854 23.919239044189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926777601242065 2.228806972503662 24.080747604370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925913333892822 2.2048442363739014 23.841033935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916263341903687 2.2058820724487305 23.850446701049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921733856201172 2.3902878761291504 25.695051193237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79166841506958 2.3813531398773193 25.605199813842773
  batch 40 loss: 1.79166841506958, 2.3813531398773193, 25.605199813842773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921944856643677 2.286156177520752 24.65375518798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924216985702515 1.5730797052383423 17.523218154907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792069911956787 2.248000383377075 24.27207374572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791597604751587 2.0936691761016846 22.728288650512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792527675628662 2.28003191947937 24.59284782409668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79220712184906 2.1469101905822754 23.261308670043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920862436294556 2.0821473598480225 22.61355972290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919199466705322 2.138726234436035 23.179182052612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916117906570435 2.3720686435699463 25.512298583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792062520980835 1.9654369354248047 21.44643211364746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910414934158325 2.275127410888672 24.542316436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924048900604248 2.3137669563293457 24.930072784423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922697067260742 2.3017497062683105 24.809768676757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926212549209595 2.175446033477783 23.547080993652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791853904724121 2.3180651664733887 24.97250747680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792589545249939 2.1873443126678467 23.666032791137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927764654159546 2.4256136417388916 26.048912048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927519083023071 2.156256914138794 23.35531997680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931989431381226 2.405815601348877 25.851354598999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921515703201294 2.0847861766815186 22.6400146484375
  batch 60 loss: 1.7921515703201294, 2.0847861766815186, 22.6400146484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928016185760498 2.3218300342559814 25.01110076904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791674256324768 2.1590163707733154 23.381837844848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925883531570435 2.2058370113372803 23.85095977783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921658754348755 1.9339913129806519 21.132078170776367
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792744517326355 1.5893620252609253 17.686365127563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918692827224731 2.3855977058410645 25.647846221923828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917803525924683 2.4978556632995605 26.77033805847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915600538253784 2.0212275981903076 22.003835678100586
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919002771377563 1.899305820465088 20.784957885742188
Total LOSS train 23.83662065359262 valid 23.80174446105957
CE LOSS train 1.7922824364442091 valid 0.4479750692844391
Contrastive LOSS train 2.204433831801781 valid 0.474826455116272
EPOCH 182:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922453880310059 2.3885552883148193 25.677799224853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915254831314087 2.6814520359039307 28.60604476928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925100326538086 2.091611385345459 22.70862579345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923977375030518 2.1037991046905518 22.83038902282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920875549316406 2.33585262298584 25.15061378479004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929214239120483 2.25566029548645 24.349523544311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791870355606079 2.441784381866455 26.209712982177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919182777404785 2.2060463428497314 23.852380752563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917453050613403 2.224475622177124 24.036502838134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917670011520386 2.0180351734161377 21.972118377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792764663696289 2.3099257946014404 24.89202308654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792432188987732 2.2831902503967285 24.62433433532715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930395603179932 2.084604024887085 22.639080047607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926822900772095 2.3108344078063965 24.90102767944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910457849502563 2.3091132640838623 24.882179260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927753925323486 2.3913345336914062 25.70612144470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926193475723267 2.2840306758880615 24.63292694091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924669981002808 2.3671915531158447 25.46438217163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792346477508545 2.1674818992614746 23.467166900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915791273117065 2.0965216159820557 22.756793975830078
  batch 20 loss: 1.7915791273117065, 2.0965216159820557, 22.756793975830078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925093173980713 1.9853403568267822 21.645912170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924898862838745 2.1842384338378906 23.63487434387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792268991470337 1.865267038345337 20.44493865966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792762279510498 2.2484586238861084 24.2773494720459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924271821975708 2.5820136070251465 27.612564086914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925509214401245 2.286210298538208 24.654653549194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929413318634033 2.367832660675049 25.471267700195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918907403945923 1.8571134805679321 20.363025665283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934224605560303 2.495988368988037 26.753307342529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791479468345642 2.051497459411621 22.306453704833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926044464111328 2.352126121520996 25.313865661621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913825511932373 2.2046186923980713 23.837568283081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 2.0572187900543213 22.364665985107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926185131072998 1.933892011642456 21.13153839111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925679683685303 2.3109521865844727 24.902090072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926822900772095 2.2851202487945557 24.643884658813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925971746444702 1.9458476305007935 21.251073837280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916311025619507 1.8406720161437988 20.19835090637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792177438735962 2.2475647926330566 24.267824172973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791670322418213 2.3185126781463623 24.976797103881836
  batch 40 loss: 1.791670322418213, 2.3185126781463623, 24.976797103881836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921957969665527 2.1767196655273438 23.55939292907715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924214601516724 2.033543586730957 22.127857208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920688390731812 2.2060606479644775 23.85267448425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915960550308228 2.011366367340088 21.905258178710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925251722335815 2.0570433139801025 22.362957000732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922009229660034 2.3559410572052 25.35161018371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920794486999512 2.1295409202575684 23.08749008178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919162511825562 2.1186463832855225 22.97838020324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916114330291748 2.2859694957733154 24.65130615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920606136322021 2.0915749073028564 22.707809448242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910398244857788 2.4487931728363037 26.278972625732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924004793167114 2.2862086296081543 24.65448760986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922672033309937 2.092238664627075 22.71465301513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926183938980103 2.2804434299468994 24.5970516204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918537855148315 2.2758724689483643 24.55057716369629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925876379013062 2.3082098960876465 24.87468719482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927755117416382 2.5510358810424805 27.30313491821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927523851394653 2.1426713466644287 23.219467163085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793201208114624 2.6761889457702637 28.555091857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921558618545532 2.277776002883911 24.569915771484375
  batch 60 loss: 1.7921558618545532, 2.277776002883911, 24.569915771484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792798399925232 2.3627254962921143 25.420053482055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916696071624756 1.9179999828338623 20.971670150756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925783395767212 1.9976691007614136 21.769268035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921552658081055 2.4542033672332764 26.334190368652344
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792736291885376 1.7178244590759277 18.97098159790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791871428489685 2.2553513050079346 24.34538459777832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791782259941101 2.2562921047210693 24.354703903198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791562795639038 1.9659078121185303 21.450641632080078
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791903018951416 1.8581937551498413 20.37384033203125
Total LOSS train 23.95047217149001 valid 22.631142616271973
CE LOSS train 1.792279368180495 valid 0.447975754737854
Contrastive LOSS train 2.2158192762961755 valid 0.4645484387874603
EPOCH 183:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922475337982178 2.2262587547302246 24.05483627319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915340662002563 2.5562098026275635 27.3536319732666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925218343734741 2.0431230068206787 22.223752975463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924089431762695 2.08982515335083 22.690658569335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792093276977539 2.036067008972168 22.15276336669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929232120513916 2.20682430267334 23.86116600036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918685674667358 2.434770345687866 26.139572143554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919161319732666 2.2468724250793457 24.260639190673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917420864105225 2.2181410789489746 23.973154067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917611598968506 1.8774008750915527 20.565771102905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927602529525757 2.3301846981048584 25.094608306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924301624298096 2.2250592708587646 24.04302406311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930392026901245 2.2910313606262207 24.703351974487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.47223162651062 26.514999389648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791049599647522 1.999670386314392 21.78775405883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927768230438232 2.0509932041168213 22.30270767211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926210165023804 2.3530659675598145 25.323280334472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924683094024658 2.418570041656494 25.978168487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792349934577942 1.9910218715667725 21.70256996154785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915796041488647 1.867061734199524 20.46219825744629
  batch 20 loss: 1.7915796041488647, 1.867061734199524, 20.46219825744629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 1.7892400026321411 19.684906005859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792484164237976 2.059943199157715 22.391916275024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792264699935913 1.944955825805664 21.241823196411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927614450454712 1.9586278200149536 21.379039764404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924318313598633 2.13543701171875 23.146800994873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925554513931274 2.209925651550293 23.89181137084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929447889328003 2.464837074279785 26.441314697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918950319290161 2.290642023086548 24.698314666748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934226989746094 2.1739718914031982 23.53314208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791483759880066 2.1022932529449463 22.814414978027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926061153411865 2.31125807762146 24.905187606811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913862466812134 2.417581081390381 25.96719741821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924772500991821 2.152195453643799 23.314430236816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926160097122192 1.9682103395462036 21.474720001220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925629615783691 2.212486743927002 23.917428970336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678952217102 2.400019884109497 25.792877197265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925952672958374 1.8892921209335327 20.685516357421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916315793991089 1.73768150806427 19.168447494506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792178988456726 2.2353720664978027 24.14590072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791670322418213 2.076458692550659 22.556257247924805
  batch 40 loss: 1.791670322418213, 2.076458692550659, 22.556257247924805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921977043151855 2.0961685180664062 22.753883361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79242742061615 2.1304538249969482 23.096965789794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792075276374817 2.2749695777893066 24.541770935058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916070222854614 2.1629083156585693 23.420690536499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925313711166382 1.9622608423233032 21.41514015197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922073602676392 2.2282423973083496 24.07463264465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920863628387451 2.083265781402588 22.62474250793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919167280197144 2.146991729736328 23.26183319091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916074991226196 2.239006996154785 24.181676864624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920564413070679 1.735432505607605 19.146381378173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910374402999878 2.3149807453155518 24.94084358215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923945188522339 2.205106019973755 23.843454360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922611236572266 2.2300479412078857 24.092741012573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926145792007446 2.121699571609497 23.00960922241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918546199798584 1.9698371887207031 21.49022674560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925935983657837 1.7205578088760376 18.998170852661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927799224853516 2.456759214401245 26.36037254333496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927557229995728 2.305222272872925 24.84497833251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932002544403076 2.466188669204712 26.45508575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921528816223145 2.2767996788024902 24.560150146484375
  batch 60 loss: 1.7921528816223145, 2.2767996788024902, 24.560150146484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927943468093872 2.338381290435791 25.176607131958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916715145111084 2.187072515487671 23.662397384643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925806045532227 2.13493275642395 23.14190673828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792159914970398 2.0142910480499268 21.935070037841797
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927411794662476 1.9764480590820312 21.557222366333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918741703033447 2.2996861934661865 24.78873634338379
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917845249176025 2.338392496109009 25.175708770751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79156494140625 1.9689123630523682 21.480688095092773
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919062376022339 1.9781309366226196 21.57321548461914
Total LOSS train 23.368101677527793 valid 23.254587173461914
CE LOSS train 1.7922805272615874 valid 0.44797655940055847
Contrastive LOSS train 2.157582121628981 valid 0.4945327341556549
EPOCH 184:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922495603561401 2.198133945465088 23.773588180541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791532039642334 2.433396339416504 26.12549591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925139665603638 2.1564157009124756 23.356672286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924011945724487 2.1355178356170654 23.147579193115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792088270187378 2.281377077102661 24.605857849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792919635772705 2.324458599090576 25.037506103515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918691635131836 2.393336057662964 25.725231170654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919197082519531 2.243389129638672 24.225811004638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917463779449463 2.216034173965454 23.95208740234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917684316635132 2.1291112899780273 23.082881927490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792769193649292 2.34055495262146 25.198318481445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924389839172363 2.0556952953338623 22.34939193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930395603179932 1.928505778312683 21.078096389770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926803827285767 2.3047118186950684 24.839799880981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910480499267578 2.1769113540649414 23.560161590576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792771339416504 2.2632415294647217 24.425186157226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926191091537476 2.0979442596435547 22.772062301635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924710512161255 2.4347245693206787 26.13971710205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792354941368103 2.144597053527832 23.238325119018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915881872177124 1.7370022535324097 19.161609649658203
  batch 20 loss: 1.7915881872177124, 1.7370022535324097, 19.161609649658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925091981887817 1.9972889423370361 21.765398025512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924835681915283 2.0995240211486816 22.787723541259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922567129135132 1.852108120918274 20.313339233398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792749285697937 2.1412088871002197 23.204837799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924185991287231 2.523838520050049 27.030803680419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.3100035190582275 24.892576217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929339408874512 2.1994469165802 23.787403106689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918932437896729 2.1782240867614746 23.574134826660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934260368347168 2.225003719329834 24.04346466064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914907932281494 2.0776307582855225 22.567798614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926084995269775 2.3821451663970947 25.614059448242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913888692855835 2.1920852661132812 23.712242126464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 2.057772397994995 22.370203018188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926124334335327 2.196338176727295 23.75599479675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925559282302856 2.4587416648864746 26.379974365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926692962646484 2.350567579269409 25.2983455657959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586088180542 2.1650400161743164 23.44298553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916265726089478 2.152280569076538 23.31443214416504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921736240386963 2.331658124923706 25.108755111694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916672229766846 2.2254228591918945 24.045896530151367
  batch 40 loss: 1.7916672229766846, 2.2254228591918945, 24.045896530151367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921943664550781 2.2409207820892334 24.20140266418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792423129081726 2.1177031993865967 22.96945571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920697927474976 2.3281779289245605 25.073850631713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916003465652466 2.3142406940460205 24.93400764465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925201654434204 1.9261486530303955 21.054006576538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921974658966064 2.2401554584503174 24.19375228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920788526535034 2.1000607013702393 22.79268455505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791916847229004 1.9455312490463257 21.247230529785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916141748428345 2.3852179050445557 25.6437931060791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920689582824707 1.9473662376403809 21.265731811523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791048526763916 1.9269458055496216 21.06050682067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924007177352905 2.2851312160491943 24.643712997436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922625541687012 2.175379753112793 23.54606056213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926101684570312 1.9682788848876953 21.475399017333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918477058410645 1.9266263246536255 21.0581111907959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792587399482727 2.1866443157196045 23.65903091430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927744388580322 2.4980618953704834 26.773393630981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927515506744385 2.151047945022583 23.30323028564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931954860687256 2.4084064960479736 25.877260208129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921541929244995 2.2309279441833496 24.1014347076416
  batch 60 loss: 1.7921541929244995, 2.2309279441833496, 24.1014347076416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927943468093872 2.1731886863708496 23.524682998657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916725873947144 2.2973146438598633 24.76481819152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580008506775 2.1101365089416504 22.893943786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921546697616577 1.9043967723846436 20.836122512817383
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927354574203491 1.8025281429290771 19.818017959594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918709516525269 2.2027528285980225 23.819398880004883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917819023132324 2.189497470855713 23.686756134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915626764297485 1.9021122455596924 20.812685012817383
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919023036956787 1.6410900354385376 18.202802658081055
Total LOSS train 23.59257521996131 valid 21.63041067123413
CE LOSS train 1.792278658426725 valid 0.4479755759239197
Contrastive LOSS train 2.180029637996967 valid 0.4102725088596344
EPOCH 185:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922457456588745 2.2085115909576416 23.877361297607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915318012237549 2.499047040939331 26.78200340270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925143241882324 2.3900868892669678 25.693384170532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924033403396606 2.157231092453003 23.364715576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920893430709839 2.0813820362091064 22.60590934753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929190397262573 2.2492637634277344 24.28555679321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918657064437866 2.0969951152801514 22.761817932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919162511825562 2.2298710346221924 24.090625762939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917437553405762 2.234710931777954 24.138853073120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917636632919312 2.049572467803955 22.287487030029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927615642547607 2.3653974533081055 25.446735382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924320697784424 2.365452766418457 25.44696044921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930338382720947 2.3102500438690186 24.89553451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926791906356812 2.3377530574798584 25.170209884643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910534143447876 2.272491455078125 24.515968322753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927733659744263 2.4217844009399414 26.010618209838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926208972930908 2.243351459503174 24.22613525390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924644947052002 2.3637359142303467 25.429824829101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923460006713867 2.063490152359009 22.42724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915796041488647 1.916841745376587 20.959997177124023
  batch 20 loss: 1.7915796041488647, 1.916841745376587, 20.959997177124023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792500615119934 2.3619983196258545 25.412485122680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924784421920776 2.2632431983947754 24.424909591674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922559976577759 1.9992108345031738 21.78436279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927491664886475 2.072456121444702 22.517311096191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419672012329 2.510334014892578 26.89575958251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792546033859253 2.29201340675354 24.712678909301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929387092590332 2.377756357192993 25.57050132751465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918955087661743 2.1423025131225586 23.214920043945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934218645095825 2.122795581817627 23.021377563476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914927005767822 2.295562744140625 24.747119903564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926117181777954 2.5226550102233887 27.019163131713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791396141052246 2.377727746963501 25.56867218017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924822568893433 1.9626028537750244 21.41851043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926156520843506 2.0140650272369385 21.933265686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925602197647095 2.3633716106414795 25.42627716064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926689386367798 2.147364377975464 23.266313552856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925838232040405 2.2190589904785156 23.983173370361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 1.6958298683166504 18.749921798706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921684980392456 2.2327094078063965 24.1192626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916622161865234 1.8654853105545044 20.446516036987305
  batch 40 loss: 1.7916622161865234, 1.8654853105545044, 20.446516036987305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792191743850708 2.1532628536224365 23.32482147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924211025238037 2.139491558074951 23.18733787536621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792070746421814 2.265838861465454 24.450458526611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.1562609672546387 23.354215621948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925232648849487 2.0583953857421875 22.376476287841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921994924545288 1.8404686450958252 20.19688606262207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920817136764526 2.1071174144744873 22.863256454467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919187545776367 2.0936977863311768 22.728897094726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612982749939 2.2535011768341064 24.326623916625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920633554458618 1.9026925563812256 20.818988800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910456657409668 2.1986303329467773 23.7773494720459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923965454101562 2.3227643966674805 25.02004051208496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922630310058594 2.1629481315612793 23.42174530029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 2.282644033432007 24.619050979614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918459177017212 2.3617379665374756 25.409225463867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580246925354 1.6833235025405884 18.62581443786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927653789520264 2.5599398612976074 27.39216423034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927443981170654 2.1357762813568115 23.1505069732666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793192982673645 2.499095916748047 26.78415298461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921531200408936 2.2595326900482178 24.387479782104492
  batch 60 loss: 1.7921531200408936, 2.2595326900482178, 24.387479782104492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792791724205017 2.263582468032837 24.42861557006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916760444641113 1.9420807361602783 21.212482452392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925820350646973 2.111457347869873 22.907155990600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792158842086792 2.271980047225952 24.511959075927734
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927321195602417 1.5522319078445435 17.31505012512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918696403503418 2.3436291217803955 25.228160858154297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917804718017578 2.225843906402588 24.05021858215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915616035461426 2.1154403686523438 22.945964813232422
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919023036956787 1.8806575536727905 20.598478317260742
Total LOSS train 23.680618491539587 valid 23.205705642700195
CE LOSS train 1.7922774589978732 valid 0.4479755759239197
Contrastive LOSS train 2.188834100503188 valid 0.47016438841819763
EPOCH 186:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922446727752686 2.2202999591827393 23.995243072509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915314435958862 2.3609700202941895 25.401229858398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925102710723877 2.2279574871063232 24.072086334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923966646194458 2.0967907905578613 22.760303497314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920833826065063 1.692429780960083 18.716381072998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929165363311768 2.251253604888916 24.305452346801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.046591281890869 22.257776260375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919138669967651 2.109297275543213 22.884885787963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917404174804688 2.143707752227783 23.228816986083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917613983154297 2.265469789505005 24.44645881652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792758584022522 2.354473352432251 25.337491989135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924329042434692 2.4614412784576416 26.406845092773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930330038070679 2.3956987857818604 25.75002098083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680025100708 2.229346990585327 24.086151123046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910550832748413 1.8578333854675293 20.3693904876709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927697896957397 2.484628438949585 26.639055252075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926145792007446 2.0796990394592285 22.589603424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792460322380066 2.438469648361206 26.177156448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923446893692017 1.899349331855774 20.785839080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79158353805542 2.3708887100219727 25.500471115112305
  batch 20 loss: 1.79158353805542, 2.3708887100219727, 25.500471115112305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925012111663818 2.016188859939575 21.954389572143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924809455871582 2.287795066833496 24.67043113708496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792256236076355 2.1274123191833496 23.06637954711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927502393722534 1.8454697132110596 20.247446060180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 2.542161226272583 27.214031219482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925447225570679 2.2702598571777344 24.49514389038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929342985153198 2.3286335468292236 25.079269409179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791892647743225 2.278428792953491 24.576181411743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934154272079468 2.3878705501556396 25.672121047973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791492223739624 2.0177783966064453 21.969276428222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926069498062134 2.374528646469116 25.537893295288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913920879364014 2.2313473224639893 24.1048641204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924784421920776 2.230748176574707 24.099960327148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926119565963745 2.3006131649017334 24.798744201660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792558193206787 2.4365415573120117 26.157974243164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792667031288147 2.335691213607788 25.149580001831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925782203674316 2.3963990211486816 25.756567001342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916207313537598 2.166752815246582 23.459148406982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921682596206665 2.3387558460235596 25.179725646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79166579246521 1.9122052192687988 20.91371726989746
  batch 40 loss: 1.79166579246521, 1.9122052192687988, 20.91371726989746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921953201293945 2.199427366256714 23.786468505859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924234867095947 1.9893802404403687 21.68622589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920681238174438 2.2816951274871826 24.609020233154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916001081466675 2.0736453533172607 22.528053283691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925163507461548 2.1161439418792725 22.953956604003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921942472457886 2.190525770187378 23.697452545166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920805215835571 2.0530221462249756 22.322301864624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919187545776367 2.160327911376953 23.395198822021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916096448898315 2.3944923877716064 25.73653221130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792059302330017 1.6241275072097778 18.033334732055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910432815551758 1.8997268676757812 20.788311004638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923929691314697 2.435591459274292 26.14830780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922625541687012 1.699041724205017 18.78268051147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792611837387085 2.18037486076355 23.59636116027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791849970817566 2.2308056354522705 24.099905014038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925857305526733 2.2399935722351074 24.192522048950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927727699279785 2.4990293979644775 26.783065795898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927519083023071 1.7576537132263184 19.36928939819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931947708129883 2.482090473175049 26.614097595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921537160873413 2.2873284816741943 24.66543960571289
  batch 60 loss: 1.7921537160873413, 2.2873284816741943, 24.66543960571289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927886247634888 2.1891801357269287 23.68459129333496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916748523712158 2.0693860054016113 22.48553466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925795316696167 2.1150965690612793 22.943546295166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921606302261353 2.011108636856079 21.90324592590332
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927391529083252 1.809673547744751 19.889474868774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918773889541626 2.277557134628296 24.56744956970215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917876243591309 2.286195993423462 24.65374755859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 1.9403233528137207 21.194801330566406
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919104099273682 1.9842451810836792 21.634361267089844
Total LOSS train 23.607821860680215 valid 23.012589931488037
CE LOSS train 1.7922762944148136 valid 0.44797760248184204
Contrastive LOSS train 2.1815545668968785 valid 0.4960612952709198
EPOCH 187:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922509908676147 2.2901241779327393 24.693492889404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915349006652832 2.4820852279663086 26.61238670349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792509913444519 2.3412482738494873 25.204992294311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397141456604 2.205270290374756 23.84510040283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920829057693481 2.180943012237549 23.601512908935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929097414016724 2.340261936187744 25.195528030395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 2.3924877643585205 25.71673583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791906714439392 2.189385414123535 23.685760498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917340993881226 2.270843267440796 24.500167846679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917537689208984 1.841292142868042 20.204675674438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927523851394653 2.261652946472168 24.409282684326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924296855926514 2.4405405521392822 26.197834014892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930268049240112 2.2691099643707275 24.4841251373291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926740646362305 2.331857919692993 25.111251831054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910549640655518 2.3612701892852783 25.403757095336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792770504951477 2.050711154937744 22.299880981445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926194667816162 1.830587387084961 20.098493576049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924623489379883 2.336422920227051 25.156692504882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923450469970703 1.891236424446106 20.704710006713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915805578231812 1.9547003507614136 21.33858299255371
  batch 20 loss: 1.7915805578231812, 1.9547003507614136, 21.33858299255371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792497158050537 2.3169350624084473 24.96184730529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924771308898926 2.302924633026123 24.82172393798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922545671463013 1.9938455820083618 21.730710983276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927476167678833 2.258046865463257 24.37321662902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419672012329 2.4427030086517334 26.219449996948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925423383712769 2.2144665718078613 23.937206268310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792929768562317 2.3243179321289062 25.036109924316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918858528137207 2.2519919872283936 24.311805725097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793407917022705 2.2282938957214355 24.07634735107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914905548095703 2.233945608139038 24.13094711303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926043272018433 2.365750789642334 25.45011329650879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791392207145691 2.2133629322052 23.925020217895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924734354019165 2.0901525020599365 22.693998336791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926025390625 2.2614126205444336 24.406728744506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925477027893066 2.513962984085083 26.93217658996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792658805847168 2.174225091934204 23.534908294677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792575478553772 2.4179296493530273 25.971872329711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916228771209717 2.1266019344329834 23.057641983032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79216730594635 2.2247045040130615 24.039213180541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79166579246521 2.234747886657715 24.139144897460938
  batch 40 loss: 1.79166579246521, 2.234747886657715, 24.139144897460938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921947240829468 2.1989684104919434 23.781879425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924233675003052 2.0749752521514893 22.54217529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920687198638916 2.422281503677368 26.014883041381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.198920488357544 23.78080940246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519211769104 1.9588152170181274 21.38067054748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921946048736572 2.282233715057373 24.614532470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920781373977661 1.9451824426651 21.2439022064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791917324066162 2.0714149475097656 22.506067276000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916104793548584 2.1368420124053955 23.160030364990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920606136322021 2.0685079097747803 22.477140426635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910453081130981 2.2794389724731445 24.58543586730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923897504806519 2.1598289012908936 23.39067840576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922592163085938 2.0616674423217773 22.408933639526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926084995269775 2.1732871532440186 23.525480270385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918506860733032 2.2778851985931396 24.570703506469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925846576690674 2.021446704864502 22.007051467895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79276442527771 2.33844256401062 25.17719078063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927409410476685 1.9588844776153564 21.3815860748291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931851148605347 2.472557544708252 26.51875877380371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921507358551025 2.141935110092163 23.211502075195312
  batch 60 loss: 1.7921507358551025, 2.141935110092163, 23.211502075195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927895784378052 2.4261982440948486 26.054771423339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916779518127441 2.165409803390503 23.445775985717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925796508789062 2.2394065856933594 24.1866455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921576499938965 1.8654013872146606 20.4461727142334
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927314043045044 1.4108922481536865 15.901654243469238
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918694019317627 2.155679225921631 23.348663330078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917803525924683 2.096435070037842 22.756132125854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915619611740112 1.840321660041809 20.194778442382812
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919031381607056 1.903030514717102 20.822208404541016
Total LOSS train 23.761993510906514 valid 21.78044557571411
CE LOSS train 1.7922739322368915 valid 0.4479757845401764
Contrastive LOSS train 2.1969719630021314 valid 0.4757576286792755
EPOCH 188:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922433614730835 2.2725915908813477 24.518159866333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791528344154358 2.344303607940674 25.23456382751465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792501449584961 2.028519868850708 22.077699661254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923932075500488 1.7798656225204468 19.591049194335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792082667350769 2.099647045135498 22.78855323791504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929136753082275 2.1807713508605957 23.60062599182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864037513733 2.272388458251953 24.515748977661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919151782989502 1.897698998451233 20.768905639648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 2.0097858905792236 21.889598846435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917598485946655 2.108614206314087 22.877901077270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927559614181519 2.342440366744995 25.217159271240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79243004322052 2.392230987548828 25.714740753173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793025016784668 2.0257794857025146 22.050819396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79267418384552 2.3569176197052 25.36185073852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910597324371338 2.2837612628936768 24.628671646118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792769432067871 2.36202335357666 25.413002014160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792618989944458 2.175382375717163 23.546443939208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924606800079346 2.2271342277526855 24.063804626464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923434972763062 2.003943681716919 21.83177947998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915817499160767 2.316042900085449 24.952011108398438
  batch 20 loss: 1.7915817499160767, 2.316042900085449, 24.952011108398438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792493224143982 2.1608664989471436 23.401159286499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924712896347046 2.151934862136841 23.311819076538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922475337982178 1.5166445970535278 16.95869255065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927440404891968 2.211688756942749 23.909631729125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924184799194336 2.3639252185821533 25.431671142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925435304641724 1.9492113590240479 21.284656524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792929768562317 2.169677972793579 23.489709854125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79188871383667 2.081427812576294 22.60616683959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7934045791625977 2.467141628265381 26.464820861816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791491150856018 2.165804862976074 23.449539184570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926008701324463 2.5226519107818604 27.019119262695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913899421691895 2.5040173530578613 26.831562042236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924690246582031 2.2069478034973145 23.86194610595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925999164581299 2.147221088409424 23.26481056213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925467491149902 2.3516907691955566 25.3094539642334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926579713821411 2.222370147705078 24.016359329223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925740480422974 2.1646697521209717 23.439271926879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916229963302612 2.233928680419922 24.130908966064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792169213294983 2.443190813064575 26.224077224731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916650772094727 2.2128641605377197 23.920307159423828
  batch 40 loss: 1.7916650772094727, 2.2128641605377197, 23.920307159423828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792191982269287 1.8685253858566284 20.477445602416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924166917800903 2.0746991634368896 22.539409637451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920637130737305 2.134856939315796 23.14063262939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916017770767212 2.043025255203247 22.221853256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519211769104 2.2252931594848633 24.04545021057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921943664550781 2.368427038192749 25.476465225219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920774221420288 2.0933616161346436 22.72569465637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791914939880371 2.167158603668213 23.4635009765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79160737991333 2.0825765132904053 22.617372512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920550107955933 2.1130282878875732 22.922338485717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910412549972534 2.4442358016967773 26.2333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923839092254639 2.2654221057891846 24.446603775024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922518253326416 2.152221441268921 23.31446647644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600393295288 2.321509838104248 25.007699966430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918479442596436 2.2774808406829834 24.5666561126709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792582631111145 2.277003526687622 24.562618255615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927700281143188 2.472424030303955 26.517009735107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792750597000122 2.2126166820526123 23.91891860961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79319167137146 2.4287261962890625 26.080453872680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921547889709473 2.347878932952881 25.270944595336914
  batch 60 loss: 1.7921547889709473, 2.347878932952881, 25.270944595336914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927860021591187 2.1675307750701904 23.468093872070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916710376739502 2.277904510498047 24.570716857910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925646305084229 2.191749334335327 23.710058212280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792140007019043 2.3881449699401855 25.67359161376953
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927137613296509 1.8023433685302734 19.816146850585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 2.131063222885132 23.102495193481445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917739152908325 2.1071178913116455 22.862953186035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915557622909546 1.8585340976715088 20.376895904541016
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894793510437 1.7788857221603394 19.580751419067383
Total LOSS train 23.78086635883038 valid 21.48077392578125
CE LOSS train 1.792272030390226 valid 0.44797369837760925
Contrastive LOSS train 2.1988594348614035 valid 0.44472143054008484
EPOCH 189:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922353744506836 2.1710598468780518 23.50283432006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915265560150146 2.300652027130127 24.798046112060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792505145072937 2.169853448867798 23.491039276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923989295959473 2.2196738719940186 23.989137649536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920854091644287 2.0716631412506104 22.508716583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929117679595947 2.278717517852783 24.58008575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918577194213867 2.1123170852661133 22.915027618408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791907787322998 1.937157392501831 21.163482666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917333841323853 2.314358711242676 24.935319900512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917555570602417 2.1274850368499756 23.066606521606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927513122558594 2.377208948135376 25.56484031677246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924275398254395 2.3503196239471436 25.295623779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930190563201904 2.1470422744750977 23.26344108581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926695346832275 2.06292986869812 22.421968460083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791056752204895 2.3166847229003906 24.957904815673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927626371383667 2.3991494178771973 25.784255981445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792612910270691 2.1394853591918945 23.18746566772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924546003341675 2.1524269580841064 23.316722869873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923400402069092 2.0008561611175537 21.800901412963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915812730789185 2.1250975131988525 23.042556762695312
  batch 20 loss: 1.7915812730789185, 2.1250975131988525, 23.042556762695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924935817718506 2.1182594299316406 22.975088119506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792474627494812 2.1268668174743652 23.06114387512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922497987747192 1.5324478149414062 17.116727828979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927417755126953 2.1923773288726807 23.716514587402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924119234085083 2.007300853729248 21.865421295166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253351688385 2.1254355907440186 23.046890258789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929211854934692 2.4050397872924805 25.843318939208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918866872787476 2.3212461471557617 25.004348754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79340398311615 2.2536332607269287 24.329736709594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914974689483643 2.075174331665039 22.543241500854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926055192947388 2.3298118114471436 25.09072494506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791395664215088 2.26503324508667 24.441728591918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792467713356018 2.178156852722168 23.57403564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925987243652344 2.4397308826446533 26.18990707397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925463914871216 2.3246238231658936 25.038785934448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926592826843262 2.41058349609375 25.898494720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925753593444824 2.1299655437469482 23.09223175048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916219234466553 2.1256868839263916 23.048490524291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921642065048218 2.438199281692505 26.174156188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165780544281 2.2102341651916504 23.893999099731445
  batch 40 loss: 1.79165780544281, 2.2102341651916504, 23.893999099731445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921870946884155 2.188096523284912 23.673152923583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792415738105774 2.1742682456970215 23.535099029541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920665740966797 1.8521238565444946 20.313304901123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916074991226196 1.7717633247375488 19.509239196777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925165891647339 2.2918782234191895 24.7112979888916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921897172927856 2.3111886978149414 24.904077529907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920732498168945 1.9730525016784668 21.522598266601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791913390159607 2.2739813327789307 24.531726837158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916086912155151 2.316815137863159 24.959760665893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.0164835453033447 21.956897735595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910501956939697 2.3479690551757812 25.270740509033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923853397369385 2.192690849304199 23.71929359436035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922546863555908 1.6532070636749268 18.324325561523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792602777481079 2.256283760070801 24.355440139770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918483018875122 2.251734733581543 24.30919647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792583703994751 2.1576952934265137 23.369537353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927619218826294 1.970310926437378 21.495872497558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927368879318237 2.2114312648773193 23.90704917907715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793174147605896 2.438206672668457 26.175241470336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792142391204834 2.1343491077423096 23.13563346862793
  batch 60 loss: 1.792142391204834, 2.1343491077423096, 23.13563346862793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792773723602295 2.125122308731079 23.043996810913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791670799255371 2.1388473510742188 23.180145263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925673723220825 1.9442566633224487 21.23513412475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921465635299683 2.2500874996185303 24.29302215576172
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927193641662598 1.6356860399246216 18.149580001831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918661832809448 2.425795078277588 26.049816131591797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917767763137817 2.3052666187286377 24.84444236755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791558861732483 2.0583949089050293 22.37550926208496
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919009923934937 1.983323335647583 21.625133514404297
Total LOSS train 23.432497347318208 valid 23.72372531890869
CE LOSS train 1.792270146883451 valid 0.4479752480983734
Contrastive LOSS train 2.1640227116071262 valid 0.49583083391189575
EPOCH 190:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922395467758179 2.306710720062256 24.85934829711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915323972702026 2.5784401893615723 27.5759334564209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792503833770752 2.003178119659424 21.824283599853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923964262008667 2.080704927444458 22.599445343017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920855283737183 2.1365888118743896 23.157974243164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929120063781738 2.278458833694458 24.577499389648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918602228164673 2.3323445320129395 25.115304946899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791912317276001 2.1621944904327393 23.413856506347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917368412017822 2.042423725128174 22.215972900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917561531066895 2.00618314743042 21.853588104248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927504777908325 2.424130916595459 26.034061431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924284934997559 2.257117509841919 24.363603591918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930163145065308 1.908313274383545 20.876150131225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792663812637329 2.2403724193573 24.196388244628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910561561584473 2.1812777519226074 23.60383415222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792756199836731 2.42144775390625 26.007234573364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926084995269775 2.2680935859680176 24.473543167114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924476861953735 2.0313148498535156 22.1055965423584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923367023468018 1.9042212963104248 20.834550857543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915788888931274 2.064918279647827 22.44076156616211
  batch 20 loss: 1.7915788888931274, 2.064918279647827, 22.44076156616211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792488694190979 1.9550176858901978 21.34266471862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 2.162830352783203 23.4207763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922500371932983 1.9464478492736816 21.25672721862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927449941635132 2.22733473777771 24.06609344482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924164533615112 2.4916622638702393 26.70903778076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925342321395874 2.244093179702759 24.23346519470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929136753082275 2.2357733249664307 24.150646209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918766736984253 2.0420479774475098 22.21235466003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933872938156128 2.167365550994873 23.467042922973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914875745773315 2.2283387184143066 24.074872970581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925888299942017 2.3011484146118164 24.804073333740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913897037506104 2.431072235107422 26.10211181640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924672365188599 2.15869402885437 23.37940788269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600393295288 2.460340976715088 26.39600944519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254949092865 2.4856221675872803 26.648771286010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926592826843262 2.4145684242248535 25.938343048095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925729751586914 2.2155563831329346 23.948135375976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791619896888733 2.193326711654663 23.72488784790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921608686447144 2.267183780670166 24.46399688720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916560173034668 2.344210624694824 25.233762741088867
  batch 40 loss: 1.7916560173034668, 2.344210624694824, 25.233762741088867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792184591293335 2.3619983196258545 25.412168502807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792412281036377 1.9780561923980713 21.572973251342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79206383228302 2.458958387374878 26.381649017333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916069030761719 2.3450653553009033 25.242259979248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925163507461548 2.063143730163574 22.423954010009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921912670135498 2.2188591957092285 23.98078155517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920751571655273 2.0974183082580566 22.766258239746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919107675552368 2.0748512744903564 22.540422439575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.253800630569458 24.329608917236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920551300048828 1.9170769453048706 20.96282386779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910487651824951 2.4090652465820312 25.88170051574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923893928527832 2.389009952545166 25.68248748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922638654708862 2.0847060680389404 22.639324188232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926087379455566 1.8340952396392822 20.133560180664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918537855148315 2.3615176677703857 25.40703010559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925807237625122 2.3009767532348633 24.802349090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927560806274414 2.415175437927246 25.94451141357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792733073234558 2.2053205966949463 23.845937728881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931746244430542 2.5415802001953125 27.20897674560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921491861343384 2.2708580493927 24.500728607177734
  batch 60 loss: 1.7921491861343384, 2.2708580493927, 24.500728607177734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927838563919067 2.3183157444000244 24.975940704345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916826009750366 2.210216760635376 23.893850326538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925775051116943 2.1707921028137207 23.500497817993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921531200408936 2.3669679164886475 25.46183204650879
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927217483520508 1.8219399452209473 20.01211929321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918658256530762 2.102257490158081 22.814441680908203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917765378952026 2.122580051422119 23.017576217651367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915592193603516 1.8603447675704956 20.39500617980957
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919001579284668 1.9231698513031006 21.02359962463379
Total LOSS train 23.95701320354755 valid 21.812655925750732
CE LOSS train 1.7922694206237793 valid 0.4479750394821167
Contrastive LOSS train 2.216474408369798 valid 0.48079246282577515
EPOCH 191:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922382354736328 2.1482818126678467 23.275056838989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915287017822266 2.4723708629608154 26.51523780822754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924954891204834 1.7976152896881104 19.768648147583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923920154571533 2.0619966983795166 22.4123592376709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792084813117981 2.0946686267852783 22.738771438598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929134368896484 2.1563799381256104 23.356712341308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 2.3386871814727783 25.178735733032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919174432754517 2.1439056396484375 23.230974197387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917417287826538 2.090878963470459 22.700532913208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917591333389282 2.115432024002075 22.94607925415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927507162094116 2.209007501602173 23.88282585144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924286127090454 2.427410840988159 26.066537857055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793013334274292 2.291210174560547 24.705114364624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792660117149353 2.4369778633117676 26.162437438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910540103912354 2.161785840988159 23.408912658691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927520275115967 2.380756378173828 25.60031509399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792605996131897 2.2747740745544434 24.540348052978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924498319625854 2.3345859050750732 25.138309478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792340636253357 1.9802772998809814 21.59511375427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915832996368408 2.1570425033569336 23.362009048461914
  batch 20 loss: 1.7915832996368408, 2.1570425033569336, 23.362009048461914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924892902374268 2.065418004989624 22.446670532226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792472243309021 2.19497013092041 23.74217414855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922511100769043 1.907204270362854 20.864294052124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927464246749878 2.1569736003875732 23.36248207092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924182415008545 2.5695436000823975 27.48785400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792535424232483 2.2088308334350586 23.880844116210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929141521453857 2.4073424339294434 25.8663387298584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918791770935059 2.1827096939086914 23.618976593017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933874130249023 2.294430732727051 24.737693786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914916276931763 2.1569409370422363 23.36090087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925955057144165 2.287069082260132 24.663286209106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913938760757446 2.0252349376678467 22.043743133544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924704551696777 2.2491393089294434 24.283864974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792603611946106 2.3682868480682373 25.475473403930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925517559051514 2.170048236846924 23.493032455444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926571369171143 2.105778217315674 22.850439071655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792572259902954 2.0373711585998535 22.166282653808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916244268417358 2.3084793090820312 24.87641716003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921676635742188 2.236020088195801 24.152368545532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791663646697998 2.2993838787078857 24.785503387451172
  batch 40 loss: 1.791663646697998, 2.2993838787078857, 24.785503387451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792191743850708 2.3463923931121826 25.25611686706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924164533615112 1.9121848344802856 20.914264678955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920631170272827 2.214022636413574 23.932289123535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916078567504883 2.0547492504119873 22.339099884033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514443397522 1.628173589706421 18.074251174926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792189598083496 2.3107900619506836 24.900089263916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920738458633423 1.99840247631073 21.776098251342773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791911244392395 1.798345923423767 19.775371551513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916048765182495 2.0668225288391113 22.459829330444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920554876327515 2.11092472076416 22.901302337646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910505533218384 2.364211082458496 25.43316078186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923890352249146 2.1652674674987793 23.445064544677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792262315750122 2.1634531021118164 23.426794052124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926045656204224 2.184459686279297 23.6372013092041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918493747711182 2.349944829940796 25.291297912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925760746002197 2.1497600078582764 23.290176391601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927531003952026 2.3792974948883057 25.58572769165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927303314208984 2.251790761947632 24.310638427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931746244430542 2.3917336463928223 25.71051025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921515703201294 2.187063455581665 23.66278648376465
  batch 60 loss: 1.7921515703201294, 2.187063455581665, 23.66278648376465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927836179733276 2.1866185665130615 23.65896987915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916818857192993 2.228111505508423 24.072795867919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792571783065796 2.252255916595459 24.31513214111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921473979949951 1.99498450756073 21.74199104309082
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792714238166809 1.4868701696395874 16.66141700744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918657064437866 2.3017256259918213 24.80912208557129
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917767763137817 2.2435271739959717 24.227048873901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915594577789307 2.075450897216797 22.54606819152832
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919000387191772 2.030470609664917 22.096607208251953
Total LOSS train 23.55870842566857 valid 23.419711589813232
CE LOSS train 1.792269673714271 valid 0.4479750096797943
Contrastive LOSS train 2.17664386675908 valid 0.5076176524162292
EPOCH 192:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792237401008606 2.410402774810791 25.896265029907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915310859680176 2.633697271347046 28.128503799438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792495846748352 2.306501865386963 24.857513427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391300201416 2.15623140335083 23.354703903198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920812368392944 2.075918436050415 22.551265716552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929115295410156 2.231264591217041 24.10555648803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918661832809448 2.0527281761169434 22.319149017333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919201850891113 1.8642131090164185 20.434051513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917418479919434 2.115884304046631 22.950586318969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791753888130188 2.0295040607452393 22.086793899536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927383184432983 2.52883243560791 27.08106231689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924162149429321 2.2825746536254883 24.618162155151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929998636245728 1.7870501279830933 19.66349983215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926561832427979 2.092531442642212 22.71796989440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910584211349487 2.179176092147827 23.58281898498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79275643825531 2.4986846446990967 26.779603958129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926148176193237 2.3873484134674072 25.66609764099121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924585342407227 2.395524263381958 25.747699737548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923506498336792 1.932603120803833 21.11838150024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915928363800049 2.0461175441741943 22.252769470214844
  batch 20 loss: 1.7915928363800049, 2.0461175441741943, 22.252769470214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924906015396118 2.168877124786377 23.481260299682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924649715423584 2.1723744869232178 23.516210556030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922358512878418 1.9403873682022095 21.196109771728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927284240722656 1.7996649742126465 19.789379119873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924039363861084 2.3202807903289795 24.99521255493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523980140686 2.25252366065979 24.317760467529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929095029830933 2.3389947414398193 25.182857513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791883111000061 2.1999125480651855 23.7910099029541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933919429779053 2.4343044757843018 26.136436462402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914985418319702 2.171848773956299 23.509984970092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925931215286255 2.3363027572631836 25.155620574951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913910150527954 1.9454598426818848 21.245988845825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924633026123047 2.1643753051757812 23.436216354370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925952672958374 2.3345134258270264 25.13772964477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925440073013306 2.226321220397949 24.055755615234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926499843597412 2.367851972579956 25.47117042541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925620079040527 2.2187788486480713 23.980350494384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916183471679688 2.1642906665802 23.434524536132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921608686447144 2.3211982250213623 25.00414276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916563749313354 2.0205278396606445 21.99693489074707
  batch 40 loss: 1.7916563749313354, 2.0205278396606445, 21.99693489074707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792187213897705 2.3825621604919434 25.617809295654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924150228500366 2.0702555179595947 22.494970321655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920633554458618 2.348965644836426 25.281719207763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916083335876465 2.164609432220459 23.43770408630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925103902816772 1.7903395891189575 19.695907592773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921847105026245 1.9400910139083862 21.19309425354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79207181930542 1.982441782951355 21.61648941040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919093370437622 2.033539056777954 22.127300262451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79160475730896 2.0976269245147705 22.767873764038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920551300048828 2.1429531574249268 23.221586227416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910500764846802 2.385148286819458 25.642532348632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792377233505249 2.2926793098449707 24.71916961669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922484874725342 2.197852373123169 23.770771026611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925933599472046 2.3268749713897705 25.061342239379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918426990509033 1.923359751701355 21.025440216064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925746440887451 2.1157846450805664 22.950420379638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927534580230713 2.35089373588562 25.30169105529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927314043045044 2.1597323417663574 23.390056610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931677103042603 2.269684314727783 24.490009307861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921451330184937 2.307184934616089 24.863994598388672
  batch 60 loss: 1.7921451330184937, 2.307184934616089, 24.863994598388672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792772650718689 2.411250114440918 25.9052734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916746139526367 2.12713360786438 23.063011169433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925642728805542 1.968667984008789 21.479244232177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921416759490967 2.0698046684265137 22.490188598632812
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792710542678833 1.7413524389266968 19.206235885620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791868805885315 2.461747407913208 26.40934181213379
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917802333831787 2.388310432434082 25.674884796142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915630340576172 2.2254979610443115 24.04654312133789
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919033765792847 2.2020387649536133 23.81229019165039
Total LOSS train 23.562476084782528 valid 24.985764980316162
CE LOSS train 1.7922660919336173 valid 0.44797584414482117
Contrastive LOSS train 2.1770210082714376 valid 0.5505096912384033
EPOCH 193:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792239785194397 2.3678057193756104 25.47029685974121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915345430374146 2.5485494136810303 27.277029037475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924973964691162 2.0904884338378906 22.6973819732666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792389154434204 2.100128412246704 22.793672561645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920764684677124 2.167841672897339 23.47049331665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928978204727173 2.123904228210449 23.031940460205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918524742126465 2.2620012760162354 24.411865234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791906476020813 2.193653106689453 23.728437423706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791735291481018 2.2035531997680664 23.827266693115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917530536651611 2.156848192214966 23.3602352142334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927428483963013 2.399024486541748 25.782989501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924250364303589 2.5025267601013184 26.81769371032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930060625076294 2.2897443771362305 24.69045066833496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926602363586426 2.1863136291503906 23.65579605102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910643815994263 2.3315634727478027 25.106700897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927546501159668 2.445211887359619 26.244873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926068305969238 2.143566131591797 23.228267669677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924494743347168 2.112628936767578 22.918739318847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923418283462524 1.9676450490951538 21.468791961669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915865182876587 2.0297811031341553 22.089397430419922
  batch 20 loss: 1.7915865182876587, 2.0297811031341553, 22.089397430419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924822568893433 2.1627721786499023 23.420204162597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924576997756958 1.8279660940170288 20.072118759155273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922314405441284 2.1232216358184814 23.024446487426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927253246307373 2.307852268218994 24.871246337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924026250839233 2.4337031841278076 26.12943458557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925223112106323 2.4574196338653564 26.366718292236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792903184890747 2.3478927612304688 25.271831512451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918763160705566 1.8711141347885132 20.50301742553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933812141418457 2.3115181922912598 24.90856170654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914937734603882 2.132631540298462 23.117809295654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792587399482727 2.361694574356079 25.40953254699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913928031921387 2.398516893386841 25.776561737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924619913101196 1.8539623022079468 20.33208465576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925925254821777 2.2207186222076416 23.999778747558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925406694412231 2.306384325027466 24.85638427734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926441431045532 2.1988751888275146 23.781396865844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925608158111572 2.3565614223480225 25.35817527770996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916181087493896 2.07246470451355 22.516265869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921621799468994 2.3742055892944336 25.534217834472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916592359542847 2.402235507965088 25.81401252746582
  batch 40 loss: 1.7916592359542847, 2.402235507965088, 25.81401252746582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921853065490723 2.140059232711792 23.192777633666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924104928970337 1.8738702535629272 20.531112670898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792056918144226 2.3897554874420166 25.689611434936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.393284797668457 25.72445297241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925059795379639 2.089569091796875 22.688196182250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921826839447021 2.2740061283111572 24.532243728637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792070746421814 2.013296365737915 21.925033569335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919152975082397 1.9998105764389038 21.790021896362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.1291987895965576 23.083593368530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920525074005127 1.8619277477264404 20.411331176757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910491228103638 1.9817439317703247 21.608489990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923744916915894 2.155092716217041 23.343299865722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922497987747192 2.1785666942596436 23.577917098999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925950288772583 2.273702383041382 24.529619216918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918481826782227 2.4005730152130127 25.797576904296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.091844320297241 22.71101951599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927496433258057 2.3510208129882812 25.30295753479004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927255630493164 1.812049150466919 19.91321563720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931640148162842 2.358036756515503 25.373531341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921431064605713 2.368062734603882 25.47277069091797
  batch 60 loss: 1.7921431064605713, 2.368062734603882, 25.47277069091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927666902542114 2.2927706241607666 24.72047233581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916711568832397 1.9401072263717651 21.1927433013916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556881904602 2.117638349533081 22.96894073486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921366691589355 2.314866065979004 24.940797805786133
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792708396911621 1.8134348392486572 19.92705535888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918668985366821 2.346001148223877 25.251876831054688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917782068252563 2.2844858169555664 24.63663673400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791561484336853 2.1170401573181152 22.961963653564453
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919014692306519 1.8644070625305176 20.435970306396484
Total LOSS train 23.755152306189903 valid 23.321611881256104
CE LOSS train 1.7922634179775532 valid 0.44797536730766296
Contrastive LOSS train 2.1962888974409838 valid 0.4661017656326294
EPOCH 194:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792236566543579 2.2860453128814697 24.65268898010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915327548980713 2.460169792175293 26.393230438232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924898862838745 2.1241819858551025 23.03430938720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923846244812012 2.222515821456909 24.01754379272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920737266540527 2.1209092140197754 23.00116539001465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79289972782135 2.2712173461914062 24.50507354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791854739189148 2.397148370742798 25.763338088989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190993309021 2.031797170639038 22.109882354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917357683181763 2.1168017387390137 22.959754943847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917523384094238 1.7191323041915894 18.983074188232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927393913269043 2.5739552974700928 27.53229331970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924219369888306 2.3103859424591064 24.89628028869629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792997121810913 2.120762348175049 23.000619888305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792651653289795 2.3368656635284424 25.16130828857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910598516464233 2.4085774421691895 25.876832962036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927442789077759 2.421295166015625 26.005695343017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926005125045776 2.346527338027954 25.25787353515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441725730896 2.3818957805633545 25.611400604248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923328876495361 2.1241819858551025 23.03415298461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915797233581543 1.6689627170562744 18.4812068939209
  batch 20 loss: 1.7915797233581543, 1.6689627170562744, 18.4812068939209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924766540527344 1.7064712047576904 18.857189178466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924606800079346 2.177927255630493 23.571733474731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792241096496582 1.5230544805526733 17.022785186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927354574203491 2.1291418075561523 23.08415412902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792414665222168 2.002235174179077 21.81476593017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925304174423218 2.288408041000366 24.676610946655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929078340530396 2.363126516342163 25.42417335510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918778657913208 2.2571890354156494 24.363767623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933769226074219 2.338677406311035 25.180150985717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914971113204956 1.995972752571106 21.751224517822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792588710784912 2.4528119564056396 26.320709228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791396975517273 2.212207794189453 23.913475036621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924615144729614 2.3666985034942627 25.45944595336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925922870635986 2.4028847217559814 25.821439743041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925419807434082 2.277740001678467 24.569942474365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926474809646606 2.3590028285980225 25.38267707824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925634384155273 2.283189535140991 24.62445831298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916204929351807 1.8405334949493408 20.19695472717285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792160153388977 2.146198272705078 23.25414276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.192899465560913 23.72064971923828
  batch 40 loss: 1.791654109954834, 2.192899465560913, 23.72064971923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921770811080933 2.181408405303955 23.606260299682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792402744293213 2.0098888874053955 21.89129066467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920525074005127 2.160639524459839 23.398448944091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916053533554077 2.261164426803589 24.403249740600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925090789794922 1.9758391380310059 21.550901412963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921886444091797 2.2633957862854004 24.426145553588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792076587677002 2.19260835647583 23.718158721923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919182777404785 2.138291120529175 23.174829483032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916090488433838 2.2695512771606445 24.48712158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920536994934082 2.0030441284179688 21.822494506835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910492420196533 2.501471757888794 26.805767059326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923704385757446 2.183804512023926 23.630414962768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922428846359253 2.111564874649048 22.90789031982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925868034362793 2.121326208114624 23.005849838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791843295097351 2.4786036014556885 26.577878952026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925745248794556 1.7277683019638062 19.07025718688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927531003952026 2.348721504211426 25.27996826171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927314043045044 2.0959553718566895 22.75228500366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931655645370483 2.5604376792907715 27.39754295349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921444177627563 2.2430379390716553 24.222524642944336
  batch 60 loss: 1.7921444177627563, 2.2430379390716553, 24.222524642944336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792761206626892 2.30655574798584 24.858318328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916673421859741 2.1394803524017334 23.186471939086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792549729347229 1.8101271390914917 19.89381980895996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792130708694458 1.9360193014144897 21.152324676513672
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926996946334839 1.3568824529647827 15.36152458190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866421699524 2.376154661178589 25.55341339111328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917780876159668 2.32568359375 25.048614501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915621995925903 2.128589391708374 23.077457427978516
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918994426727295 1.9790425300598145 21.58232307434082
Total LOSS train 23.50569100013146 valid 23.815452098846436
CE LOSS train 1.7922622827383188 valid 0.4479748606681824
Contrastive LOSS train 2.171342873573303 valid 0.4947606325149536
EPOCH 195:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922343015670776 2.3128271102905273 24.92050552368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791535496711731 2.5775725841522217 27.567262649536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924953699111938 2.1338775157928467 23.131271362304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923890352249146 2.150452136993408 23.29690933227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792076587677002 1.9696829319000244 21.48890495300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792893409729004 1.8811837434768677 20.604732513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918468713760376 2.21142840385437 23.906131744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918986082077026 2.066673517227173 22.458633422851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791723370552063 2.071016788482666 22.501890182495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917406558990479 1.6784356832504272 18.57609748840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927303314208984 2.3355672359466553 25.14840316772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792420744895935 2.373187780380249 25.52429962158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793000340461731 2.291180372238159 24.704805374145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926568984985352 2.3553483486175537 25.346141815185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910677194595337 2.2802958488464355 24.594026565551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792744755744934 2.3313839435577393 25.106584548950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925974130630493 2.2614259719848633 24.406856536865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792437195777893 2.2523305416107178 24.31574249267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923331260681152 1.9811818599700928 21.60415267944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791585087776184 2.077659845352173 22.56818389892578
  batch 20 loss: 1.791585087776184, 2.077659845352173, 22.56818389892578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924813032150269 2.361638069152832 25.40886116027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924665212631226 1.8860024213790894 20.652490615844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922412157058716 2.0863521099090576 22.655763626098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927318811416626 2.086519718170166 22.657928466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924073934555054 2.481389045715332 26.606298446655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79252290725708 2.3722617626190186 25.515140533447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928999662399292 2.1765031814575195 23.557931900024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918753623962402 2.086341619491577 22.655292510986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933703660964966 2.3299038410186768 25.092409133911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914979457855225 2.1070806980133057 22.8623046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925881147384644 2.408754825592041 25.88013458251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791398525238037 2.4142301082611084 25.933700561523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924575805664062 2.1616971492767334 23.4094295501709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925851345062256 2.1393826007843018 23.186410903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925331592559814 2.362907648086548 25.42160987854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926379442214966 2.479567766189575 26.588315963745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925554513931274 2.298560380935669 24.77815818786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791620135307312 2.266444444656372 24.456064224243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921632528305054 2.423048496246338 26.022647857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916618585586548 2.1993777751922607 23.78544044494629
  batch 40 loss: 1.7916618585586548, 2.1993777751922607, 23.78544044494629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921892404556274 2.291243314743042 24.704622268676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924144268035889 1.5903842449188232 17.696256637573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792059063911438 2.0639960765838623 22.43202018737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916085720062256 2.1723132133483887 23.514741897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925009727478027 2.1863341331481934 23.65584373474121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921782732009888 2.2833170890808105 24.625350952148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920690774917603 2.3250253200531006 25.042322158813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791910171508789 2.095276117324829 22.744670867919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916038036346436 2.278703212738037 24.578636169433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920533418655396 1.8436869382858276 20.22892189025879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910560369491577 2.2610480785369873 24.40153694152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923755645751953 2.2960662841796875 24.75303840637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922502756118774 1.8997304439544678 20.789554595947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925910949707031 2.1426961421966553 23.219552993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791844129562378 2.2665724754333496 24.457569122314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79257071018219 2.1640713214874268 23.43328285217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927463054656982 2.3144872188568115 24.937618255615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927253246307373 2.335144281387329 25.144166946411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931618690490723 2.4847710132598877 26.640871047973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921457290649414 2.2196671962738037 23.988819122314453
  batch 60 loss: 1.7921457290649414, 2.2196671962738037, 23.988819122314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792767882347107 2.2637128829956055 24.42989730834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916803359985352 1.9561140537261963 21.352821350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792560338973999 1.826682448387146 20.059385299682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921388149261475 2.270716905593872 24.49930763244629
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7927042245864868 1.7587158679962158 19.37986183166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918663024902344 1.9799227714538574 21.591094970703125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917777299880981 2.0041871070861816 21.833648681640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915617227554321 1.762630581855774 19.41786766052246
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900634765625 1.7293972969055176 19.085872650146484
Total LOSS train 23.686285224327673 valid 20.482120990753174
CE LOSS train 1.7922616756879366 valid 0.44797515869140625
Contrastive LOSS train 2.1894023400086624 valid 0.4323493242263794
Saved best model. Old loss 21.16740083694458 and new best loss 20.482120990753174
EPOCH 196:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792233943939209 2.078742504119873 22.579660415649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791533350944519 2.236612558364868 24.157657623291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924902439117432 2.0399653911590576 22.1921443939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923851013183594 2.207432508468628 23.866710662841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920769453048706 2.0916318893432617 22.70839500427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928988933563232 2.1413841247558594 23.20673942565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858196258545 2.281766176223755 24.609519958496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791914701461792 2.222888946533203 24.020803451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917367219924927 2.2011544704437256 23.803281784057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791749119758606 2.1412317752838135 23.20406723022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927300930023193 2.3948686122894287 25.741416931152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792413592338562 2.492466926574707 26.717082977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929871082305908 2.35988187789917 25.391807556152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926440238952637 2.3388235569000244 25.180879592895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910627126693726 2.2989718914031982 24.78078269958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927379608154297 2.1553077697753906 23.345815658569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925950288772583 2.084550619125366 22.63810157775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924373149871826 2.4326717853546143 26.11915397644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923327684402466 2.0441935062408447 22.234268188476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915853261947632 2.069253444671631 22.484121322631836
  batch 20 loss: 1.7915853261947632, 2.069253444671631, 22.484121322631836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924758195877075 2.0853612422943115 22.646089553833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924600839614868 2.2366116046905518 24.1585750579834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922368049621582 2.2635133266448975 24.427370071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927298545837402 2.2192838191986084 23.98556900024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924067974090576 2.539393186569214 27.186338424682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79252028465271 2.3908584117889404 25.70110511779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928919792175293 1.9696475267410278 21.489368438720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918699979782104 1.7385419607162476 19.177289962768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933635711669922 2.1458168029785156 23.25153160095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914968729019165 2.100015163421631 22.791648864746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925854921340942 2.48641300201416 26.656715393066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914013862609863 2.0503010749816895 22.294410705566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924617528915405 1.810583472251892 19.898296356201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925903797149658 2.165436029434204 23.446950912475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925420999526978 2.4359304904937744 26.151845932006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926453351974487 2.3344907760620117 25.13755226135254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925604581832886 2.272878885269165 24.52134895324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916202545166016 1.9173719882965088 20.96533966064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921565771102905 2.277985095977783 24.572006225585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.1988232135772705 23.77988624572754
  batch 40 loss: 1.791654109954834, 2.1988232135772705, 23.77988624572754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921760082244873 2.3639893531799316 25.43206787109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792399525642395 2.190365791320801 23.69605827331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049527168274 2.463608503341675 26.42813491821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916035652160645 2.1448416709899902 23.240020751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792501449584961 2.0781946182250977 22.574447631835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921808958053589 2.346292734146118 25.255107879638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920702695846558 2.2258055210113525 24.050125122070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919142246246338 2.181715726852417 23.609071731567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791604995727539 2.2785751819610596 24.577356338500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920494079589844 2.1623942852020264 23.415992736816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791054129600525 2.4113800525665283 25.90485382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923696041107178 2.250081777572632 24.293188095092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922446727752686 2.043792963027954 22.230173110961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925883531570435 2.198416233062744 23.776750564575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918466329574585 2.384873867034912 25.640586853027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925680875778198 2.1096155643463135 22.888723373413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792738914489746 2.4764490127563477 26.557228088378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927162647247314 1.9248745441436768 21.041461944580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931549549102783 2.526810884475708 27.061264038085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792143702507019 2.275311231613159 24.545255661010742
  batch 60 loss: 1.792143702507019, 2.275311231613159, 24.545255661010742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792759656906128 2.0907468795776367 22.700227737426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916755676269531 2.2658579349517822 24.450254440307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.0989153385162354 22.781707763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921347618103027 2.2319257259368896 24.111392974853516
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926993370056152 1.9223897457122803 21.016597747802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918639183044434 2.3678486347198486 25.47035026550293
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917755842208862 2.3006319999694824 24.798095703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791560173034668 1.957356572151184 21.365127563476562
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918989658355713 2.1577742099761963 23.369640350341797
Total LOSS train 23.884610748291017 valid 23.750803470611572
CE LOSS train 1.7922595739364624 valid 0.4479747414588928
Contrastive LOSS train 2.209235116151663 valid 0.5394435524940491
EPOCH 197:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922316789627075 2.2216644287109375 24.00887680053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915339469909668 2.5344350337982178 27.13588523864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924860715866089 2.311565637588501 24.90814208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923839092254639 2.0369019508361816 22.161401748657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920759916305542 2.25878643989563 24.379940032958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928944826126099 2.0894479751586914 22.687374114990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918510437011719 2.2071290016174316 23.863140106201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919033765792847 2.009765863418579 21.88956069946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917262315750122 2.0831427574157715 22.62315559387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917420864105225 2.1670196056365967 23.461938858032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927266359329224 2.436363458633423 26.156360626220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924164533615112 2.350940227508545 25.30181884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792991042137146 2.4252679347991943 26.045671463012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926493883132935 2.2995643615722656 24.788293838500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791066288948059 2.3283300399780273 25.07436752319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927372455596924 2.4325270652770996 26.118009567260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925885915756226 2.1092846393585205 22.885435104370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924295663833618 2.269963026046753 24.4920597076416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923246622085571 2.151670217514038 23.30902671813965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915825843811035 2.2537994384765625 24.32957649230957
  batch 20 loss: 1.7915825843811035, 2.2537994384765625, 24.32957649230957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924726009368896 1.8067539930343628 19.86001205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924602031707764 2.1078717708587646 22.871177673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922356128692627 2.1260695457458496 23.052932739257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792726993560791 2.0849995613098145 22.64272117614746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924059629440308 2.6856467723846436 28.648874282836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925208806991577 2.3769543170928955 25.562063217163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792894721031189 2.199169158935547 23.78458595275879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918730974197388 2.2638726234436035 24.430599212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933586835861206 2.5387561321258545 27.180919647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914974689483643 2.33921480178833 25.183645248413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.361459255218506 25.407169342041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913931608200073 2.3372702598571777 25.16409683227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792452335357666 2.386326551437378 25.655717849731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925809621810913 2.3885068893432617 25.677650451660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792529821395874 2.453505277633667 26.32758331298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926346063613892 2.1434080600738525 23.226715087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925512790679932 2.1165056228637695 22.95760726928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916202545166016 2.081801414489746 22.609634399414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921597957611084 2.0294606685638428 22.086767196655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791658639907837 2.1390039920806885 23.181697845458984
  batch 40 loss: 1.791658639907837, 2.1390039920806885, 23.181697845458984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921864986419678 2.3433260917663574 25.225448608398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924147844314575 1.9919241666793823 21.71165657043457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920645475387573 1.9519699811935425 21.311763763427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916160821914673 2.1998744010925293 23.790361404418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925024032592773 2.336655378341675 25.1590576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921769618988037 2.0668859481811523 22.461036682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792065978050232 2.2031033039093018 23.82309913635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919079065322876 2.343778371810913 25.229692459106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.111126184463501 22.90286636352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920514345169067 2.038780689239502 22.17985725402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910569906234741 2.437088966369629 26.16194725036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923671007156372 2.0050339698791504 21.84270668029785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922425270080566 2.165501832962036 23.4472599029541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925846576690674 2.243616819381714 24.2287540435791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918460369110107 2.3461227416992188 25.25307273864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925684452056885 1.7555632591247559 19.348201751708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792739748954773 2.263057231903076 24.42331314086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927160263061523 2.0419118404388428 22.211833953857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931495904922485 2.288313150405884 24.676280975341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921431064605713 2.3519959449768066 25.312101364135742
  batch 60 loss: 1.7921431064605713, 2.3519959449768066, 25.312101364135742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927653789520264 2.2916853427886963 24.709617614746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916854619979858 2.032597303390503 22.117658615112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925612926483154 2.132859945297241 23.12116050720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921401262283325 2.515143871307373 26.943580627441406
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792700171470642 1.8838831186294556 20.63153076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 1.824205756187439 20.033920288085938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917753458023071 1.7625219821929932 19.416994094848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915605306625366 1.5056415796279907 16.847976684570312
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918980121612549 1.5230129957199097 17.02202796936035
Total LOSS train 23.990093319232646 valid 18.33022975921631
CE LOSS train 1.7922585083888127 valid 0.4479745030403137
Contrastive LOSS train 2.2197834711808424 valid 0.3807532489299774
Saved best model. Old loss 20.482120990753174 and new best loss 18.33022975921631
EPOCH 198:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922301292419434 1.9330246448516846 21.12247657775879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915314435958862 2.594773769378662 27.739269256591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924786806106567 2.150951385498047 23.301992416381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923766374588013 2.145569324493408 23.248069763183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920677661895752 2.2926888465881348 24.718955993652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928839921951294 2.351428270339966 25.307167053222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918481826782227 2.2515599727630615 24.307449340820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919076681137085 2.185500144958496 23.646909713745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791735053062439 2.292785167694092 24.719587326049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917490005493164 2.1273181438446045 23.064929962158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927303314208984 2.3467860221862793 25.260591506958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241943359375 2.535701036453247 27.149429321289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792984962463379 2.3443386554718018 25.236370086669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926448583602905 1.9720399379730225 21.513044357299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910709381103516 2.104118824005127 22.832258224487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927358150482178 2.331494092941284 25.107677459716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925891876220703 2.375427722930908 25.546865463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924314737319946 2.3250882625579834 25.04331398010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792328119277954 2.136809825897217 23.16042709350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791582465171814 1.9684021472930908 21.475603103637695
  batch 20 loss: 1.791582465171814, 1.9684021472930908, 21.475603103637695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924669981002808 1.7147767543792725 18.940235137939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924528121948242 2.2185094356536865 23.97754669189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922312021255493 2.118948459625244 22.981714248657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927266359329224 1.763875961303711 19.431486129760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792409896850586 2.184924364089966 23.641653060913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925243377685547 1.9643886089324951 21.436410903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792896032333374 2.34930157661438 25.285911560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918744087219238 2.3181846141815186 24.97372055053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933565378189087 2.2031545639038086 23.824901580810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915008068084717 2.0518479347229004 22.309978485107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925797700881958 2.2129769325256348 23.922348022460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914007902145386 2.2228612899780273 24.0200138092041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924535274505615 2.403932809829712 25.8317813873291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925806045532227 2.193755626678467 23.73013687133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925314903259277 2.3566081523895264 25.358613967895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926356792449951 2.308314323425293 24.875778198242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925513982772827 2.2661807537078857 24.45435905456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 2.1670305728912354 23.46192169189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792153000831604 2.3491575717926025 25.283727645874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916514873504639 2.2183125019073486 23.974775314331055
  batch 40 loss: 1.7916514873504639, 2.2183125019073486, 23.974775314331055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921760082244873 2.171206474304199 23.504240036010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924010753631592 1.77146577835083 19.507057189941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792052984237671 2.1213533878326416 23.005586624145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610598564148 2.022601366043091 22.017623901367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924972772598267 2.1026835441589355 22.819334030151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921733856201172 1.8769627809524536 20.56180191040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920639514923096 2.1577255725860596 23.369319915771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919058799743652 2.243858575820923 24.230491638183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916027307510376 2.298588275909424 24.777484893798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920494079589844 1.9157729148864746 20.949779510498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910610437393188 2.3688721656799316 25.479782104492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923697233200073 2.1421351432800293 23.213722229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922452688217163 2.0096802711486816 21.889047622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925840616226196 2.1835081577301025 23.62766456604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918447256088257 1.971313714981079 21.504981994628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925639152526855 2.019251585006714 21.98508071899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927309274673462 2.3726418018341064 25.519147872924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 2.016132116317749 21.954029083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931427955627441 2.5550332069396973 27.343473434448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921350002288818 2.185826539993286 23.650400161743164
  batch 60 loss: 1.7921350002288818, 2.185826539993286, 23.650400161743164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927500009536743 2.225682258605957 24.049571990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916747331619263 2.118992805480957 22.981603622436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925488948822021 2.141981840133667 23.21236801147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921311855316162 2.083648681640625 22.628618240356445
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926909923553467 1.6797358989715576 18.590049743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918641567230225 2.383770227432251 25.629566192626953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791776180267334 2.368720531463623 25.47898292541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791561245918274 1.9543932676315308 21.335493087768555
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919012308120728 1.9227286577224731 21.019187927246094
Total LOSS train 23.501410205547625 valid 23.36580753326416
CE LOSS train 1.7922559169622567 valid 0.4479753077030182
Contrastive LOSS train 2.1709154440806464 valid 0.4806821644306183
EPOCH 199:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792231559753418 2.3198156356811523 24.990386962890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915420532226562 2.292670249938965 24.718244552612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924885749816895 1.8568564653396606 20.361053466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385220527649 1.8005541563034058 19.79792594909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920730113983154 2.142137289047241 23.21344566345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928829193115234 2.1095869541168213 22.888751983642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918446063995361 2.1679399013519287 23.47124481201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 2.0615365505218506 22.407268524169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.2383620738983154 24.175350189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791745662689209 2.0578227043151855 22.36997413635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724847793579 2.4850428104400635 26.643152236938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924137115478516 2.487428903579712 26.666702270507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929818630218506 2.223954439163208 24.03252601623535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926454544067383 2.337388753890991 25.166534423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910754680633545 2.2306034564971924 24.097108840942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927340269088745 2.373701333999634 25.529747009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925902605056763 2.1716036796569824 23.508628845214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924283742904663 2.048386335372925 22.27629280090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923258543014526 1.8768513202667236 20.56083869934082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915840148925781 1.973071575164795 21.522300720214844
  batch 20 loss: 1.7915840148925781, 1.973071575164795, 21.522300720214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924668788909912 2.1951117515563965 23.74358558654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924515008926392 2.2403597831726074 24.1960506439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792227864265442 2.1443963050842285 23.236190795898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792718529701233 2.4356496334075928 26.149215698242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924011945724487 2.3948400020599365 25.740800857543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925152778625488 2.3516016006469727 25.308530807495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792887568473816 2.1986260414123535 23.779146194458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918741703033447 1.8655461072921753 20.44733428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793353796005249 2.0682754516601562 22.47610855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915048599243164 2.1536760330200195 23.328266143798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580485343933 2.4568965435028076 26.36154556274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914010286331177 2.4830899238586426 26.622299194335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924493551254272 2.2146289348602295 23.938739776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925746440887451 2.2643251419067383 24.43582534790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792525053024292 2.2998385429382324 24.790910720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926297187805176 2.120046854019165 22.99309730529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925490140914917 2.229623556137085 24.08878517150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916209697723389 2.1654677391052246 23.446298599243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921552658081055 2.385474920272827 25.64690399169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.261101007461548 24.402664184570312
  batch 40 loss: 1.7916545867919922, 2.261101007461548, 24.402664184570312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792177438735962 2.127509832382202 23.067276000976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924001216888428 2.000178098678589 21.79418182373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920514345169067 2.410205364227295 25.894105911254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79160737991333 2.1807398796081543 23.59900665283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924904823303223 1.7081356048583984 18.87384605407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921696901321411 2.267085552215576 24.46302604675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920632362365723 1.892958402633667 20.721647262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791911005973816 2.0974602699279785 22.766511917114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916080951690674 2.1907553672790527 23.69916343688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920533418655396 2.019465208053589 21.986705780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791062831878662 2.3405699729919434 25.19676399230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792364239692688 2.422928810119629 26.021652221679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792236566543579 2.0956785678863525 22.749021530151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79257333278656 1.9106075763702393 20.898649215698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.3272101879119873 25.06393814086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557954788208 2.2098824977874756 23.89138412475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792729139328003 2.3815815448760986 25.608543395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927112579345703 2.075751781463623 22.550230026245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931451797485352 2.487586736679077 26.66901397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921408414840698 2.432053804397583 26.11267852783203
  batch 60 loss: 1.7921408414840698, 2.432053804397583, 26.11267852783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927534580230713 2.211475133895874 23.90750503540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916789054870605 2.1277427673339844 23.069107055664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925481796264648 2.1602988243103027 23.395538330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921305894851685 2.1663506031036377 23.455636978149414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926890850067139 1.7494508028030396 19.28719711303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918668985366821 2.4238462448120117 26.03032875061035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917789220809937 2.4760239124298096 26.552017211914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915650606155396 2.202528238296509 23.81684684753418
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919021844863892 2.1919233798980713 23.711135864257812
Total LOSS train 23.665724739661584 valid 25.0275821685791
CE LOSS train 1.79225476705111 valid 0.4479755461215973
Contrastive LOSS train 2.1873469792879545 valid 0.5479808449745178
EPOCH 200:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922319173812866 2.357694149017334 25.36917495727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915384769439697 2.416743755340576 25.95897674560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924827337265015 2.0283830165863037 22.076313018798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792379379272461 2.1050760746002197 22.8431396484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920678853988647 2.2979824542999268 24.771892547607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928807735443115 1.865075707435608 20.44363784790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918472290039062 2.3914642333984375 25.70648956298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919044494628906 2.084463357925415 22.636537551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917314767837524 2.0177440643310547 21.96917152404785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917495965957642 2.1907272338867188 23.69902229309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927254438400269 2.406797170639038 25.86069679260254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924153804779053 2.5076887607574463 26.86930274963379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929797172546387 2.318514585494995 24.978126525878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926440238952637 2.3340132236480713 25.132776260375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910771369934082 2.260590076446533 24.396976470947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927308082580566 2.323545217514038 25.028182983398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925838232040405 2.290076971054077 24.6933536529541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792423129081726 2.2213315963745117 24.005739212036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923210859298706 1.970418930053711 21.496509552001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791585922241211 2.1628472805023193 23.420059204101562
  batch 20 loss: 1.791585922241211, 2.1628472805023193, 23.420059204101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924673557281494 2.379594564437866 25.58841323852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792454481124878 2.1488759517669678 23.281213760375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792230248451233 2.131197690963745 23.10420799255371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927180528640747 2.3169636726379395 24.96235466003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79239821434021 2.506904125213623 26.861440658569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925094366073608 2.3106839656829834 24.899349212646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928825616836548 2.2433345317840576 24.226228713989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918730974197388 2.0745699405670166 22.537572860717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933509349822998 2.2801568508148193 24.594919204711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915055751800537 2.0385959148406982 22.177465438842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925751209259033 2.0597150325775146 22.389726638793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791398286819458 2.1340768337249756 23.13216781616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924455404281616 2.2533066272735596 24.325511932373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792571783065796 2.3524200916290283 25.3167724609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925244569778442 2.290921211242676 24.701736450195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926318645477295 2.350682020187378 25.29945182800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925500869750977 2.2495107650756836 24.28765869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916227579116821 2.0811076164245605 22.602699279785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921555042266846 2.366607427597046 25.45823097229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916549444198608 2.0921730995178223 22.7133846282959
  batch 40 loss: 1.7916549444198608, 2.0921730995178223, 22.7133846282959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792176365852356 2.4320759773254395 26.11293601989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923970222473145 2.0708656311035156 22.501052856445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920448780059814 2.3483307361602783 25.275352478027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79160475730896 1.7688530683517456 19.480134963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924879789352417 2.1563313007354736 23.35580062866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921706438064575 2.3220279216766357 25.012451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920629978179932 1.8647631406784058 20.439693450927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919079065322876 2.0106923580169678 21.898832321166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 2.2862744331359863 24.654348373413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920479774475098 2.081456184387207 22.606609344482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910631895065308 2.427886486053467 26.069929122924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923617362976074 2.36643123626709 25.456674575805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792239785194397 2.198784589767456 23.780086517333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925785779953003 2.1677327156066895 23.46990394592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791843056678772 2.347546100616455 25.267303466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792558193206787 2.1498732566833496 23.291292190551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927275896072388 2.2935194969177246 24.727924346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927091121673584 2.198455810546875 23.777267456054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931420803070068 2.1669113636016846 23.462255477905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921407222747803 2.2931532859802246 24.723674774169922
  batch 60 loss: 1.7921407222747803, 2.2931532859802246, 24.723674774169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927504777908325 2.280349016189575 24.596240997314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916803359985352 2.2752225399017334 24.543907165527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925491333007812 2.1835265159606934 23.62781524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921323776245117 2.331714153289795 25.109275817871094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926865816116333 1.4688447713851929 16.48113441467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918624877929688 2.2786335945129395 24.578197479248047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774868965149 2.2852706909179688 24.644481658935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915617227554321 2.009052276611328 21.882083892822266
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918970584869385 1.9628006219863892 21.419902801513672
Total LOSS train 23.900592041015624 valid 23.131166458129883
CE LOSS train 1.7922536886655367 valid 0.4479742646217346
Contrastive LOSS train 2.2108338135939376 valid 0.4907001554965973
EPOCH 201:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922263145446777 2.35261607170105 25.318387985229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915352582931519 2.559455394744873 27.386089324951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924755811691284 2.2035486698150635 23.827960968017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 2.1504149436950684 23.296525955200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920665740966797 2.2034547328948975 23.826614379882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792879343032837 2.106688976287842 22.859769821166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918484210968018 2.430143117904663 26.093280792236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919068336486816 2.1760995388031006 23.552902221679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917335033416748 2.2780566215515137 24.57229995727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917475700378418 2.100425958633423 22.79600715637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927221059799194 2.3748350143432617 25.541072845458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924139499664307 1.9497344493865967 21.289758682250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929755449295044 1.7177064418792725 18.970041275024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926386594772339 1.8599358797073364 20.391998291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79107666015625 2.0191264152526855 21.982341766357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927289009094238 2.3630783557891846 25.423511505126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586088180542 2.229008674621582 24.082672119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924267053604126 2.281548023223877 24.607906341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923245429992676 2.0048787593841553 21.84111213684082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915886640548706 2.220858097076416 24.000167846679688
  batch 20 loss: 1.7915886640548706, 2.220858097076416, 24.000167846679688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924644947052002 2.120774745941162 23.000213623046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924498319625854 2.2182693481445312 23.975143432617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922254800796509 1.9759323596954346 21.55154800415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927138805389404 2.081550359725952 22.608217239379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923966646194458 2.280975341796875 24.602149963378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792510986328125 2.268580436706543 24.478315353393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792883276939392 1.8457695245742798 20.250577926635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918751239776611 2.0940961837768555 22.732837677001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793349027633667 2.338008165359497 25.173429489135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791507363319397 2.3512346744537354 25.30385398864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925742864608765 2.362548828125 25.418062210083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914007902145386 2.1953036785125732 23.74443817138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924432754516602 2.207927703857422 23.871719360351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792568564414978 2.156721591949463 23.359783172607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925201654434204 2.2693846225738525 24.486366271972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926243543624878 2.263751268386841 24.43013572692871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925437688827515 2.241185188293457 24.204395294189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621208190918 2.1953930854797363 23.74555206298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921546697616577 2.3261184692382812 25.0533390045166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791655421257019 1.826945424079895 20.06110954284668
  batch 40 loss: 1.791655421257019, 1.826945424079895, 20.06110954284668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921757698059082 2.177260637283325 23.564781188964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397141456604 1.9530357122421265 21.32275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920482158660889 2.242222547531128 24.21427345275879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916101217269897 1.985643744468689 21.648048400878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924906015396118 2.0000617504119873 21.793107986450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170524597168 2.1786422729492188 23.578594207763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920640707015991 2.1264395713806152 23.056461334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919074296951294 1.9978464841842651 21.77037239074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791604995727539 2.1654129028320312 23.44573402404785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920472621917725 1.9899543523788452 21.691591262817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791062593460083 2.2204396724700928 23.995460510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923611402511597 2.17150616645813 23.507421493530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922390699386597 2.0871243476867676 22.663480758666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925764322280884 2.1922805309295654 23.715381622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791840672492981 2.2415244579315186 24.20708656311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557954788208 2.1217281818389893 23.00984001159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792725920677185 2.2834060192108154 24.626787185668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927039861679077 2.1537678241729736 23.330381393432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931358814239502 2.2090113162994385 23.883249282836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921377420425415 2.0907633304595947 22.699769973754883
  batch 60 loss: 1.7921377420425415, 2.0907633304595947, 22.699769973754883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927477359771729 2.2951395511627197 24.744142532348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916797399520874 2.235436201095581 24.146041870117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925477027893066 1.7349594831466675 19.142141342163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921310663223267 2.1521427631378174 23.31355857849121
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926874160766602 1.7666863203048706 19.459548950195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918652296066284 2.1887753009796143 23.679616928100586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917770147323608 2.2061240673065186 23.853017807006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915642261505127 2.0388123989105225 22.179689407348633
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919007539749146 1.8766580820083618 20.558481216430664
Total LOSS train 23.3267941401555 valid 22.56770133972168
CE LOSS train 1.792252467228816 valid 0.44797518849372864
Contrastive LOSS train 2.1534541735282313 valid 0.46916452050209045
EPOCH 202:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792229175567627 2.2336995601654053 24.12922477722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915384769439697 2.44779896736145 26.269527435302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 2.0840930938720703 22.63340950012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792377233505249 2.0870282649993896 22.662660598754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920688390731812 2.2616591453552246 24.408660888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928802967071533 2.1458375453948975 23.251256942749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918484210968018 2.274066209793091 24.53251075744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919056415557861 2.2431254386901855 24.223161697387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917317152023315 2.2344462871551514 24.136194229125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917453050613403 2.0259850025177 22.05159568786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927191257476807 2.448911428451538 26.28183364868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792411208152771 2.547137975692749 27.263792037963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792973518371582 2.281648874282837 24.60946273803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926371097564697 2.085561990737915 22.648256301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791076421737671 2.19111704826355 23.702247619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927279472351074 2.2324790954589844 24.11751937866211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586088180542 2.3914260864257812 25.706846237182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792426586151123 2.376756429672241 25.55999183654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923245429992676 2.1926162242889404 23.718486785888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915873527526855 1.9806734323501587 21.59832191467285
  batch 20 loss: 1.7915873527526855, 1.9806734323501587, 21.59832191467285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924628257751465 2.1740007400512695 23.532470703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924480438232422 2.20900559425354 23.882503509521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922239303588867 2.116086483001709 22.95309066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927125692367554 2.2798879146575928 24.59159278869629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792396068572998 2.33121919631958 25.10458755493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925102710723877 2.0914785861968994 22.70729637145996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928816080093384 2.335771322250366 25.15059471130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918734550476074 2.1937360763549805 23.72923469543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933471202850342 2.322514533996582 25.018491744995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915068864822388 2.252967596054077 24.321184158325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925727367401123 2.3450798988342285 25.243370056152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7913994789123535 2.2750449180603027 24.54184913635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924423217773438 1.8884485960006714 20.67692756652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925682067871094 2.2870333194732666 24.662900924682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925198078155518 2.3045432567596436 24.837953567504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926231622695923 2.298729658126831 24.77992057800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925426959991455 2.2862207889556885 24.65475082397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916204929351807 2.015007495880127 21.941694259643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921537160873413 2.358736753463745 25.3795223236084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.3517630100250244 25.309284210205078
  batch 40 loss: 1.7916545867919922, 2.3517630100250244, 25.309284210205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79217529296875 2.2876474857330322 24.668649673461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923977375030518 1.9343217611312866 21.135616302490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.254462480545044 24.336673736572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916110754013062 2.2993788719177246 24.785400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924913167953491 2.128350019454956 23.075992584228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921706438064575 2.1435563564300537 23.22773551940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920637130737305 2.0301921367645264 22.09398651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919070720672607 2.0063347816467285 21.855253219604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.3135898113250732 24.92750358581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 1.8894506692886353 20.686553955078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910631895065308 2.294520854949951 24.73627281188965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923616170883179 2.164569616317749 23.438058853149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922395467758179 2.1365966796875 23.158206939697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925764322280884 2.115600824356079 22.948583602905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79184091091156 2.4655380249023438 26.447221755981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925578355789185 2.1581239700317383 23.373798370361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792725920677185 2.325554370880127 25.048269271850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927042245864868 2.09806489944458 22.773351669311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931358814239502 2.427123546600342 26.064373016357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921379804611206 2.2859160900115967 24.65129852294922
  batch 60 loss: 1.7921379804611206, 2.2859160900115967, 24.65129852294922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927484512329102 2.2679896354675293 24.472644805908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916803359985352 2.2249057292938232 24.04073715209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925477027893066 2.110372543334961 22.896272659301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792130947113037 2.3049027919769287 24.84115982055664
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926872968673706 1.48501455783844 16.642831802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918651103973389 2.1543283462524414 23.335147857666016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79177725315094 2.127755880355835 23.0693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915642261505127 1.9634469747543335 21.426034927368164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919011116027832 1.896437168121338 20.756271362304688
Total LOSS train 23.89000965998723 valid 22.146697521209717
CE LOSS train 1.7922521426127507 valid 0.4479752779006958
Contrastive LOSS train 2.209775728445787 valid 0.4741092920303345
EPOCH 203:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922292947769165 2.156369686126709 23.355926513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791538953781128 2.4973702430725098 26.765239715576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924782037734985 2.168469190597534 23.477170944213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923775911331177 2.1113598346710205 22.905975341796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920689582824707 2.1950292587280273 23.742361068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928800582885742 2.3278985023498535 25.07186508178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791848063468933 1.9746489524841309 21.53833770751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919057607650757 2.0786404609680176 22.578310012817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917320728302002 2.154454231262207 23.336275100708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917457818984985 1.9952532052993774 21.744277954101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927193641662598 2.46325945854187 26.42531394958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792410969734192 2.4775049686431885 26.567461013793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792972445487976 2.311518430709839 24.908157348632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926361560821533 2.15878963470459 23.38053321838379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910765409469604 2.255096912384033 24.342044830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927281856536865 2.357548713684082 25.368215560913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925865650177002 2.140187978744507 23.194467544555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924270629882812 2.304316282272339 24.835590362548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923249006271362 1.948582410812378 21.278148651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915873527526855 2.1610777378082275 23.40236473083496
  batch 20 loss: 1.7915873527526855, 2.1610777378082275, 23.40236473083496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924617528915405 2.174910068511963 23.541561126708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924470901489258 2.2689921855926514 24.48236846923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922230958938599 2.156999349594116 23.36221694946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711615562439 2.066885232925415 22.461563110351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923955917358398 2.5931146144866943 27.723541259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925097942352295 2.2964375019073486 24.75688362121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928805351257324 2.094378709793091 22.73666763305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918732166290283 2.077345609664917 22.565330505371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933458089828491 2.2053515911102295 23.84686279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915071249008179 2.2766952514648438 24.558460235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925729751586914 2.427781820297241 26.070392608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914003133773804 2.411731004714966 25.908710479736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924420833587646 2.3109843730926514 24.902286529541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925673723220825 2.256946563720703 24.36203384399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925190925598145 2.3035404682159424 24.827922821044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926228046417236 2.490612745285034 26.69875144958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925426959991455 1.7801662683486938 19.594205856323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916213274002075 2.0819590091705322 22.6112117767334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921545505523682 2.318047285079956 24.972627639770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916555404663086 2.1433215141296387 23.224872589111328
  batch 40 loss: 1.7916555404663086, 2.1433215141296387, 23.224872589111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792176604270935 2.160949230194092 23.401670455932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79239821434021 2.147616147994995 23.2685604095459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920496463775635 2.220451831817627 23.996566772460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916113138198853 2.08811616897583 22.672771453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924913167953491 2.1191747188568115 22.98423957824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170524597168 2.265371799468994 24.44588851928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920639514923096 2.0802829265594482 22.594894409179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919071912765503 2.2487096786499023 24.279003143310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916046380996704 2.1015594005584717 22.807199478149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920466661453247 2.0356605052948 22.148653030395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791062831878662 2.169830083847046 23.489364624023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923601865768433 2.159088611602783 23.38324546813965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922383546829224 1.984954595565796 21.64178466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925758361816406 2.2111268043518066 23.90384292602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918405532836914 2.5051095485687256 26.842937469482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557716369629 2.0136878490448 21.92943572998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927250862121582 2.432469606399536 26.117420196533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927031517028809 2.051859140396118 22.311294555664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931346893310547 2.3925235271453857 25.71837043762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921371459960938 2.3203237056732178 24.99537467956543
  batch 60 loss: 1.7921371459960938, 2.3203237056732178, 24.99537467956543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927470207214355 2.3275418281555176 25.068164825439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916799783706665 2.27396559715271 24.531335830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925469875335693 2.2420241832733154 24.21278953552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921303510665894 2.2587037086486816 24.379165649414062
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926864624023438 1.8357740640640259 20.150426864624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918647527694702 2.3497707843780518 25.28957176208496
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917768955230713 2.28792142868042 24.670991897583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915639877319336 1.9300254583358765 21.091819763183594
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900634765625 1.9098401069641113 20.890300750732422
Total LOSS train 23.887706287090595 valid 22.985671043395996
CE LOSS train 1.7922519243680515 valid 0.44797515869140625
Contrastive LOSS train 2.209545423434331 valid 0.47746002674102783
EPOCH 204:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922289371490479 2.244568109512329 24.2379093170166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915388345718384 2.6116793155670166 27.9083309173584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924778461456299 2.1090164184570312 22.88264274597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923771142959595 2.120244264602661 22.99481964111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920689582824707 2.0680153369903564 22.47222137451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792879581451416 2.290971517562866 24.702594757080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791848063468933 2.3854329586029053 25.646177291870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791905403137207 2.368452548980713 25.476428985595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917311191558838 2.1153736114501953 22.945466995239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917448282241821 2.105380058288574 22.845544815063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927179336547852 2.444422960281372 26.23694610595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924104928970337 2.342527151107788 25.217681884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792972207069397 2.153709888458252 23.33007049560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926362752914429 2.281247854232788 24.60511589050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910770177841187 2.235002279281616 24.14109992980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927277088165283 2.3404946327209473 25.197673797607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925854921340942 2.377932071685791 25.5719051361084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924258708953857 2.2560501098632812 24.35292625427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923239469528198 2.0539283752441406 22.331607818603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915871143341064 1.988681435585022 21.678401947021484
  batch 20 loss: 1.7915871143341064, 1.988681435585022, 21.678401947021484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79246187210083 2.015109062194824 21.943552017211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924470901489258 2.1724255084991455 23.516700744628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792223334312439 2.1736979484558105 23.529203414916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711853981018 2.187455177307129 23.66726303100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923955917358398 2.5039241313934326 26.83163833618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925093173980713 2.1248934268951416 23.04144287109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928800582885742 2.3614518642425537 25.407398223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918728590011597 2.3048031330108643 24.839902877807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79334557056427 2.367816209793091 25.471508026123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915074825286865 2.3039839267730713 24.83134651184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925732135772705 2.384136438369751 25.63393783569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914003133773804 2.3298065662384033 25.089466094970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924423217773438 2.1117680072784424 22.91012191772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925676107406616 2.0991950035095215 22.78451919555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925193309783936 2.4308130741119385 26.100648880004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926230430603027 2.3406121730804443 25.198745727539062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542576789856 2.025630235671997 22.048845291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916207313537598 2.2764956951141357 24.556577682495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921539545059204 2.346827507019043 25.26042938232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 2.4001641273498535 25.79329490661621
  batch 40 loss: 1.79165518283844, 2.4001641273498535, 25.79329490661621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921754121780396 2.1926071643829346 23.718246459960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923970222473145 1.8678556680679321 20.4709529876709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 2.262291669845581 24.414966583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610836982727 2.1432454586029053 23.22406578063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924904823303223 2.068854808807373 22.48103904724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921701669692993 2.298567533493042 24.77784538269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920633554458618 2.1424014568328857 23.21607780456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919076681137085 2.0788326263427734 22.58023452758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916043996810913 2.326054334640503 25.052148818969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920469045639038 1.9221700429916382 21.013748168945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910627126693726 1.8990368843078613 20.781431198120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923600673675537 1.8652833700180054 20.445194244384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922381162643433 2.0637519359588623 22.429758071899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925751209259033 2.2980456352233887 24.773033142089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791840672492981 2.4954419136047363 26.746259689331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925574779510498 2.05700945854187 22.362651824951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927253246307373 2.381399631500244 25.606719970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792703628540039 2.286123037338257 24.653934478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793135166168213 2.472923994064331 26.522375106811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921380996704102 2.356616973876953 25.358306884765625
  batch 60 loss: 1.7921380996704102, 2.356616973876953, 25.358306884765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927472591400146 2.362370491027832 25.416452407836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916803359985352 2.1962950229644775 23.75463104248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925472259521484 2.1451833248138428 23.244380950927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921308279037476 2.192173480987549 23.713865280151367
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926865816116333 1.8270151615142822 20.062837600708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791865348815918 2.394688606262207 25.738750457763672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791777491569519 2.19490385055542 23.740816116333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915644645690918 1.9600906372070312 21.392471313476562
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919011116027832 1.7613755464553833 19.405656814575195
Total LOSS train 24.000819485004133 valid 22.56942367553711
CE LOSS train 1.7922517134593083 valid 0.4479752779006958
Contrastive LOSS train 2.22085678760822 valid 0.4403438866138458
EPOCH 205:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922290563583374 2.290839433670044 24.70062255859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915395498275757 2.4640655517578125 26.43219566345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924784421920776 2.006355047225952 21.856029510498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923773527145386 2.278542995452881 24.577808380126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792069435119629 2.1093053817749023 22.88512420654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928797006607056 2.2271368503570557 24.064247131347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918484210968018 2.3133366107940674 24.925214767456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919058799743652 2.214818000793457 23.940086364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917323112487793 2.194262981414795 23.734363555908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917457818984985 2.0502703189849854 22.294448852539062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927186489105225 2.2389581203460693 24.182300567626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241144657135 2.4762332439422607 26.554744720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929725646972656 2.277778148651123 24.570755004882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926368713378906 2.19706130027771 23.76325035095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910778522491455 2.1936538219451904 23.727617263793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792728066444397 2.2269856929779053 24.062585830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586088180542 2.416874647140503 25.961332321166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924262285232544 2.1920902729034424 23.713329315185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923247814178467 2.1590065956115723 23.382389068603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791588306427002 2.1949400901794434 23.740989685058594
  batch 20 loss: 1.791588306427002, 2.1949400901794434, 23.740989685058594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924619913101196 2.0775368213653564 22.567829132080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924474477767944 2.1763880252838135 23.55632781982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792223572731018 2.120270252227783 22.994924545288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927122116088867 2.159435272216797 23.387065887451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923964262008667 2.4819958209991455 26.612354278564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925101518630981 1.9776732921600342 21.569244384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928805351257324 2.3311996459960938 25.104877471923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918736934661865 2.0701963901519775 22.493837356567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933458089828491 2.3061070442199707 24.854415893554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915079593658447 2.0870039463043213 22.66154670715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925732135772705 2.400628089904785 25.79885482788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914007902145386 2.187340259552002 23.66480255126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441964149475 2.2534332275390625 24.32677459716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925671339035034 2.2676756381988525 24.469322204589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519211769104 2.322152853012085 25.014047622680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792623519897461 2.534741163253784 27.14003562927246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925432920455933 1.873216152191162 20.52470588684082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916216850280762 2.21842098236084 23.975831985473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921550273895264 2.2137680053710938 23.929834365844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916557788848877 2.3062477111816406 24.85413360595703
  batch 40 loss: 1.7916557788848877, 2.3062477111816406, 24.85413360595703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921760082244873 2.2451772689819336 24.243947982788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923974990844727 2.058332920074463 22.37572479248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920489311218262 2.3286311626434326 25.07836151123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916115522384644 2.19335675239563 23.725177764892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924907207489014 2.1644339561462402 23.436830520629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921706438064575 2.2456953525543213 24.24912452697754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920644283294678 2.153472900390625 23.326793670654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919083833694458 2.1708123683929443 23.500032424926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.1459310054779053 23.25091552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920480966567993 2.06584095954895 22.450456619262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910640239715576 2.1517202854156494 23.308265686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923609018325806 2.274487018585205 24.537229537963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922389507293701 2.159235954284668 23.384597778320312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.2883946895599365 24.676523208618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918412685394287 2.4704787731170654 26.49662971496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925573587417603 2.068103551864624 22.47359275817871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927252054214478 2.473951816558838 26.532241821289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927043437957764 1.6276062726974487 18.06876564025879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931357622146606 2.13712215423584 23.164358139038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921384572982788 2.220212697982788 23.994266510009766
  batch 60 loss: 1.7921384572982788, 2.220212697982788, 23.994266510009766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927478551864624 2.290389060974121 24.696638107299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916810512542725 2.2164549827575684 23.95623207092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925469875335693 2.0713794231414795 22.5063419342041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792130470275879 2.353980541229248 25.33193588256836
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926864624023438 1.822213053703308 20.014816284179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791865348815918 2.3003461360931396 24.795326232910156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791777491569519 2.2175722122192383 23.967498779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791564702987671 2.06881046295166 22.47966957092285
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919009923934937 1.907974362373352 20.871644973754883
Total LOSS train 23.866923112135666 valid 23.02853488922119
CE LOSS train 1.7922522251422588 valid 0.4479752480983734
Contrastive LOSS train 2.207467086498554 valid 0.476993590593338
EPOCH 206:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922290563583374 2.2858762741088867 24.650991439819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915396690368652 2.5197904109954834 26.989444732666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 1.7869046926498413 19.66152572631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79237699508667 1.8630506992340088 20.422883987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920687198638916 2.2722973823547363 24.51504135131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928791046142578 2.304919481277466 24.842073440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918479442596436 2.39762020111084 25.768049240112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919055223464966 2.2442550659179688 24.23445701599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791731595993042 2.0307765007019043 22.099496841430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917450666427612 1.9794037342071533 21.58578109741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927181720733643 2.335948944091797 25.15220832824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924110889434814 2.5499093532562256 27.291505813598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929713726043701 2.2533116340637207 24.326086044311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926355600357056 2.3339903354644775 25.132537841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910773754119873 2.2946465015411377 24.73754119873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927271127700806 2.2293765544891357 24.08649253845215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925853729248047 2.237058162689209 24.16316795349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924259901046753 2.358603000640869 25.378454208374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792324423789978 2.062504768371582 22.41737174987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915875911712646 2.163679838180542 23.428386688232422
  batch 20 loss: 1.7915875911712646, 2.163679838180542, 23.428386688232422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924613952636719 2.1098945140838623 22.891407012939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924466133117676 1.981520414352417 21.607650756835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922229766845703 2.006856679916382 21.860790252685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711615562439 2.2744152545928955 24.536863327026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923959493637085 2.6269538402557373 28.061935424804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792509913444519 2.2938811779022217 24.731321334838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928802967071533 2.365961790084839 25.452499389648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791873574256897 2.040437936782837 22.196252822875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933450937271118 2.287594795227051 24.669292449951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915078401565552 2.0382604598999023 22.17411231994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925729751586914 2.2056243419647217 23.84881591796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914010286331177 2.4011447429656982 25.80284881591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924425601959229 2.117772340774536 22.970165252685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925676107406616 2.3071601390838623 24.86417007446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925195693969727 2.2483670711517334 24.27619171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926231622695923 2.3524465560913086 25.317089080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925432920455933 2.2369589805603027 24.162134170532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916221618652344 1.8834731578826904 20.626354217529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79215407371521 2.3307220935821533 25.099374771118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.196136713027954 23.753021240234375
  batch 40 loss: 1.7916548252105713, 2.196136713027954, 23.753021240234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921756505966187 2.2192256450653076 23.984432220458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923970222473145 2.138169527053833 23.174091339111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 2.2430078983306885 24.222126007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916115522384644 2.1624844074249268 23.416454315185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924904823303223 1.7544883489608765 19.337373733520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921699285507202 1.894692063331604 20.739089965820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792063593864441 2.039473295211792 22.186796188354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919080257415771 1.9287816286087036 21.07972526550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916048765182495 2.3015518188476562 24.8071231842041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920472621917725 2.0070154666900635 21.862201690673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910640239715576 2.3221166133880615 25.012229919433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792360544204712 2.216608762741089 23.95844841003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922389507293701 2.1924400329589844 23.716638565063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.36944580078125 25.48703384399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918412685394287 2.3697192668914795 25.48903465270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925575971603394 1.9837543964385986 21.63010025024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927252054214478 2.3556418418884277 25.349143981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927041053771973 2.1216213703155518 23.0089168548584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931350469589233 2.356065273284912 25.353788375854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792138695716858 2.305922508239746 24.851364135742188
  batch 60 loss: 1.792138695716858, 2.305922508239746, 24.851364135742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927478551864624 2.2537386417388916 24.33013343811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916815280914307 1.9930919408798218 21.72260093688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925474643707275 2.089007616043091 22.6826229095459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921314239501953 2.020911455154419 22.001245498657227
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926872968673706 1.7686123847961426 19.478809356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866421699524 2.236776113510132 24.15962791442871
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791778326034546 2.270482301712036 24.496601104736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915656566619873 2.0672922134399414 22.464487075805664
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.79190194606781 2.0358986854553223 22.150888442993164
Total LOSS train 23.67180480957031 valid 23.317901134490967
CE LOSS train 1.7922519335379967 valid 0.4479754865169525
Contrastive LOSS train 2.187955300624554 valid 0.5089746713638306
EPOCH 207:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922300100326538 2.3279366493225098 25.071596145629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540503501892 2.396698474884033 25.75852394104004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924784421920776 2.2230372428894043 24.022851943969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923778295516968 2.0813190937042236 22.605567932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920695543289185 2.0905978679656982 22.698049545288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928798198699951 2.270773410797119 24.500612258911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791849136352539 2.2924869060516357 24.716718673706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919065952301025 2.075274705886841 22.544652938842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917331457138062 2.1520156860351562 23.3118896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917464971542358 2.158080816268921 23.372554779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927188873291016 2.293822765350342 24.730947494506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 2.1894149780273438 23.686561584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792972445487976 2.1360557079315186 23.15353012084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926363945007324 2.333859920501709 25.131237030029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910789251327515 2.430161476135254 26.092693328857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927274703979492 2.497225761413574 26.764984130859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792586088180542 2.282477617263794 24.617361068725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924264669418335 2.3204920291900635 24.997346878051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923253774642944 1.9798592329025269 21.590917587280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915889024734497 2.0707802772521973 22.499391555786133
  batch 20 loss: 1.7915889024734497, 2.0707802772521973, 22.499391555786133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924621105194092 2.027209520339966 22.064556121826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792447566986084 1.8137584924697876 19.93003273010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792223334312439 1.7535099983215332 19.327322006225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927122116088867 2.151059865951538 23.30331039428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923963069915771 2.3670358657836914 25.46275520324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925093173980713 2.2244861125946045 24.037370681762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792879581451416 2.3578810691833496 25.37169075012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918730974197388 2.178887128829956 23.580745697021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793344259262085 2.4792771339416504 26.58611488342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915079593658447 2.2244436740875244 24.03594398498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925726175308228 2.5416059494018555 27.20863151550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914016246795654 2.497110605239868 26.76250648498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924423217773438 2.317899465560913 24.971437454223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925673723220825 2.296938180923462 24.76194953918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925187349319458 2.2002112865448 23.794631958007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926222085952759 2.255901575088501 24.35163688659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.3835272789001465 25.62781524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621208190918 2.2488982677459717 24.28060531616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921538352966309 2.3235456943511963 25.027610778808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916547060012817 2.222425937652588 24.015913009643555
  batch 40 loss: 1.7916547060012817, 2.222425937652588, 24.015913009643555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921754121780396 2.294848918914795 24.740665435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397141456604 2.175321102142334 23.545608520507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.354722261428833 25.339271545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916120290756226 2.070669651031494 22.498308181762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924896478652954 1.8933719396591187 20.72620964050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921695709228516 2.2567026615142822 24.359195709228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920634746551514 2.102536201477051 22.817424774169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919083833694458 2.23954176902771 24.187326431274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.2126517295837402 23.918123245239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920483350753784 2.072226047515869 22.514307022094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791064977645874 2.2366654872894287 24.1577205657959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923609018325806 2.286299705505371 24.655357360839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922385931015015 2.1780011653900146 23.572250366210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925750017166138 2.162841796875 23.42099380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79184091091156 2.252837896347046 24.320220947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556881904602 1.995633602142334 21.74889373779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927247285842896 2.500446081161499 26.79718589782715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927032709121704 2.1812314987182617 23.605018615722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931333780288696 2.417126417160034 25.964397430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921371459960938 2.2989001274108887 24.781139373779297
  batch 60 loss: 1.7921371459960938, 2.2989001274108887, 24.781139373779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927461862564087 2.2183806896209717 23.976552963256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916808128356934 2.146467447280884 23.25635528564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792546272277832 2.1380715370178223 23.173259735107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921302318572998 2.4456701278686523 26.248830795288086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792685627937317 1.554193377494812 17.334619522094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918647527694702 2.1703872680664062 23.495737075805664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917770147323608 2.143132209777832 23.223098754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915644645690918 1.7930115461349487 19.7216796875
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919005155563354 1.8242714405059814 20.03461456298828
Total LOSS train 23.96968932518592 valid 21.61878252029419
CE LOSS train 1.7922519995616033 valid 0.44797512888908386
Contrastive LOSS train 2.2177437378810003 valid 0.45606786012649536
EPOCH 208:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922284603118896 2.095707416534424 22.74930191040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915388345718384 2.3579487800598145 25.37102508544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924766540527344 2.2085275650024414 23.87775230407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923760414123535 2.153111696243286 23.3234920501709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920674085617065 2.0315935611724854 22.108001708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928776741027832 2.106496572494507 22.85784339904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918471097946167 2.14897084236145 23.28155517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919052839279175 2.1359896659851074 23.15180206298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917317152023315 2.201913356781006 23.81086540222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791745901107788 2.1874635219573975 23.6663818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927180528640747 2.4019570350646973 25.812288284301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924108505249023 2.306100606918335 24.853416442871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929704189300537 2.278773307800293 24.580703735351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792635440826416 2.2672250270843506 24.464885711669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910784482955933 2.3696749210357666 25.48782730102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927268743515015 2.2551190853118896 24.343917846679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925853729248047 2.3543896675109863 25.33648109436035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924258708953857 2.4165117740631104 25.957542419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923246622085571 2.192610025405884 23.71842384338379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915891408920288 2.3614883422851562 25.40647315979004
  batch 20 loss: 1.7915891408920288, 2.3614883422851562, 25.40647315979004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924625873565674 1.8966469764709473 20.75893211364746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792447805404663 2.234449625015259 24.136943817138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922236919403076 1.8615776300430298 20.40799903869629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711853981018 2.1495652198791504 23.288362503051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923964262008667 2.4854159355163574 26.646556854248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925100326538086 2.238581895828247 24.178329467773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928799390792847 2.313605785369873 24.928937911987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791873574256897 2.182469129562378 23.616565704345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933437824249268 2.307933807373047 24.872682571411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915087938308716 2.3510544300079346 25.302053451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925724983215332 2.2369163036346436 24.16173553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914016246795654 1.9874345064163208 21.665746688842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924418449401855 1.8603782653808594 20.396224975585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925667762756348 2.1854729652404785 23.647294998168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925188541412354 2.2213683128356934 24.006202697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926223278045654 2.313319683074951 24.925819396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925423383712769 2.25071120262146 24.299654006958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916219234466553 2.083967924118042 22.631301879882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79215407371521 2.1797633171081543 23.58978843688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 1.9203945398330688 20.9955997467041
  batch 40 loss: 1.7916544675827026, 1.9203945398330688, 20.9955997467041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921743392944336 2.294177532196045 24.733951568603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394995689392 2.123014211654663 23.022537231445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920466661453247 2.1246509552001953 23.038557052612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916098833084106 1.978675365447998 21.578365325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924880981445312 2.0680880546569824 22.473369598388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921693325042725 1.90480637550354 20.840232849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792063593864441 2.1464691162109375 23.25675392150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791909098625183 2.0689752101898193 22.481660842895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 1.9047244787216187 20.838850021362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 1.611039400100708 17.902442932128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910652160644531 2.4320242404937744 26.11130714416504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923599481582642 2.2611429691314697 24.403789520263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922383546829224 2.111745595932007 22.90969467163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792573094367981 2.152177095413208 23.31434440612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 2.298576593399048 24.777605056762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.182964324951172 23.6221981048584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927216291427612 2.4228804111480713 26.02152442932129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927006483078003 2.1640141010284424 23.43284034729004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931325435638428 2.3182830810546875 24.975963592529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921371459960938 2.339318037033081 25.185317993164062
  batch 60 loss: 1.7921371459960938, 2.339318037033081, 25.185317993164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927464246749878 2.3939504623413086 25.732250213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791681170463562 2.148883581161499 23.280517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925465106964111 2.243077516555786 24.22332191467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921305894851685 2.2866218090057373 24.658349990844727
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926851511001587 1.5892349481582642 17.685033798217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 2.4017138481140137 25.809003829956055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917768955230713 2.2166173458099365 23.957950592041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915642261505127 1.9746886491775513 21.5384521484375
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900634765625 2.028184652328491 22.073747634887695
Total LOSS train 23.58642296424279 valid 23.344788551330566
CE LOSS train 1.7922513594994178 valid 0.44797515869140625
Contrastive LOSS train 2.179417164509113 valid 0.5070461630821228
EPOCH 209:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922282218933105 2.2617201805114746 24.40943145751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915393114089966 2.5423357486724854 27.21489715576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792476773262024 2.2430686950683594 24.223163604736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923762798309326 2.204267978668213 23.835054397583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920680046081543 2.211888313293457 23.910951614379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928781509399414 2.122608184814453 23.018959045410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791847586631775 2.253880500793457 24.330652236938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791904330253601 1.9698363542556763 21.49026870727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917308807373047 1.9713706970214844 21.50543785095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917453050613403 2.1083924770355225 22.87567138671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927170991897583 2.3503525257110596 25.296241760253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924089431762695 2.3930916786193848 25.723323822021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792968511581421 2.175786018371582 23.55082893371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792633056640625 2.2561137676239014 24.353771209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910770177841187 2.3035876750946045 24.826953887939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724609375 2.278803586959839 24.580760955810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925832271575928 2.119418144226074 22.986764907836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924250364303589 2.3724937438964844 25.517362594604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792324185371399 1.9652286767959595 21.444610595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791588306427002 2.150909185409546 23.30068016052246
  batch 20 loss: 1.791588306427002, 2.150909185409546, 23.30068016052246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924609184265137 2.135807991027832 23.150541305541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924467325210571 2.2363786697387695 24.156232833862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792221188545227 2.0300567150115967 22.092788696289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792709231376648 1.9930959939956665 21.723669052124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923940420150757 2.391162157058716 25.704015731811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792507529258728 2.091019630432129 22.70270347595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928777933120728 1.9117887020111084 20.910764694213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791872501373291 2.0544350147247314 22.33622169494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933436632156372 2.182112455368042 23.614469528198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 2.4536540508270264 26.32805061340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925710678100586 2.507674217224121 26.869312286376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914005517959595 2.2994775772094727 24.786176681518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924399375915527 2.282985210418701 24.62229347229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925647497177124 1.8235766887664795 20.028331756591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925167083740234 2.1737074851989746 23.529592514038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926207780838013 2.3585550785064697 25.378171920776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925410270690918 2.160799741744995 23.40053939819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.20222806930542 23.81390380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921545505523682 2.405580759048462 25.84796142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916558980941772 2.112464189529419 22.916297912597656
  batch 40 loss: 1.7916558980941772, 2.112464189529419, 22.916297912597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921757698059082 2.086623191833496 22.65840721130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923954725265503 2.0554986000061035 22.347379684448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920472621917725 2.3046228885650635 24.838275909423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916104793548584 2.259845018386841 24.390060424804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924885749816895 1.9297616481781006 21.090105056762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921701669692993 2.18335223197937 23.62569236755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79206383228302 2.136583089828491 23.157896041870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919083833694458 2.015643835067749 21.948347091674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916043996810913 2.2142903804779053 23.93450927734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920459508895874 2.1796700954437256 23.588747024536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791063904762268 2.1046485900878906 22.837549209594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923588752746582 2.0163094997406006 21.955453872680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922372817993164 1.98969304561615 21.689167022705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79257333278656 2.1035056114196777 22.8276309967041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918397188186646 2.186690330505371 23.658742904663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925554513931274 2.0079429149627686 21.871984481811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927236557006836 2.310399055480957 24.896713256835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702078819275 2.283397912979126 24.626680374145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793131709098816 2.435101270675659 26.14414405822754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136311531067 2.160968065261841 23.401817321777344
  batch 60 loss: 1.792136311531067, 2.160968065261841, 23.401817321777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792744517326355 2.3301424980163574 25.09416961669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916796207427979 2.207023859024048 23.86191749572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792544960975647 2.173452138900757 23.527067184448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921288013458252 2.2087442874908447 23.87957191467285
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792683720588684 1.8254430294036865 20.047115325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918652296066284 2.112938404083252 22.921247482299805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791777491569519 2.051063060760498 22.30240821838379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915648221969604 1.8210855722427368 20.00242042541504
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919014692306519 1.6661618947982788 18.453519821166992
Total LOSS train 23.603184157151443 valid 20.919898986816406
CE LOSS train 1.7922506002279428 valid 0.44797536730766296
Contrastive LOSS train 2.181093347989596 valid 0.4165404736995697
EPOCH 210:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922286987304688 2.1076173782348633 22.8684024810791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915390729904175 2.453596830368042 26.32750701904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924755811691284 2.0296549797058105 22.089025497436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923747301101685 2.087670087814331 22.669076919555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792067050933838 2.2300920486450195 24.092987060546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928768396377563 2.175888776779175 23.55176544189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918472290039062 2.3930511474609375 25.72235870361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919045686721802 2.1308329105377197 23.10023307800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917309999465942 2.019153118133545 21.98326301574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917442321777344 2.1802546977996826 23.59429168701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927159070968628 2.4588491916656494 26.381206512451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924094200134277 2.337024450302124 25.162654876708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929688692092896 2.1322379112243652 23.11534881591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792634129524231 2.2514734268188477 24.307369232177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910782098770142 2.3804004192352295 25.595083236694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724370956421 2.3853373527526855 25.646099090576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925827503204346 2.323906660079956 25.03165054321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924232482910156 2.1875226497650146 23.66765022277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923226356506348 2.0555925369262695 22.348247528076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915875911712646 2.212376117706299 23.915348052978516
  batch 20 loss: 1.7915875911712646, 2.212376117706299, 23.915348052978516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924597263336182 2.0898358821868896 22.690818786621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924450635910034 2.2139017581939697 23.931461334228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922213077545166 1.952278971672058 21.31501007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.0716638565063477 22.509347915649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923935651779175 2.473004102706909 26.52243423461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925069332122803 2.1732475757598877 23.524982452392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792876958847046 2.351959705352783 25.31247329711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918723821640015 2.316948890686035 24.961360931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933419942855835 2.233813524246216 24.13147735595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915081977844238 2.2674002647399902 24.465511322021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792570948600769 2.243743896484375 24.230009078979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914010286331177 2.351351261138916 25.304912567138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792440414428711 2.2028167247772217 23.820608139038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792564868927002 1.7899419069290161 19.691984176635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792517066001892 2.559189558029175 27.38441276550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926204204559326 2.285585880279541 24.64847755432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925409078598022 2.326385736465454 25.056398391723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916210889816284 2.1010489463806152 22.80211067199707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921528816223145 2.1785199642181396 23.57735252380371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.1843149662017822 23.634803771972656
  batch 40 loss: 1.7916542291641235, 2.1843149662017822, 23.634803771972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921745777130127 2.1876039505004883 23.668214797973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923953533172607 2.1066579818725586 22.85897445678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920466661453247 2.3061954975128174 24.854001998901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916102409362793 2.068956136703491 22.481172561645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792487621307373 1.969165563583374 21.48414421081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921679019927979 2.1511576175689697 23.303743362426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920626401901245 2.0073161125183105 21.865224838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919068336486816 2.061859369277954 22.410499572753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 2.1073410511016846 22.865013122558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920453548431396 2.1813583374023438 23.605628967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910633087158203 2.3566412925720215 25.35747718811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923579216003418 2.2601571083068848 24.39392852783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922366857528687 2.173678398132324 23.529020309448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79257333278656 2.215467929840088 23.94725227355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918403148651123 2.2648494243621826 24.44033432006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792555332183838 2.0300166606903076 22.092721939086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927215099334717 2.4968154430389404 26.760875701904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927000522613525 2.178969621658325 23.582395553588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931307554244995 2.429054021835327 26.08367156982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921366691589355 2.4001784324645996 25.793922424316406
  batch 60 loss: 1.7921366691589355, 2.4001784324645996, 25.793922424316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792744755744934 2.3378732204437256 25.171478271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916806936264038 2.180392265319824 23.595603942871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792544960975647 2.074730634689331 22.539852142333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921284437179565 2.141643524169922 23.20856285095215
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926828861236572 1.6953626871109009 18.746309280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918637990951538 2.127868413925171 23.07054901123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917758226394653 2.023037910461426 22.02215576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915633916854858 1.8037008047103882 19.828571319580078
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918996810913086 1.8903744220733643 20.69564437866211
Total LOSS train 23.83577754680927 valid 21.40423011779785
CE LOSS train 1.792250055533189 valid 0.44797492027282715
Contrastive LOSS train 2.20435274380904 valid 0.47259360551834106
EPOCH 211:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922272682189941 2.115699529647827 22.949222564697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791538953781128 2.3421754837036133 25.213293075561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792475700378418 2.303583860397339 24.82831573486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 2.17490291595459 23.541404724121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920668125152588 2.241680860519409 24.20887565612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792876124382019 2.1812140941619873 23.605016708374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918461561203003 2.1946945190429688 23.73879051208496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919037342071533 2.0299391746520996 22.091297149658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.2521231174468994 24.31296157836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791744351387024 2.063633441925049 22.428077697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927154302597046 2.529057741165161 27.08329200744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924095392227173 2.5304081439971924 27.09649085998535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79296875 2.355347156524658 25.346439361572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926338911056519 2.266357898712158 24.45621109008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910791635513306 2.3338332176208496 25.129411697387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927250862121582 2.3169913291931152 24.96263885498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925831079483032 2.1148297786712646 22.940881729125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924237251281738 2.2183327674865723 23.975749969482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923228740692139 2.1393704414367676 23.186025619506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915881872177124 2.183114767074585 23.62273597717285
  batch 20 loss: 1.7915881872177124, 2.183114767074585, 23.62273597717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924600839614868 2.1443731784820557 23.236190795898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924458980560303 2.1568076610565186 23.360523223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922216653823853 1.9241424798965454 21.033645629882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927099466323853 2.156832218170166 23.36103057861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923948764801025 2.522113561630249 27.013530731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925080060958862 2.3586301803588867 25.378808975219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928777933120728 2.2815935611724854 24.60881233215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918732166290283 1.9446591138839722 21.23846435546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933419942855835 2.414872407913208 25.942066192626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 2.1266446113586426 23.057954788208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792570948600769 2.299994945526123 24.79252052307129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914010286331177 2.3278448581695557 25.069849014282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79244065284729 2.1961047649383545 23.753488540649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925652265548706 2.1420862674713135 23.21342658996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925175428390503 2.3208048343658447 25.000564575195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926204204559326 2.1892549991607666 23.685169219970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.2540183067321777 24.33272361755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621446609497 2.1028425693511963 22.82004737854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921526432037354 2.241912364959717 24.21127700805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.298130512237549 24.772958755493164
  batch 40 loss: 1.791654109954834, 2.298130512237549, 24.772958755493164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921737432479858 2.248030662536621 24.272480010986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923946380615234 2.0493342876434326 22.285737991333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920467853546143 2.2880184650421143 24.672231674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610598564148 2.1405580043792725 23.19719123840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924877405166626 2.1175334453582764 22.967823028564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921680212020874 2.2919130325317383 24.7112979888916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920621633529663 2.1672935485839844 23.464998245239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919063568115234 1.9517195224761963 21.309101104736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.077444553375244 22.56604766845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920454740524292 2.1543772220611572 23.335817337036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910640239715576 2.3984878063201904 25.775941848754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923580408096313 2.1343469619750977 23.135828018188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792236566543579 1.9671179056167603 21.463415145874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925727367401123 2.2829976081848145 24.622547149658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918404340744019 2.4094698429107666 25.886537551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925549745559692 2.2607438564300537 24.399993896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927212715148926 2.5979578495025635 27.77229881286621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926998138427734 2.16587495803833 23.451448440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793129801750183 2.3717641830444336 25.510770797729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79213547706604 2.254189968109131 24.334035873413086
  batch 60 loss: 1.79213547706604, 2.254189968109131, 24.334035873413086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927430868148804 2.3655014038085938 25.447757720947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916796207427979 2.2628018856048584 24.41969871520996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925437688827515 2.295790910720825 24.750452041625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921278476715088 2.2065210342407227 23.857337951660156
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926826477050781 1.8525547981262207 20.31822967529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918647527694702 2.2612526416778564 24.404390335083008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917767763137817 2.1950910091400146 23.742687225341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791564702987671 2.076589822769165 22.557462692260742
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900634765625 1.9150985479354858 20.942886352539062
Total LOSS train 24.03891085111178 valid 22.911856651306152
CE LOSS train 1.7922500115174513 valid 0.44797515869140625
Contrastive LOSS train 2.224666113119859 valid 0.47877463698387146
EPOCH 212:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922276258468628 2.324211597442627 25.03434181213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915396690368652 2.3632376194000244 25.42391586303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792475700378418 2.117561101913452 22.96808624267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923753261566162 2.164293050765991 23.435306549072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920668125152588 2.1151926517486572 22.943992614746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928756475448608 2.2039480209350586 23.832355499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918462753295898 2.4124131202697754 25.915977478027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791904091835022 2.128769874572754 23.07960319519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730523109436 1.9114437103271484 20.90616798400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917439937591553 1.9524595737457275 21.31633949279785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927148342132568 2.341456413269043 25.207279205322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79240882396698 2.4170432090759277 25.962841033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792967677116394 2.312049388885498 24.913461685180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792632818222046 2.266535520553589 24.457988739013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910783290863037 2.315396308898926 24.94504165649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792723536491394 2.1887662410736084 23.68038558959961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925821542739868 2.331937313079834 25.111955642700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924224138259888 2.349271059036255 25.285133361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923219203948975 2.089162826538086 22.683950424194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915877103805542 2.13283109664917 23.11989974975586
  batch 20 loss: 1.7915877103805542, 2.13283109664917, 23.11989974975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924590110778809 2.101278305053711 22.80524253845215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924447059631348 2.2081217765808105 23.8736629486084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922208309173584 2.1226534843444824 23.018756866455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.194056987762451 23.733280181884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792393684387207 2.0282816886901855 22.075210571289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925069332122803 2.2631733417510986 24.424240112304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792876124382019 2.1725761890411377 23.51863670349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918719053268433 1.9830307960510254 21.62217903137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933402061462402 2.271570920944214 24.509050369262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915085554122925 2.3814432621002197 25.605939865112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925701141357422 2.4547669887542725 26.340240478515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914007902145386 2.096325635910034 22.754657745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 2.0626211166381836 22.418649673461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925641536712646 2.2915470600128174 24.70803451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925163507461548 2.268409490585327 24.476612091064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926197052001953 2.368187665939331 25.474496841430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.3769690990448 25.562231063842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621446609497 2.1032893657684326 22.82451629638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921534776687622 2.2019968032836914 23.812122344970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916553020477295 2.06673264503479 22.458980560302734
  batch 40 loss: 1.7916553020477295, 2.06673264503479, 22.458980560302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792174220085144 2.162384510040283 23.416017532348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394995689392 2.11194109916687 22.911806106567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920470237731934 2.361166477203369 25.403711318969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916111946105957 2.0278232097625732 22.069843292236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924878597259521 2.145244836807251 23.244935989379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921682596206665 2.139270067214966 23.18486785888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.0875322818756104 22.66738510131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919068336486816 2.244659423828125 24.238500595092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916046380996704 2.1982622146606445 23.774227142333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920461893081665 1.9748313426971436 21.540359497070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910648584365845 2.336284637451172 25.153911590576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923579216003418 2.2904508113861084 24.696866989135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922364473342896 1.9917911291122437 21.710147857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925723791122437 2.2237625122070312 24.030197143554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918401956558228 2.3470606803894043 25.262447357177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925550937652588 2.0239264965057373 22.03182029724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927219867706299 1.9906694889068604 21.699417114257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927004098892212 2.135089635848999 23.143596649169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931303977966309 2.4348134994506836 26.141265869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921361923217773 2.1773674488067627 23.565811157226562
  batch 60 loss: 1.7921361923217773, 2.1773674488067627, 23.565811157226562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792743444442749 2.036285161972046 22.155595779418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916805744171143 2.0247881412506104 22.039562225341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925442457199097 2.1148769855499268 22.941312789916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792128562927246 2.364232301712036 25.434452056884766
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926828861236572 1.7052781581878662 18.8454647064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918645143508911 1.8099310398101807 19.89117431640625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917768955230713 1.8179457187652588 19.971233367919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915645837783813 1.6217478513717651 18.009042739868164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919005155563354 1.5596001148223877 17.387901306152344
Total LOSS train 23.654527517465446 valid 18.81483793258667
CE LOSS train 1.792249795106741 valid 0.44797512888908386
Contrastive LOSS train 2.1862277672841 valid 0.3899000287055969
EPOCH 213:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792227864265442 1.8906004428863525 20.698232650756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915397882461548 2.3493893146514893 25.285432815551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924748659133911 2.0811607837677 22.604082107543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923747301101685 2.2734334468841553 24.526710510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920666933059692 2.3034284114837646 24.826351165771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928755283355713 2.2783596515655518 24.57647132873535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918468713760376 2.3282630443573 25.074478149414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919045686721802 2.3274762630462646 25.066667556762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917312383651733 2.027160167694092 22.06333351135254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917442321777344 2.266101360321045 24.4527587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927147150039673 2.5202414989471436 26.99513053894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924087047576904 2.386054515838623 25.6529541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929675579071045 2.412414073944092 25.9171085357666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926331758499146 2.4420692920684814 26.21332550048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910794019699097 2.3246870040893555 25.037948608398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927241325378418 2.251972198486328 24.31244659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925829887390137 2.2873101234436035 24.66568374633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792423129081726 2.1101574897766113 22.893997192382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923225164413452 2.026379108428955 22.05611228942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915879487991333 2.1606192588806152 23.397781372070312
  batch 20 loss: 1.7915879487991333, 2.1606192588806152, 23.397781372070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924587726593018 2.117100238800049 22.96346092224121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924447059631348 2.0779836177825928 22.572280883789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792220950126648 2.240065097808838 24.19287109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927087545394897 2.0557031631469727 22.349740982055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923938035964966 2.4847891330718994 26.64028549194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925070524215698 2.188870668411255 23.68121337890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79287588596344 2.3690719604492188 25.48359489440918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918720245361328 2.26747465133667 24.46661949157715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933398485183716 2.4468958377838135 26.262298583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915092706680298 2.1104795932769775 22.896305084228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79257071018219 1.9929096698760986 21.72166633605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791401743888855 2.180982828140259 23.601228713989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439579963684 2.089611768722534 22.68855857849121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925641536712646 2.2197811603546143 23.990375518798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925164699554443 2.2535736560821533 24.3282527923584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926195859909058 2.153116464614868 23.32378387451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925398349761963 2.1547300815582275 23.339839935302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916216850280762 1.9878679513931274 21.67030143737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921535968780518 2.2904205322265625 24.696359634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791655421257019 1.832279920578003 20.11445426940918
  batch 40 loss: 1.791655421257019, 1.832279920578003, 20.11445426940918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921745777130127 1.9105943441390991 20.89811897277832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923952341079712 2.0310189723968506 22.102584838867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920472621917725 2.457448720932007 26.366535186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791611909866333 2.024061441421509 22.0322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924882173538208 2.0298497676849365 22.090986251831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921684980392456 2.161036968231201 23.402538299560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.253925085067749 24.331314086914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919070720672607 2.0351979732513428 22.14388656616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.0954384803771973 22.745988845825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920461893081665 1.6907354593276978 18.699399948120117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910652160644531 2.339489459991455 25.185958862304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792358160018921 2.3230674266815186 25.023033142089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922366857528687 2.070025682449341 22.49249267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925721406936646 2.032632350921631 22.118896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918399572372437 2.244452953338623 24.236370086669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925541400909424 2.09639048576355 22.756460189819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792720913887024 2.4954259395599365 26.746980667114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926995754241943 1.9852244853973389 21.64494514465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931294441223145 2.4715893268585205 26.509021759033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921359539031982 2.3897056579589844 25.689191818237305
  batch 60 loss: 1.7921359539031982, 2.3897056579589844, 25.689191818237305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927435636520386 2.3752355575561523 25.54509925842285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916810512542725 1.729315996170044 19.084840774536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925442457199097 1.8808305263519287 20.600849151611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921284437179565 2.323037624359131 25.022504806518555
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926822900772095 1.6104280948638916 17.896963119506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.1671464443206787 23.463329315185547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917765378952026 2.110363721847534 22.895414352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915645837783813 1.857916235923767 20.3707275390625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919002771377563 1.7134298086166382 18.926198959350586
Total LOSS train 23.579502868652344 valid 21.413917541503906
CE LOSS train 1.7922498758022603 valid 0.4479750692844391
Contrastive LOSS train 2.178725295800429 valid 0.42835745215415955
EPOCH 214:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922272682189941 2.104093551635742 22.833162307739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915396690368652 2.5277578830718994 27.06911849975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924747467041016 2.1276285648345947 23.06875991821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792375087738037 2.189840793609619 23.69078254699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920671701431274 2.0603113174438477 22.395179748535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792876124382019 2.1706783771514893 23.499658584594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918468713760376 2.154677391052246 23.338621139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919042110443115 2.0688467025756836 22.480371475219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.244224786758423 24.233978271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917441129684448 2.065316915512085 22.444913864135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927138805389404 2.2858152389526367 24.65086555480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792409062385559 2.237395763397217 24.16636848449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929672002792358 2.387929677963257 25.672264099121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926331758499146 2.2811925411224365 24.60455894470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910804748535156 2.2416114807128906 24.207195281982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792724609375 2.2591335773468018 24.38405990600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925833463668823 2.2604587078094482 24.397171020507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924237251281738 2.4132168292999268 25.924591064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792323112487793 2.0269439220428467 22.061763763427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915890216827393 2.090421438217163 22.695804595947266
  batch 20 loss: 1.7915890216827393, 2.090421438217163, 22.695804595947266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924590110778809 1.9876646995544434 21.66910743713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924449443817139 2.185699462890625 23.649438858032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922208309173584 2.044790029525757 22.240121841430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927091121673584 2.3155903816223145 24.948612213134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923942804336548 2.459304094314575 26.385435104370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925074100494385 2.198045015335083 23.77295684814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928764820098877 2.113719940185547 22.930076599121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918729782104492 2.208010673522949 23.871978759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933399677276611 2.473867654800415 26.53201675415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915098667144775 2.0238847732543945 22.030357360839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925711870193481 2.49324631690979 26.725034713745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914025783538818 2.4950602054595947 26.74200439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924401760101318 2.3528542518615723 25.320981979370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925643920898438 2.1838865280151367 23.63142967224121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792516827583313 2.3368523120880127 25.161039352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926197052001953 2.3538005352020264 25.330625534057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925399541854858 2.3185219764709473 24.977758407592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621446609497 2.1104390621185303 22.896013259887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921528816223145 2.4334561824798584 26.1267147064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 2.0814120769500732 22.605775833129883
  batch 40 loss: 1.79165518283844, 2.0814120769500732, 22.605775833129883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921745777130127 2.153311252593994 23.325286865234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923945188522339 2.0749690532684326 22.542085647583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920467853546143 2.4125261306762695 25.917308807373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916114330291748 2.254063606262207 24.332246780395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924877405166626 2.1051218509674072 22.843706130981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921671867370605 2.157179832458496 23.36396598815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920621633529663 2.137967824935913 23.171741485595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791906714439392 1.9507848024368286 21.299755096435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.2722535133361816 24.51413917541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920455932617188 2.120877504348755 23.00082015991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910655736923218 2.2819507122039795 24.610572814941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923576831817627 2.2664639949798584 24.456998825073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922364473342896 2.1116976737976074 22.90921401977539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925715446472168 2.208444833755493 23.87701988220215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918400764465332 2.22751522064209 24.066991806030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925543785095215 1.8560032844543457 20.35258674621582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927212715148926 2.0395596027374268 22.188316345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792699933052063 2.1604843139648438 23.39754295349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931296825408936 2.5471608638763428 27.264738082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136549949646 2.254467487335205 24.336811065673828
  batch 60 loss: 1.792136549949646, 2.254467487335205, 24.336811065673828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927429676055908 2.4029488563537598 25.82223129272461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916810512542725 2.043001174926758 22.22169303894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925444841384888 1.8602557182312012 20.395103454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921290397644043 2.027728796005249 22.06941795349121
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926828861236572 1.957091212272644 21.36359405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918651103973389 2.2450642585754395 24.24250602722168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917773723602295 2.260270833969116 24.394485473632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791565179824829 2.017998456954956 21.97154998779297
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791900634765625 1.8412820100784302 20.204721450805664
Total LOSS train 23.830931648841272 valid 22.70331573486328
CE LOSS train 1.7922499583317684 valid 0.44797515869140625
Contrastive LOSS train 2.203868165382972 valid 0.46032050251960754
EPOCH 215:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922277450561523 2.1983635425567627 23.775863647460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915401458740234 2.431256055831909 26.104101181030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924747467041016 2.0152511596679688 21.94498634338379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923747301101685 1.8277745246887207 20.070119857788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920668125152588 1.9444340467453003 21.236406326293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928751707077026 1.8824361562728882 20.617237091064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918466329574585 2.348100185394287 25.272850036621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919044494628906 2.234165906906128 24.133563995361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917309999465942 1.9762011766433716 21.553743362426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917442321777344 2.173884630203247 23.530590057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 1.9703221321105957 21.495933532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924081087112427 2.6485953330993652 28.278362274169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929657697677612 2.228766441345215 24.080629348754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79263174533844 2.2104597091674805 23.897228240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910796403884888 2.361659049987793 25.407670974731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927227020263672 2.361037015914917 25.403093338012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925822734832764 1.9174492359161377 20.967073440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924226522445679 2.0460197925567627 22.252620697021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792321801185608 2.021820068359375 22.010522842407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915879487991333 2.1276652812957764 23.068241119384766
  batch 20 loss: 1.7915879487991333, 2.1276652812957764, 23.068241119384766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792457938194275 2.109393835067749 22.886396408081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924444675445557 2.2800283432006836 24.592727661132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922207117080688 2.1471660137176514 23.26388168334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927085161209106 2.2239866256713867 24.032575607299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923938035964966 2.4106550216674805 25.898944854736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792506456375122 2.1844446659088135 23.636953353881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792874813079834 2.1677639484405518 23.47051429748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918721437454224 2.039412021636963 22.185991287231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933380603790283 2.108616590499878 22.879505157470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915096282958984 2.239612340927124 24.187633514404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792569637298584 2.4677603244781494 26.470172882080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914023399353027 1.961507797241211 21.40648078918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924392223358154 2.180285692214966 23.595294952392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925643920898438 2.3149757385253906 24.94232177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925163507461548 2.38655948638916 25.658111572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926194667816162 2.2445597648620605 24.238218307495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925392389297485 2.1502816677093506 23.29535675048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916215658187866 2.086029052734375 22.651912689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792152762413025 2.121250867843628 23.004661560058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.27874493598938 24.579103469848633
  batch 40 loss: 1.7916548252105713, 2.27874493598938, 24.579103469848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921737432479858 2.120894432067871 23.001117706298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923945188522339 1.9545141458511353 21.337535858154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920470237731934 2.2713472843170166 24.50551986694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612148284912 1.996479868888855 21.756410598754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924871444702148 2.053654432296753 22.32903289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921675443649292 2.1530206203460693 23.32237434387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920620441436768 2.364640712738037 25.4384708404541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919070720672607 1.9617513418197632 21.409420013427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.16281795501709 23.419784545898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920458316802979 2.092512845993042 22.717174530029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910659313201904 2.3736343383789062 25.527408599853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923567295074463 2.2293856143951416 24.086212158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922358512878418 2.110337018966675 22.895606994628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925701141357422 2.2466185092926025 24.25875473022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79183828830719 2.473949909210205 26.531335830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925528287887573 2.1935641765594482 23.728195190429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927197217941284 2.3018722534179688 24.81144142150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926987409591675 2.118297576904297 22.97567367553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931281328201294 2.4178473949432373 25.971603393554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921355962753296 2.0921289920806885 22.713424682617188
  batch 60 loss: 1.7921355962753296, 2.0921289920806885, 22.713424682617188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927422523498535 2.204939603805542 23.842138290405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791680932044983 2.125676393508911 23.048444747924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925432920455933 2.143507480621338 23.227617263793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921278476715088 2.3729918003082275 25.522045135498047
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792680025100708 1.357262372970581 15.365303993225098
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 1.9482287168502808 21.27414894104004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791775107383728 1.9214255809783936 21.006031036376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915630340576172 1.6579127311706543 18.370691299438477
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918989658355713 1.4829732179641724 16.621631622314453
Total LOSS train 23.47260990142822 valid 19.31812572479248
CE LOSS train 1.792249321937561 valid 0.4479747414588928
Contrastive LOSS train 2.1680360500629132 valid 0.3707433044910431
EPOCH 216:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922253608703613 1.9644813537597656 21.43703842163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791538953781128 2.60164475440979 27.807985305786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924736738204956 2.0525596141815186 22.318069458007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923742532730103 2.0959086418151855 22.751461029052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920664548873901 2.348374366760254 25.27581024169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928729057312012 2.100041151046753 22.793285369873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918457984924316 2.287977695465088 24.671621322631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919050455093384 2.1258444786071777 23.050350189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917317152023315 1.9515246152877808 21.306976318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917449474334717 2.0844197273254395 22.635940551757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927143573760986 2.3713953495025635 25.506668090820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924091815948486 2.395988941192627 25.75229835510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929662466049194 2.3056633472442627 24.849599838256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792631983757019 2.000239372253418 21.795024871826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910809516906738 2.041429042816162 22.205371856689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792723298072815 2.3810861110687256 25.60358428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792582631111145 2.3182008266448975 24.974592208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792422890663147 2.282853364944458 24.620956420898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923227548599243 2.0224244594573975 22.01656723022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791589379310608 1.961798071861267 21.409570693969727
  batch 20 loss: 1.791589379310608, 1.961798071861267, 21.409570693969727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924582958221436 2.066063404083252 22.45309066772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792444109916687 2.188106060028076 23.673505783081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922205924987793 1.9531179666519165 21.323400497436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927080392837524 2.1508710384368896 23.30141830444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792393684387207 2.291632890701294 24.708721160888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925069332122803 2.2594423294067383 24.386930465698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928752899169922 2.230982780456543 24.102703094482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918727397918701 2.096439838409424 22.756269454956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793337106704712 2.188007354736328 23.673410415649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 2.010669231414795 21.898202896118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925684452056885 2.4807209968566895 26.599777221679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914015054702759 2.4811770915985107 26.603172302246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.1734817028045654 23.52725601196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925617694854736 2.267798900604248 24.470552444458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925143241882324 2.4057233333587646 25.849748611450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926172018051147 2.363630771636963 25.428924560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792538046836853 2.3747904300689697 25.540441513061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621208190918 2.1354475021362305 23.146095275878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921518087387085 2.331000566482544 25.102157592773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.0607028007507324 22.398683547973633
  batch 40 loss: 1.791654348373413, 2.0607028007507324, 22.398683547973633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792172908782959 2.227653741836548 24.068710327148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394757270813 2.0920701026916504 22.71309471130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920475006103516 2.3573200702667236 25.36524772644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612982749939 2.207423448562622 23.865846633911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924879789352417 2.208723545074463 23.879722595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792167067527771 1.977624773979187 21.56841468811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792062759399414 1.9704759120941162 21.496822357177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919081449508667 1.9051308631896973 20.843215942382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791604995727539 1.9661298990249634 21.452903747558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920467853546143 1.8969415426254272 20.761463165283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791067123413086 2.45939302444458 26.38499641418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792356252670288 2.177137851715088 23.56373405456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922353744506836 2.191727638244629 23.709510803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792572021484375 2.0827584266662598 22.620155334472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918411493301392 2.200928211212158 23.801122665405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925539016723633 2.178054094314575 23.57309341430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927191257476807 2.434129476547241 26.134014129638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926974296569824 2.141382932662964 23.206527709960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793127179145813 2.1264874935150146 23.058002471923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792135238647461 2.3895766735076904 25.687902450561523
  batch 60 loss: 1.792135238647461, 2.3895766735076904, 25.687902450561523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927428483963013 2.2340126037597656 24.132869720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916806936264038 2.161001443862915 23.401695251464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542576789856 2.1125521659851074 22.918066024780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792127251625061 2.1524720191955566 23.31684684753418
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926803827285767 1.9468193054199219 21.260873794555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864037513733 2.2462785243988037 24.254650115966797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917758226394653 2.2260594367980957 24.052370071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915645837783813 1.92497980594635 21.041362762451172
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919001579284668 2.0029685497283936 21.82158660888672
Total LOSS train 23.638647490281326 valid 22.792492389678955
CE LOSS train 1.7922492210681622 valid 0.4479750394821167
Contrastive LOSS train 2.1846398390256443 valid 0.5007421374320984
EPOCH 217:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922263145446777 2.1213464736938477 23.005691528320312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915399074554443 2.5638465881347656 27.43000602722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924730777740479 2.131356716156006 23.106040954589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923731803894043 2.042997360229492 22.222347259521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920658588409424 2.2144601345062256 23.936668395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792873501777649 2.306520938873291 24.858081817626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918457984924316 2.399918794631958 25.791032791137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919039726257324 2.1783487796783447 23.57539176940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.063357353210449 22.425304412841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917436361312866 2.115266799926758 22.944412231445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792712688446045 2.3916540145874023 25.709253311157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924073934555054 2.2363991737365723 24.15639877319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792964220046997 2.044384241104126 22.236806869506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926305532455444 2.292975425720215 24.72238540649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791080117225647 2.3613882064819336 25.40496253967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927219867706299 2.360947847366333 25.40220069885254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925808429718018 2.1826870441436768 23.61945152282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924214601516724 2.241769552230835 24.21011734008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923213243484497 2.041689157485962 22.209213256835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915886640548706 2.1888372898101807 23.679960250854492
  batch 20 loss: 1.7915886640548706, 2.1888372898101807, 23.679960250854492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924569845199585 2.108633518218994 22.87879180908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792442798614502 2.070526123046875 22.497703552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218804359436 1.8093277215957642 19.885496139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792705774307251 2.2297797203063965 24.090503692626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923914194107056 2.40708065032959 25.863197326660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925047874450684 2.3482584953308105 25.27509117126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928736209869385 2.311157703399658 24.904449462890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918717861175537 2.0939126014709473 22.73099708557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933366298675537 2.31201171875 24.913454055786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915098667144775 2.1598305702209473 23.389814376831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925686836242676 2.4337196350097656 26.129764556884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791401982307434 2.2182087898254395 23.97348976135254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924379110336304 2.2567052841186523 24.3594913482666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925618886947632 2.399991035461426 25.79247283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925149202346802 2.039626359939575 22.188777923583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926181554794312 2.1637256145477295 23.429874420166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925382852554321 2.263840675354004 24.430944442749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916219234466553 2.093735933303833 22.728981018066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792151927947998 2.3100674152374268 24.892826080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.218156576156616 23.973220825195312
  batch 40 loss: 1.7916542291641235, 2.218156576156616, 23.973220825195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921724319458008 2.174084424972534 23.533016204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923927307128906 1.917844295501709 20.970836639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204523563385 2.1426172256469727 23.218217849731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612148284912 1.7767008543014526 19.55862045288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924864292144775 1.733642816543579 19.12891387939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921674251556396 2.1547508239746094 23.339675903320312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.062018632888794 22.412248611450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919069528579712 2.188561201095581 23.677518844604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916045188903809 2.3379273414611816 25.17087745666504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204523563385 2.1777331829071045 23.569377899169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79106605052948 2.299302339553833 24.784088134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923561334609985 2.131971836090088 23.11207389831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792235255241394 2.1616358757019043 23.408594131469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925702333450317 2.1270976066589355 23.063547134399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839838027954 2.2925045490264893 24.71688461303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792553186416626 2.235966920852661 24.1522216796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927191257476807 2.223069906234741 24.023418426513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926976680755615 1.9708248376846313 21.500946044921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931268215179443 2.503279685974121 26.825923919677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921353578567505 2.419050931930542 25.98264503479004
  batch 60 loss: 1.7921353578567505, 2.419050931930542, 25.98264503479004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792741298675537 2.3141863346099854 24.93460464477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791681170463562 2.2333948612213135 24.125629425048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925426959991455 2.157766580581665 23.370208740234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792127013206482 2.142991304397583 23.2220401763916
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926793098449707 1.638107419013977 18.17375373840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918636798858643 2.1302695274353027 23.094560623168945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917757034301758 2.0781447887420654 22.573223114013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915642261505127 1.8212112188339233 20.003677368164062
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918998003005981 1.6688830852508545 18.480731964111328
Total LOSS train 23.67616850046011 valid 21.038048267364502
CE LOSS train 1.7922485259863046 valid 0.44797495007514954
Contrastive LOSS train 2.188391997263982 valid 0.4172207713127136
EPOCH 218:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922260761260986 2.0657472610473633 22.44969940185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915400266647339 2.336984395980835 25.16138458251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924726009368896 2.2226035594940186 24.018508911132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372703552246 2.298369884490967 24.776073455810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920660972595215 2.277496576309204 24.567031860351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928733825683594 2.3044028282165527 24.836902618408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918461561203003 2.336510181427002 25.156946182250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919044494628906 2.2232234477996826 24.024139404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917307615280151 2.115217924118042 22.943910598754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917439937591553 1.8370941877365112 20.16268539428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927124500274658 2.417437791824341 25.967090606689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924072742462158 2.5727016925811768 27.519424438476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792964220046997 2.2370169162750244 24.16313362121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792630672454834 2.151977300643921 23.31240463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910809516906738 2.2902071475982666 24.693151473999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792721152305603 2.4743165969848633 26.535886764526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925803661346436 2.2068028450012207 23.860607147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792420744895935 2.3840415477752686 25.632837295532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923206090927124 2.112149238586426 22.9138126373291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915880680084229 2.1101372241973877 22.892959594726562
  batch 20 loss: 1.7915880680084229, 2.1101372241973877, 22.892959594726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792456030845642 2.0014772415161133 21.807228088378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924420833587646 2.2162463665008545 23.954906463623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218565940857 2.127342939376831 23.065649032592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927050590515137 2.0769150257110596 22.56185531616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391300201416 2.4752678871154785 26.545068740844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792503833770752 2.1642138957977295 23.434642791748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928723096847534 2.4162817001342773 25.9556884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918707132339478 2.050783395767212 22.29970359802246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793334722518921 2.1795780658721924 23.589115142822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 2.1795382499694824 23.58689308166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792567491531372 2.4575488567352295 26.368057250976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791401982307434 2.429760217666626 26.089004516601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924373149871826 2.122953176498413 23.021968841552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925610542297363 2.2065200805664062 23.85776138305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925136089324951 2.2632954120635986 24.425466537475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926164865493774 1.8333263397216797 20.125879287719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792536973953247 2.0709972381591797 22.50251007080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916213274002075 2.029003143310547 22.081653594970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921515703201294 2.261591911315918 24.408071517944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.2637319564819336 24.428974151611328
  batch 40 loss: 1.791654109954834, 2.2637319564819336, 24.428974151611328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921723127365112 2.2142386436462402 23.934558868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923927307128906 1.8978188037872314 20.770580291748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920457124710083 2.3667662143707275 25.459707260131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916117906570435 2.129774332046509 23.08935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792485237121582 2.157489061355591 23.367374420166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921658754348755 2.252209186553955 24.31425666809082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920608520507812 1.9816657304763794 21.608718872070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919061183929443 2.248666524887085 24.27857208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 2.218235969543457 23.97396469116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792044758796692 2.2032835483551025 23.824880599975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910661697387695 2.2110583782196045 23.901649475097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923554182052612 2.1707117557525635 23.49947166442871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922345399856567 2.1113717555999756 22.90595245361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792569637298584 2.2740046977996826 24.532617568969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.4395604133605957 26.187442779541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925519943237305 2.161881446838379 23.411365509033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927179336547852 2.479259967803955 26.585315704345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926968336105347 2.196810483932495 23.760801315307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931256294250488 2.4070944786071777 25.864070892333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921346426010132 2.140015125274658 23.192285537719727
  batch 60 loss: 1.7921346426010132, 2.140015125274658, 23.192285537719727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927405834197998 2.2551207542419434 24.343948364257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916805744171143 1.9876452684402466 21.668134689331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 2.095787525177002 22.750415802001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792127013206482 2.3029136657714844 24.821264266967773
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926793098449707 1.7489612102508545 19.282291412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918645143508911 2.3075718879699707 24.867582321166992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917765378952026 2.360985279083252 25.401628494262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915650606155396 1.980878472328186 21.60034942626953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919002771377563 1.959212064743042 21.384021759033203
Total LOSS train 23.861964387160082 valid 23.313395500183105
CE LOSS train 1.79224798312554 valid 0.4479750692844391
Contrastive LOSS train 2.2069716398532573 valid 0.4898030161857605
EPOCH 219:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792225956916809 2.2447874546051025 24.240100860595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540503501892 2.4752938747406006 26.544479370117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924721240997314 2.187563419342041 23.668106079101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372703552246 2.175549268722534 23.547866821289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920653820037842 2.271000862121582 24.502073287963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928723096847534 2.2470736503601074 24.263608932495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918457984924316 2.475299119949341 26.544836044311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919044494628906 2.0469160079956055 22.261064529418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917312383651733 2.206556558609009 23.857295989990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917439937591553 2.046290397644043 22.254648208618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927119731903076 2.391545295715332 25.70816421508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792406678199768 2.491457939147949 26.706985473632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792962908744812 2.1773579120635986 23.56654167175293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792629361152649 2.469895839691162 26.491588592529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910809516906738 2.232508420944214 24.116165161132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927207946777344 2.3769071102142334 25.561792373657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925803661346436 2.2723066806793213 24.51564598083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924206256866455 2.5036709308624268 26.829130172729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792320728302002 2.0549635887145996 22.341957092285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915881872177124 2.3120086193084717 24.91167449951172
  batch 20 loss: 1.7915881872177124, 2.3120086193084717, 24.91167449951172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792455792427063 1.9656643867492676 21.449098587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441964149475 2.069911241531372 22.491554260253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922184467315674 1.8177801370620728 19.970020294189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927054166793823 1.7084189653396606 18.876895904541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923915386199951 2.0164425373077393 21.956815719604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925043106079102 2.125514507293701 23.047649383544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928718328475952 2.3439321517944336 25.232192993164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918710708618164 2.1371750831604004 23.163619995117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933344841003418 2.165259599685669 23.44593048095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915096282958984 2.112880229949951 22.920312881469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925677299499512 2.4723169803619385 26.515737533569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914022207260132 2.4911623001098633 26.703025817871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924368381500244 2.08986759185791 22.691112518310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925609350204468 2.272679567337036 24.51935577392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925136089324951 2.3631579875946045 25.42409324645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792616605758667 2.3867554664611816 25.66016960144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 2.1073272228240967 22.86581039428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 1.6812556982040405 18.60417938232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792151927947998 2.2516064643859863 24.308216094970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.252687692642212 24.318531036376953
  batch 40 loss: 1.7916545867919922, 2.252687692642212, 24.318531036376953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921725511550903 2.323951482772827 25.031688690185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792392611503601 1.9787592887878418 21.579986572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920459508895874 2.2458436489105225 24.2504825592041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791612148284912 2.19560170173645 23.747629165649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792485237121582 2.187636137008667 23.668846130371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921661138534546 2.2138259410858154 23.9304256439209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920607328414917 2.1821181774139404 23.613243103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919057607650757 2.1980106830596924 23.77201271057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916035652160645 2.386033535003662 25.651939392089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920442819595337 1.7875863313674927 19.66790771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79106605052948 2.172344923019409 23.514514923095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792354702949524 2.2530646324157715 24.323001861572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922340631484985 2.0051980018615723 21.844213485717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925689220428467 2.1734678745269775 23.527246475219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839838027954 2.3833184242248535 25.625022888183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925519943237305 1.9990899562835693 21.783451080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927179336547852 2.137434482574463 23.16706085205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926963567733765 1.8028761148452759 19.821456909179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793125033378601 2.173438787460327 23.52751350402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921348810195923 2.3851158618927 25.643293380737305
  batch 60 loss: 1.7921348810195923, 2.3851158618927, 25.643293380737305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927401065826416 2.0674309730529785 22.46704864501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791681170463562 2.087785005569458 22.669530868530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.1310033798217773 23.102575302124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921271324157715 2.3765642642974854 25.557769775390625
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926795482635498 1.8444368839263916 20.23704719543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 2.2367959022521973 24.159822463989258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917766571044922 2.1953964233398438 23.74574089050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915652990341187 1.8851573467254639 20.643138885498047
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919005155563354 1.895645022392273 20.748350143432617
Total LOSS train 23.666507016695462 valid 22.324263095855713
CE LOSS train 1.7922479042640098 valid 0.44797512888908386
Contrastive LOSS train 2.1874259270154512 valid 0.47391125559806824
EPOCH 220:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922261953353882 2.1564972400665283 23.35719871520996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540503501892 2.462949275970459 26.42103385925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792472004890442 1.8391233682632446 20.183706283569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923721075057983 2.079453468322754 22.58690643310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920646667480469 2.0819506645202637 22.611572265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792871117591858 2.1605947017669678 23.398818969726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918455600738525 1.8649953603744507 20.44179916381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919042110443115 2.120734691619873 22.999252319335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917314767837524 2.1867012977600098 23.658742904663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791744351387024 2.0235674381256104 22.02741813659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927122116088867 2.3294336795806885 25.087047576904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924071550369263 2.5093681812286377 26.886089324951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929627895355225 2.155884265899658 23.351804733276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792629599571228 2.2988531589508057 24.781160354614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910820245742798 2.3480687141418457 25.27176856994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927203178405762 2.3572793006896973 25.36551284790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925801277160645 2.3492627143859863 25.285205841064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924203872680664 2.34810209274292 25.273441314697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923210859298706 2.0473945140838623 22.266265869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915887832641602 2.159154176712036 23.383129119873047
  batch 20 loss: 1.7915887832641602, 2.159154176712036, 23.383129119873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924562692642212 2.3514583110809326 25.307039260864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924423217773438 2.3442535400390625 25.23497772216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922186851501465 1.689500093460083 18.687219619750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927054166793823 1.7966185808181763 19.758892059326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923918962478638 2.169532299041748 23.487716674804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925041913986206 2.328934669494629 25.081850051879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928717136383057 2.315197467803955 24.94484519958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791871190071106 2.1404788494110107 23.1966609954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933340072631836 2.3915579319000244 25.708911895751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915098667144775 2.1378211975097656 23.169721603393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925665378570557 2.508686065673828 26.879426956176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791401743888855 2.255520820617676 24.346609115600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792435646057129 2.0195233821868896 21.9876708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925593852996826 2.4054923057556152 25.847482681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79251229763031 2.4588353633880615 26.38086700439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792615532875061 2.2588887214660645 24.381502151489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925366163253784 2.126666307449341 23.0591983795166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916219234466553 1.895668387413025 20.748306274414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921518087387085 2.3210673332214355 25.002826690673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.086810350418091 22.659757614135742
  batch 40 loss: 1.7916542291641235, 2.086810350418091, 22.659757614135742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921726703643799 2.332686185836792 25.119035720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923927307128906 2.0249578952789307 22.04197120666504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920465469360352 2.2514636516571045 24.306682586669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916131019592285 2.142869710922241 23.22031021118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924861907958984 2.109488010406494 22.887365341186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921667098999023 2.1729819774627686 23.521987915039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920613288879395 1.9985580444335938 21.77764129638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791906476020813 2.0312933921813965 22.104841232299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 2.217341661453247 23.96501922607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792044758796692 2.2014660835266113 23.806705474853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910672426223755 2.3356404304504395 25.147470474243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923550605773926 2.2774391174316406 24.56674575805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922343015670776 2.124293088912964 23.035165786743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792568564414978 2.078322172164917 22.575790405273438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.2733070850372314 24.52490997314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925504446029663 2.076171875 22.554269790649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792716145515442 1.975961685180664 21.55233383178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792695164680481 1.989980697631836 21.692502975463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931238412857056 2.3858444690704346 25.651567459106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921336889266968 2.14634370803833 23.255569458007812
  batch 60 loss: 1.7921336889266968, 2.14634370803833, 23.255569458007812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927385568618774 2.1366355419158936 23.159093856811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916812896728516 2.0897557735443115 22.689239501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 2.1040914058685303 22.83345603942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792127251625061 2.242199659347534 24.21412467956543
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926791906356812 1.9604206085205078 21.39688491821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918639183044434 2.395585298538208 25.747716903686523
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917762994766235 2.1474952697753906 23.2667293548584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915648221969604 2.0978493690490723 22.770057678222656
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918998003005981 1.866466999053955 20.45656967163086
Total LOSS train 23.570923731877254 valid 23.06026840209961
CE LOSS train 1.7922477593788735 valid 0.44797495007514954
Contrastive LOSS train 2.177867603302002 valid 0.46661674976348877
EPOCH 221:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922254800796509 2.2626237869262695 24.4184627532959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540265083313 2.4906342029571533 26.6978816986084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924715280532837 2.2199699878692627 23.992170333862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372465133667 2.0618109703063965 22.41048240661621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920660972595215 2.0028529167175293 21.82059669494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928720712661743 2.2226529121398926 24.019399642944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918463945388794 2.2197909355163574 23.989757537841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791905164718628 2.237114429473877 24.163047790527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917320728302002 2.0057575702667236 21.849308013916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917442321777344 2.011533260345459 21.90707778930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927113771438599 2.5109550952911377 26.90226173400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924065589904785 2.3895680904388428 25.688087463378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929627895355225 2.108649969100952 22.87946319580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792629599571228 2.237312078475952 24.16575050354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910828590393066 2.2738678455352783 24.529760360717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927201986312866 2.371943473815918 25.512155532836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925803661346436 2.0617971420288086 22.410551071166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924199104309082 2.078591823577881 22.578338623046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923200130462646 2.1478750705718994 23.27107048034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915884256362915 2.0687649250030518 22.479236602783203
  batch 20 loss: 1.7915884256362915, 2.0687649250030518, 22.479236602783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924551963806152 1.9770023822784424 21.56247901916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441725730896 2.082054376602173 22.612985610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218804359436 2.068037509918213 22.472593307495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927054166793823 1.7966939210891724 19.759645462036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923917770385742 2.3037033081054688 24.829425811767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925044298171997 2.383026599884033 25.622770309448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928718328475952 2.2583365440368652 24.376237869262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918709516525269 2.2661869525909424 24.453739166259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933322191238403 2.2225847244262695 24.019180297851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915090322494507 2.2056052684783936 23.847562789916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925652265548706 2.1868057250976562 23.660621643066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914024591445923 2.37160587310791 25.507461547851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792436122894287 2.098240613937378 22.774843215942383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925602197647095 2.0821216106414795 22.61377716064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513370513916 2.303581476211548 24.828327178955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926158905029297 2.180525779724121 23.59787368774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925366163253784 2.4369874000549316 26.16240882873535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916216850280762 2.0927844047546387 22.719467163085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921510934829712 2.3087992668151855 24.880144119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.2371888160705566 24.163541793823242
  batch 40 loss: 1.791654109954834, 2.2371888160705566, 24.163541793823242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792171597480774 2.2161154747009277 23.953327178955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923921346664429 2.1304831504821777 23.097225189208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920455932617188 2.3228061199188232 25.02010726928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916125059127808 2.1049795150756836 22.841407775878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924845218658447 2.1051878929138184 22.844364166259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921653985977173 2.2092180252075195 23.88434600830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920606136322021 2.2311015129089355 24.103076934814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919062376022339 1.9965463876724243 21.757369995117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 1.855578899383545 20.347393035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920445203781128 2.2051053047180176 23.843095779418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910672426223755 2.360701084136963 25.3980770111084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792353868484497 2.2062063217163086 23.85441780090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922337055206299 2.1911158561706543 23.703393936157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925682067871094 1.9218608140945435 21.01117706298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918397188186646 2.138878345489502 23.180622100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 2.0046069622039795 21.838621139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927160263061523 2.41981840133667 25.990901947021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926936149597168 1.9201209545135498 20.99390411376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931221723556519 2.3325533866882324 25.118656158447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921340465545654 2.305224895477295 24.844383239746094
  batch 60 loss: 1.7921340465545654, 2.305224895477295, 24.844383239746094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792738914489746 2.189180612564087 23.68454360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916812896728516 2.1751928329467773 23.543609619140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925395965576172 2.117262601852417 22.965166091918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921252250671387 2.233665704727173 24.128782272338867
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926764488220215 1.5703500509262085 17.496177673339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918641567230225 2.3054468631744385 24.846332550048828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917765378952026 2.186534881591797 23.65712547302246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915652990341187 2.0663061141967773 22.454626083374023
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7919002771377563 1.8357468843460083 20.149370193481445
Total LOSS train 23.53221682035006 valid 22.77686357498169
CE LOSS train 1.792247438430786 valid 0.4479750692844391
Contrastive LOSS train 2.173996925354004 valid 0.4589367210865021
EPOCH 222:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922254800796509 2.3116679191589355 24.908905029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915408611297607 2.4041342735290527 25.832883834838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924716472625732 2.0026495456695557 21.818965911865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372465133667 2.2045276165008545 23.837648391723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920643091201782 1.7126673460006714 18.918737411499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928698062896729 2.2962679862976074 24.755550384521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918449640274048 2.414700746536255 25.938852310180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919042110443115 2.253504753112793 24.32695198059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917317152023315 2.1385769844055176 23.177499771118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917441129684448 2.070816993713379 22.499914169311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927099466323853 2.5632591247558594 27.42530059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924054861068726 2.3184194564819336 24.976600646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929601669311523 2.374567747116089 25.538639068603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926276922225952 2.3501856327056885 25.294483184814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910829782485962 2.3150553703308105 24.94163703918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792719841003418 2.43959379196167 26.18865966796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925798892974854 2.2197372913360596 23.989952087402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924201488494873 2.392098903656006 25.713409423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792320728302002 2.036876916885376 22.161088943481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791589617729187 1.9239726066589355 21.03131675720215
  batch 20 loss: 1.791589617729187, 1.9239726066589355, 21.03131675720215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924556732177734 2.079231023788452 22.584766387939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924424409866333 2.1313326358795166 23.10576820373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218804359436 1.9672932624816895 21.465150833129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927049398422241 1.988680124282837 21.679506301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923915386199951 2.4537453651428223 26.329843521118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925037145614624 2.3280045986175537 25.07254981994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792871356010437 2.184556007385254 23.638431549072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918719053268433 2.166635751724243 23.458229064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933326959609985 2.199976921081543 23.793102264404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915114164352417 2.365874767303467 25.450260162353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925676107406616 2.1696877479553223 23.489444732666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791403889656067 2.5520265102386475 27.311670303344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792436957359314 2.160094738006592 23.39338493347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925605773925781 2.2595314979553223 24.387874603271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513370513916 2.189980983734131 23.692323684692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926145792007446 2.253347635269165 24.32608985900879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925353050231934 2.337527275085449 25.167808532714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621208190918 2.1621816158294678 23.413436889648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792150855064392 2.2749085426330566 24.541234970092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.216120481491089 23.95285987854004
  batch 40 loss: 1.7916538715362549, 2.216120481491089, 23.95285987854004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921712398529053 2.3180885314941406 24.97305679321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923908233642578 1.9844717979431152 21.637109756469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920440435409546 2.1613831520080566 23.405874252319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791610598564148 1.7024080753326416 18.815690994262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924822568893433 1.9731124639511108 21.52360725402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921642065048218 2.109570264816284 22.887866973876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920597791671753 2.0556256771087646 22.348316192626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919056415557861 2.110445499420166 22.896360397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916038036346436 2.299468517303467 24.78628921508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920440435409546 2.081265449523926 22.604698181152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910672426223755 2.3138015270233154 24.9290828704834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923532724380493 2.1731200218200684 23.5235538482666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792232871055603 1.9869202375411987 21.661434173583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925676107406616 2.1787643432617188 23.580211639404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.2603352069854736 24.395191192626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925517559051514 2.183901071548462 23.631561279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927173376083374 2.283118486404419 24.6239013671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792696237564087 2.2189676761627197 23.982372283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931241989135742 2.5118730068206787 26.911853790283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792135238647461 2.131749153137207 23.10962677001953
  batch 60 loss: 1.792135238647461, 2.131749153137207, 23.10962677001953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927378416061401 2.2576558589935303 24.36929702758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916808128356934 2.0648674964904785 22.44035530090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925395965576172 2.0788209438323975 22.58074951171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921252250671387 2.188102960586548 23.673154830932617
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926766872406006 1.6760207414627075 18.552885055541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 2.1843795776367188 23.635658264160156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917759418487549 2.159968137741089 23.39145851135254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791564702987671 1.8507587909698486 20.299152374267578
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918998003005981 1.8523750305175781 20.315650939941406
Total LOSS train 23.713458985548755 valid 21.91048002243042
CE LOSS train 1.7922472348579994 valid 0.44797495007514954
Contrastive LOSS train 2.19212117928725 valid 0.46309375762939453
EPOCH 223:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922248840332031 2.180697202682495 23.599197387695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915409803390503 2.3070261478424072 24.861801147460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924706935882568 2.0296833515167236 22.089303970336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923712730407715 2.069955348968506 22.491926193237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920645475387573 2.176079273223877 23.5528564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928698062896729 2.2413742542266846 24.20661163330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918450832366943 2.400805950164795 25.79990577697754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919026613235474 2.0226564407348633 22.01846694946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172945022583 1.679996371269226 18.591691970825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791742205619812 2.1691229343414307 23.48297119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.3886139392852783 25.678848266601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924047708511353 2.5524024963378906 27.316429138183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929598093032837 2.130544900894165 23.098407745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926278114318848 2.2805237770080566 24.597864151000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910829782485962 2.318852424621582 24.97960662841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927184104919434 2.387225866317749 25.66497802734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925786972045898 2.3023765087127686 24.81634521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924187183380127 2.1425626277923584 23.218046188354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923190593719482 2.1023454666137695 22.815773010253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915891408920288 2.1934075355529785 23.725664138793945
  batch 20 loss: 1.7915891408920288, 2.1934075355529785, 23.725664138793945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924537658691406 1.971813678741455 21.510589599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924401760101318 2.001335859298706 21.80579948425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922167778015137 1.9218792915344238 21.011009216308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927029132843018 2.204184055328369 23.834543228149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792389988899231 2.4194579124450684 25.986970901489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925021648406982 2.207141876220703 23.863920211791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928694486618042 2.298880100250244 24.78166961669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918705940246582 2.3271610736846924 25.063480377197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933309078216553 2.0045034885406494 21.83836555480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915107011795044 2.2283565998077393 24.075077056884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792565941810608 2.432511329650879 26.117679595947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79140305519104 2.381194591522217 25.603349685668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924344539642334 2.2306463718414307 24.09889793395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925580739974976 2.3009252548217773 24.80181121826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792510986328125 2.2546257972717285 24.338768005371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926135063171387 2.2181928157806396 23.97454261779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925342321395874 2.325916290283203 25.05169677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916215658187866 2.0249502658843994 22.04112434387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921500205993652 2.4001739025115967 25.79388999938965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916535139083862 2.26363468170166 24.42799949645996
  batch 40 loss: 1.7916535139083862, 2.26363468170166, 24.42799949645996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170524597168 2.1414473056793213 23.206642150878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923901081085205 1.953179121017456 21.324182510375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920440435409546 2.1296303272247314 23.088346481323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791611671447754 2.206859588623047 23.860206604003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924830913543701 2.1349754333496094 23.142236709594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921645641326904 2.3820316791534424 25.61248016357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920598983764648 1.7104166746139526 18.896224975585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791905403137207 2.169184684753418 23.483753204345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.1674742698669434 23.466346740722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920433282852173 2.09275484085083 22.71959114074707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910676002502441 2.155515432357788 23.346221923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923527956008911 2.257775068283081 24.37010383605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922320365905762 1.9508399963378906 21.30063247680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925667762756348 2.095489978790283 22.747465133666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918386459350586 2.3646178245544434 25.438018798828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925492525100708 2.177619695663452 23.56874656677246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792714238166809 2.3241708278656006 25.034423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792693018913269 2.1225264072418213 23.017955780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931214570999146 2.3484485149383545 25.277606964111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921333312988281 2.202960968017578 23.82174301147461
  batch 60 loss: 1.7921333312988281, 2.202960968017578, 23.82174301147461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792737364768982 2.2149558067321777 23.942296981811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791680932044983 2.1562891006469727 23.354572296142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792539358139038 2.174663782119751 23.53917694091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921252250671387 2.3695356845855713 25.48748207092285
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926759719848633 1.8593595027923584 20.386272430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 2.215648651123047 23.948348999023438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917757034301758 2.309739828109741 24.889175415039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79156494140625 1.895748257637024 20.749048233032227
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918994426727295 1.7969772815704346 19.76167106628418
Total LOSS train 23.693240151038538 valid 22.337060928344727
CE LOSS train 1.792246257341825 valid 0.4479748606681824
Contrastive LOSS train 2.1900993934044473 valid 0.44924432039260864
EPOCH 224:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792224407196045 2.216369867324829 23.955923080444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915408611297607 2.203174114227295 23.82328224182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924697399139404 2.2407455444335938 24.19992446899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923706769943237 2.0533931255340576 22.32630157470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920641899108887 2.2179551124572754 23.971614837646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792868971824646 2.248443126678467 24.277301788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918447256088257 1.9872205257415771 21.664051055908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919032573699951 1.6874289512634277 18.66619300842285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.083665609359741 22.628387451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917428016662598 1.6119216680526733 17.910959243774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.3435986042022705 25.228694915771484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924048900604248 2.507776975631714 26.870174407958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929586172103882 2.3669025897979736 25.461984634399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926267385482788 2.55896258354187 27.382253646850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910830974578857 2.346341371536255 25.25449562072754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927173376083374 2.236119031906128 24.153907775878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925775051116943 2.2677433490753174 24.47001075744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924177646636963 2.4135656356811523 25.92807388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923187017440796 2.085803270339966 22.65035057067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915886640548706 2.1605820655822754 23.39740753173828
  batch 20 loss: 1.7915886640548706, 2.1605820655822754, 23.39740753173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924531698226929 2.3819239139556885 25.611692428588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439579963684 2.2948720455169678 24.741161346435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216420173645 1.8197250366210938 19.98946762084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927024364471436 1.8161818981170654 19.95452117919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923892736434937 2.5685040950775146 27.47743034362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925010919570923 2.3135154247283936 24.927656173706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928677797317505 2.0106494426727295 21.899362564086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 1.8569754362106323 20.361623764038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933292388916016 2.1955573558807373 23.748903274536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915107011795044 2.105754852294922 22.84906005859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925652265548706 2.4765748977661133 26.558313369750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914031744003296 2.2832136154174805 24.623538970947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792434573173523 2.113011360168457 22.922548294067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792558193206787 2.143725872039795 23.22981834411621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925114631652832 2.466506004333496 26.457571029663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926138639450073 2.219498634338379 23.987600326538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925344705581665 2.1372182369232178 23.164716720581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916216850280762 2.1760497093200684 23.552120208740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921501398086548 2.2918903827667236 24.7110538482666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.2249538898468018 24.04119300842285
  batch 40 loss: 1.7916538715362549, 2.2249538898468018, 24.04119300842285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921712398529053 2.2644050121307373 24.436222076416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923908233642578 2.15256929397583 23.318082809448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792044758796692 2.419708013534546 25.989126205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916128635406494 2.158475637435913 23.37636947631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924832105636597 1.9586445093154907 21.37892723083496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921644449234009 2.3767731189727783 25.559894561767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920598983764648 1.7342846393585205 19.134906768798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919058799743652 1.9817423820495605 21.609331130981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.349626064300537 25.287864685058594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920433282852173 1.8037370443344116 19.82941436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910683155059814 2.327820062637329 25.06926918029785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923527956008911 2.181623697280884 23.60858917236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922327518463135 2.1878747940063477 23.67098045349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792566180229187 2.265848398208618 24.4510498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918388843536377 2.3908019065856934 25.699859619140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925490140914917 2.2499587535858154 24.292137145996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927141189575195 2.52398943901062 27.032608032226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792693018913269 1.961452841758728 21.4072208404541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793121099472046 2.3690414428710938 25.483535766601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921339273452759 2.2563812732696533 24.355945587158203
  batch 60 loss: 1.7921339273452759, 2.2563812732696533, 24.355945587158203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927372455596924 2.1821935176849365 23.614673614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916817665100098 1.998623251914978 21.77791404724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792539358139038 2.1949336528778076 23.74187660217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792125940322876 2.323817491531372 25.03030014038086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926759719848633 1.8108370304107666 19.901046752929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918639183044434 2.230278730392456 24.09465217590332
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917765378952026 2.249981641769409 24.291593551635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915656566619873 2.0049540996551514 21.841106414794922
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791899561882019 1.9182525873184204 20.974424362182617
Total LOSS train 23.662889099121095 valid 22.80044412612915
CE LOSS train 1.7922461381325354 valid 0.44797489047050476
Contrastive LOSS train 2.1870642845447246 valid 0.4795631468296051
EPOCH 225:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792224645614624 2.2548751831054688 24.34097671508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915410995483398 2.4298484325408936 26.09002685546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924697399139404 2.1210246086120605 23.002716064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923710346221924 2.2180428504943848 23.97279930114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920645475387573 2.1677613258361816 23.469676971435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792868971824646 2.164125680923462 23.434125900268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918453216552734 2.2294414043426514 24.086259841918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919037342071533 2.2062132358551025 23.854036331176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917312383651733 2.1292970180511475 23.084701538085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917433977127075 2.145321846008301 23.244962692260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.300868272781372 24.8013916015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924050092697144 2.4857265949249268 26.649669647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929582595825195 2.2938878536224365 24.73183822631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926263809204102 2.3120956420898438 24.91358184814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910836935043335 2.3004403114318848 24.795486450195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927166223526 2.196347951889038 23.756196975708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925773859024048 2.14573073387146 23.24988555908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924176454544067 2.283348321914673 24.625900268554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923190593719482 1.6717398166656494 18.509716033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915890216827393 1.7629045248031616 19.420635223388672
  batch 20 loss: 1.7915890216827393, 1.7629045248031616, 19.420635223388672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924529314041138 2.06811261177063 22.47357940673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924398183822632 2.1291027069091797 23.083467483520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216181755066 1.532978892326355 17.122005462646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927019596099854 2.1501212120056152 23.293914794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923884391784668 2.317396640777588 24.966354370117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792500615119934 1.8272806406021118 20.0653076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928670644760132 2.123412609100342 23.026994705200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918695211410522 2.0153889656066895 21.945758819580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933287620544434 2.348475933074951 25.27808952331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915114164352417 2.164533853530884 23.43684959411621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925655841827393 2.1282904148101807 23.075469970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791404128074646 2.415642499923706 25.947830200195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924349308013916 2.2280590534210205 24.07302474975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792558193206787 2.1953561305999756 23.74612045288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792511224746704 2.290140390396118 24.69391441345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926132678985596 2.183621883392334 23.628833770751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925336360931396 2.237635374069214 24.168888092041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.2210988998413086 24.00261116027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921502590179443 2.439068555831909 26.182836532592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 2.227735996246338 24.069013595581055
  batch 40 loss: 1.7916537523269653, 2.227735996246338, 24.069013595581055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921704053878784 2.052643299102783 22.318601608276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792389988899231 2.0855793952941895 22.648183822631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920445203781128 2.3160784244537354 24.95282745361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916131019592285 2.1924431324005127 23.71604347229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924822568893433 1.6457669734954834 18.250152587890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792163610458374 2.1785354614257812 23.577518463134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792059063911438 1.959119200706482 21.383251190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919055223464966 2.0135538578033447 21.927444458007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603446006775 1.9149837493896484 20.94144058227539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792043924331665 2.120898723602295 23.00103187561035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910690307617188 2.302302122116089 24.814090728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923529148101807 2.2987968921661377 24.78032112121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922331094741821 1.7349737882614136 19.141969680786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925660610198975 2.1293647289276123 23.086214065551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918386459350586 2.2930593490600586 24.722431182861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925485372543335 1.9940567016601562 21.733116149902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927132844924927 2.3117966651916504 24.91067886352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792693018913269 2.1400716304779053 23.193408966064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931203842163086 2.4992265701293945 26.785385131835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921336889266968 2.1608998775482178 23.401132583618164
  batch 60 loss: 1.7921336889266968, 2.1608998775482178, 23.401132583618164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927372455596924 2.30090069770813 24.80174446105957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916823625564575 2.1815810203552246 23.607494354248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925395965576172 2.12325119972229 23.02505111694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921258211135864 2.337523937225342 25.16736602783203
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792675256729126 1.6124660968780518 17.917335510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 2.053950548171997 22.33136749267578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917758226394653 2.035921096801758 22.15098762512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791565179824829 1.9173380136489868 20.96494483947754
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.79189932346344 1.853269100189209 20.3245906829834
Total LOSS train 23.32491821875939 valid 21.442972660064697
CE LOSS train 1.7922460611049946 valid 0.44797483086586
Contrastive LOSS train 2.1532672056784996 valid 0.46331727504730225
EPOCH 226:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922241687774658 2.0855751037597656 22.64797592163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915409803390503 2.4471681118011475 26.263221740722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792468547821045 2.2870304584503174 24.66277313232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923702001571655 2.2507927417755127 24.300296783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920637130737305 2.0219366550445557 22.011428833007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928680181503296 2.155315637588501 23.346023559570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918455600738525 2.027766466140747 22.069509506225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919042110443115 2.3758082389831543 25.54998779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791731834411621 2.0361013412475586 22.15274429321289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917437553405762 1.6709914207458496 18.501659393310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927087545394897 2.3029658794403076 24.822368621826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924048900604248 2.5289928913116455 27.082332611083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929573059082031 2.3266899585723877 25.059856414794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926251888275146 2.495547294616699 26.748098373413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910844087600708 2.1391031742095947 23.18211555480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927166223526 2.4869720935821533 26.662437438964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925771474838257 2.3080811500549316 24.873388290405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924174070358276 2.3339948654174805 25.132366180419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79231858253479 2.0015382766723633 21.807701110839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791589617729187 2.0407817363739014 22.19940757751465
  batch 20 loss: 1.791589617729187, 2.0407817363739014, 22.19940757751465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924528121948242 1.9768701791763306 21.561153411865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 1.772029161453247 19.51272964477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922163009643555 2.0052239894866943 21.84445571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927014827728271 2.125640392303467 23.04910659790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923887968063354 2.6002426147460938 27.794815063476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792500615119934 2.0362424850463867 22.154926300048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928656339645386 2.4995946884155273 26.7888126373291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918685674667358 2.1836726665496826 23.62859535217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793325662612915 2.2333502769470215 24.126829147338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915103435516357 2.0150628089904785 21.942136764526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925642728805542 2.477721691131592 26.569782257080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791403889656067 2.407489061355591 25.866294860839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792433738708496 2.220223903656006 23.994674682617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925571203231812 2.1648924350738525 23.44148063659668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925106287002563 2.2655484676361084 24.447996139526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926126718521118 2.1205763816833496 22.998376846313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925337553024292 1.9445610046386719 21.238143920898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.1044700145721436 22.836322784423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921499013900757 2.360309600830078 25.395246505737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.332451105117798 25.116165161132812
  batch 40 loss: 1.791654348373413, 2.332451105117798, 25.116165161132812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792170763015747 2.175729513168335 23.549467086791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923901081085205 1.973154902458191 21.52393913269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792044997215271 2.264073610305786 24.432781219482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791614055633545 2.0533511638641357 22.32512664794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792482852935791 1.586137294769287 17.65385627746582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921644449234009 2.2945749759674072 24.737913131713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920591831207275 2.193923234939575 23.731290817260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919045686721802 2.2340352535247803 24.13225746154785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 2.2409331798553467 24.20093536376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920438051223755 2.1927740573883057 23.719783782958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791069507598877 2.318218946456909 24.97325897216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923521995544434 2.288524627685547 24.67759895324707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792232632637024 2.0795605182647705 22.58783721923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792566180229187 2.2021114826202393 23.81368064880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 2.228292226791382 24.07476234436035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925479412078857 2.2962188720703125 24.754735946655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927122116088867 2.3535380363464355 25.328094482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792690634727478 1.9833483695983887 21.626174926757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931175231933594 2.4731380939483643 26.524497985839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921319007873535 2.2783854007720947 24.575984954833984
  batch 60 loss: 1.7921319007873535, 2.2783854007720947, 24.575984954833984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927343845367432 2.3599774837493896 25.39250946044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791680932044983 2.1927120685577393 23.718801498413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925376892089844 2.0961251258850098 22.753787994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792123556137085 2.344691038131714 25.23903465270996
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926735877990723 1.6074758768081665 17.867431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918622493743896 2.0381274223327637 22.173137664794922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791775107383728 1.9726510047912598 21.51828384399414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915644645690918 1.8064384460449219 19.85594940185547
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918983697891235 1.765952706336975 19.451425552368164
Total LOSS train 23.71229661794809 valid 20.749699115753174
CE LOSS train 1.7922455695959238 valid 0.4479745924472809
Contrastive LOSS train 2.19200510428502 valid 0.4414881765842438
EPOCH 227:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922226190567017 2.1300647258758545 23.092870712280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915403842926025 2.161971092224121 23.411251068115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792468786239624 2.096918821334839 22.76165771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792370319366455 2.200547933578491 23.797849655151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920629978179932 2.1664211750030518 23.456274032592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928667068481445 2.0316884517669678 22.109752655029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791844129562378 2.3240137100219727 25.031980514526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919023036956787 2.1511070728302 23.3029727935791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.1941065788269043 23.73279571533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791742205619812 2.06056547164917 22.397397994995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927067279815674 2.348412036895752 25.276826858520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924033403396606 2.4882218837738037 26.674623489379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929561138153076 2.180386781692505 23.59682273864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926247119903564 2.4592299461364746 26.384925842285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910841703414917 2.4067037105560303 25.858121871948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927156686782837 2.4988701343536377 26.781415939331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925759553909302 2.2224972248077393 24.017547607421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924164533615112 2.081739902496338 22.609813690185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923176288604736 1.7343167066574097 19.13548469543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915890216827393 1.984575867652893 21.637348175048828
  batch 20 loss: 1.7915890216827393, 1.984575867652893, 21.637348175048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924515008926392 2.1081039905548096 22.873491287231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924381494522095 2.1332943439483643 23.125381469726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922149896621704 2.138350248336792 23.175718307495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792700171470642 2.2300808429718018 24.093507766723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923882007598877 2.355743885040283 25.34982681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925000190734863 2.414442777633667 25.936927795410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928662300109863 2.229893207550049 24.091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918699979782104 2.0290331840515137 22.082202911376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933260202407837 2.241443395614624 24.207759857177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915116548538208 2.112948179244995 22.92099380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925634384155273 2.309175729751587 24.884319305419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914040088653564 1.9704859256744385 21.49626350402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924329042434692 2.2851991653442383 24.644424438476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925556898117065 2.151228427886963 23.304838180541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925090789794922 2.166396379470825 23.456472396850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926112413406372 2.2026591300964355 23.819204330444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925323247909546 2.1208744049072266 23.00127601623535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791621446609497 2.0182759761810303 21.974382400512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921488285064697 2.222517251968384 24.01732063293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916536331176758 2.0563294887542725 22.354949951171875
  batch 40 loss: 1.7916536331176758, 2.0563294887542725, 22.354949951171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921701669692993 2.2028729915618896 23.820899963378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923895120620728 2.036651849746704 22.158906936645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920440435409546 2.2559361457824707 24.351404190063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791613221168518 2.1383557319641113 23.175168991088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792481541633606 2.237123966217041 24.163721084594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921634912490845 2.339272975921631 25.184894561767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920591831207275 2.1262190341949463 23.054248809814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919052839279175 2.0920629501342773 22.712533950805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.340117931365967 25.192785263061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920430898666382 2.107029676437378 22.862340927124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910693883895874 2.2727341651916504 24.518409729003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792351245880127 2.248684883117676 24.279199600219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792231559753418 2.03751540184021 22.16738510131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925645112991333 2.2157161235809326 23.949726104736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918384075164795 2.2855751514434814 24.6475887298584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925471067428589 2.038257360458374 22.175121307373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711853981018 2.2884104251861572 24.676815032958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926902770996094 2.1874632835388184 23.66732406616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931169271469116 2.4933924674987793 26.72704315185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921316623687744 2.1045634746551514 22.837766647338867
  batch 60 loss: 1.7921316623687744, 2.1045634746551514, 22.837766647338867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927333116531372 2.1207919120788574 23.000654220581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916805744171143 2.0117099285125732 21.908781051635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 2.1500890254974365 23.293428421020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921242713928223 2.170788288116455 23.5000057220459
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926733493804932 1.326761245727539 15.060285568237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 2.3100674152374268 24.892536163330078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917757034301758 2.213742971420288 23.92920684814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915650606155396 2.085280179977417 22.644367218017578
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918989658355713 1.8056507110595703 19.848405838012695
Total LOSS train 23.522542014488806 valid 22.82862901687622
CE LOSS train 1.792244845170241 valid 0.4479747414588928
Contrastive LOSS train 2.173029716198261 valid 0.4514126777648926
EPOCH 228:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922230958938599 2.3039708137512207 24.83193016052246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791540265083313 2.5362164974212646 27.153705596923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792466640472412 2.1862845420837402 23.65531349182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923681735992432 2.183180809020996 23.624176025390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.2713024616241455 24.50508689880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792865514755249 2.0651118755340576 22.443984985351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918434143066406 1.9447920322418213 21.239763259887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 1.9385188817977905 21.177091598510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.147880792617798 23.270536422729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917426824569702 2.2006375789642334 23.798118591308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927058935165405 2.4331281185150146 26.123987197875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924033403396606 2.4385273456573486 26.177677154541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792955994606018 1.7104761600494385 18.897716522216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926249504089355 1.841342806816101 20.206052780151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 2.268643379211426 24.477519989013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927148342132568 2.4612514972686768 26.405229568481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925755977630615 2.2090978622436523 23.883554458618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924165725708008 2.2801272869110107 24.59368896484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923179864883423 2.0325403213500977 22.117721557617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791589617729187 2.132614850997925 23.117738723754883
  batch 20 loss: 1.791589617729187, 2.132614850997925, 23.117738723754883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924518585205078 2.140763521194458 23.20008659362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924385070800781 2.2188804149627686 23.981243133544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922145128250122 2.050132989883423 22.29354476928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926998138427734 2.107290506362915 22.865604400634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923880815505981 2.396336317062378 25.755752563476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924995422363281 2.3122925758361816 24.915424346923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928656339645386 2.472113609313965 26.514001846313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918701171875 1.882348656654358 20.6153564453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933249473571777 2.3661227226257324 25.454553604125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915126085281372 2.0950231552124023 22.741744995117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925639152526855 2.469731092453003 26.48987579345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914046049118042 2.152658700942993 23.317991256713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924326658248901 2.124269962310791 23.035131454467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792555570602417 2.3953745365142822 25.746299743652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792508840560913 2.510645627975464 26.89896583557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792610764503479 2.489135265350342 26.683963775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925317287445068 2.10516619682312 22.844194412231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.2547714710235596 24.339336395263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921494245529175 2.3898918628692627 25.69106674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.388882637023926 25.68048095703125
  batch 40 loss: 1.791654109954834, 2.388882637023926, 25.68048095703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921698093414307 2.2410836219787598 24.203004837036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792388677597046 2.0976576805114746 22.768966674804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920430898666382 2.1131346225738525 22.923389434814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916128635406494 1.7468935251235962 19.260547637939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480707168579 2.2237985134124756 24.030466079711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921630144119263 2.284712314605713 24.639286041259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920588254928589 2.1129720211029053 22.92177963256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919056415557861 2.0277211666107178 22.06911849975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 1.9591468572616577 21.383071899414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792042851448059 1.7651712894439697 19.443756103515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910701036453247 2.307957649230957 24.870647430419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923506498336792 2.2534019947052 24.326370239257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922314405441284 2.0778279304504395 22.57050895690918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925643920898438 2.131938934326172 23.111953735351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.4797399044036865 26.5892391204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792547345161438 2.1071417331695557 22.863964080810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927113771438599 2.117065191268921 22.963363647460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926905155181885 1.7809760570526123 19.60245132446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931169271469116 2.4997572898864746 26.790691375732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921324968338013 2.057913064956665 22.37126350402832
  batch 60 loss: 1.7921324968338013, 2.057913064956665, 22.37126350402832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927333116531372 2.1243581771850586 23.03631591796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916816473007202 2.0535261631011963 22.326942443847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925370931625366 2.1811814308166504 23.604351043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792124629020691 2.2329301834106445 24.12142562866211
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926735877990723 1.766884446144104 19.461517333984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 2.283797025680542 24.629833221435547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791776180267334 2.0987048149108887 22.778825759887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915657758712769 1.9880259037017822 21.671823501586914
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.79189932346344 2.0252110958099365 22.044010162353516
Total LOSS train 23.584917009793795 valid 22.781123161315918
CE LOSS train 1.7922447296289297 valid 0.44797483086586
Contrastive LOSS train 2.179267223064716 valid 0.5063027739524841
EPOCH 229:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792223334312439 2.1648881435394287 23.441104888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791541576385498 2.4294919967651367 26.086462020874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924671173095703 2.0151476860046387 21.943944931030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923688888549805 2.06296968460083 22.42206573486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920633554458618 2.2201075553894043 23.993139266967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928653955459595 2.1346778869628906 23.139644622802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918438911437988 2.499218225479126 26.784025192260742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 2.184389591217041 23.63579750061035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 1.985007405281067 21.64180564880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917423248291016 2.1254191398620605 23.045934677124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927052974700928 2.2478103637695312 24.270809173583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792402744293213 2.569591760635376 27.488319396972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929542064666748 2.365198850631714 25.444942474365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926232814788818 2.161884069442749 23.41146469116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 1.9997512102127075 21.788599014282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79271399974823 2.271402597427368 24.506738662719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925748825073242 2.2459518909454346 24.252094268798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924153804779053 2.3281397819519043 25.073814392089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923166751861572 2.1526472568511963 23.318788528442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915890216827393 2.177051305770874 23.562103271484375
  batch 20 loss: 1.7915890216827393, 2.177051305770874, 23.562103271484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924509048461914 1.9340367317199707 21.132816314697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.3173909187316895 24.966346740722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922147512435913 1.9379310607910156 21.171525955200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926993370056152 2.193936824798584 23.73206901550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923872470855713 2.4688940048217773 26.481327056884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924989461898804 2.1780450344085693 23.57295036315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792864203453064 2.3839805126190186 25.63266944885254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918696403503418 2.1490018367767334 23.281888961791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933236360549927 2.4644124507904053 26.437448501586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915122509002686 2.1688385009765625 23.479896545410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925628423690796 2.579725503921509 27.58981704711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914047241210938 2.288170337677002 24.673107147216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792432188987732 2.0693459510803223 22.485891342163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925550937652588 2.100346803665161 22.796022415161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925089597702026 2.282353639602661 24.616044998168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926102876663208 2.3560667037963867 25.3532772064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925316095352173 2.234767436981201 24.140207290649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.279995918273926 24.591581344604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921487092971802 2.110599994659424 22.898147583007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.2377769947052 24.169424057006836
  batch 40 loss: 1.7916538715362549, 2.2377769947052, 24.169424057006836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921696901321411 2.0687901973724365 22.480072021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923885583877563 1.9796030521392822 21.58841896057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920434474945068 2.017925500869751 21.971298217773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916136980056763 2.177475929260254 23.566373825073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924810647964478 2.059279441833496 22.38527488708496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921631336212158 2.301666021347046 24.80882453918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920588254928589 1.9262888431549072 21.054946899414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919058799743652 2.0830795764923096 22.62270164489746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916045188903809 2.2577624320983887 24.369230270385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920430898666382 1.9540892839431763 21.332937240600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910711765289307 2.023176908493042 22.02284049987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923506498336792 2.40789532661438 25.87130355834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922310829162598 2.1663458347320557 23.4556884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925639152526855 2.0670900344848633 22.463464736938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 2.3426475524902344 25.21831512451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925467491149902 2.0687594413757324 22.48014259338379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927109003067017 2.43418550491333 26.134565353393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926898002624512 2.222325086593628 24.015941619873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931162118911743 2.5045838356018066 26.838953018188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921326160430908 2.3221843242645264 25.01397705078125
  batch 60 loss: 1.7921326160430908, 2.3221843242645264, 25.01397705078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927335500717163 2.249526023864746 24.287994384765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916820049285889 2.1269993782043457 23.061674118041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925366163253784 2.089398145675659 22.6865177154541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921234369277954 2.010169267654419 21.893815994262695
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926719188690186 1.666465163230896 18.45732307434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 2.3824965953826904 25.61682891845703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917755842208862 2.2919790744781494 24.711565017700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915655374526978 2.0374581813812256 22.166147232055664
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918989658355713 1.9886008501052856 21.677907943725586
Total LOSS train 23.731333131056566 valid 23.54311227798462
CE LOSS train 1.7922444673684927 valid 0.4479747414588928
Contrastive LOSS train 2.1939088564652662 valid 0.4971502125263214
EPOCH 230:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922226190567017 2.243607521057129 24.22829818725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915421724319458 2.5608437061309814 27.399978637695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924668788909912 2.153893232345581 23.33139991760254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923682928085327 2.176844596862793 23.560813903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920624017715454 2.2170956134796143 23.9630184173584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792864441871643 2.111201286315918 22.904876708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918438911437988 2.4749093055725098 26.540935516357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 1.9849660396575928 21.641563415527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.1780786514282227 23.57251739501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917429208755493 2.1272521018981934 23.06426429748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927051782608032 2.4808812141418457 26.601516723632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792402744293213 2.412647247314453 25.918874740600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929538488388062 2.3078362941741943 24.87131690979004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926231622695923 2.525139093399048 27.04401397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910865545272827 2.183978319168091 23.630868911743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 2.2454841136932373 24.247554779052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925746440887451 2.24324893951416 24.22506332397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924158573150635 2.31227445602417 24.9151611328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923176288604736 1.9411112070083618 21.20343017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915904521942139 2.222425937652588 24.01584815979004
  batch 20 loss: 1.7915904521942139, 2.222425937652588, 24.01584815979004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924515008926392 2.131375551223755 23.1062068939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79243803024292 1.8002833127975464 19.795270919799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922152280807495 2.0093719959259033 21.885934829711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926989793777466 2.1197376251220703 22.990076065063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923870086669922 2.3899357318878174 25.691743850708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924988269805908 2.2495522499084473 24.288021087646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928637266159058 2.197287082672119 23.76573371887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918697595596313 2.2373509407043457 24.16537857055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933228015899658 2.124598264694214 23.039306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915127277374268 2.1788482666015625 23.57999610900879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792562484741211 2.320617437362671 24.998737335205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914049625396729 2.3319571018218994 25.11097526550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924317121505737 2.1665332317352295 23.457763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792554497718811 2.2484893798828125 24.277448654174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925081253051758 2.3705620765686035 25.498126983642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 1.9977222681045532 21.769832611083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 1.9556318521499634 21.348848342895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916215658187866 2.257086992263794 24.362491607666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921483516693115 2.3506956100463867 25.299104690551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.274925470352173 24.540908813476562
  batch 40 loss: 1.7916538715362549, 2.274925470352173, 24.540908813476562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921695709228516 2.102341413497925 22.815584182739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792388916015625 1.5287243127822876 17.079631805419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792043924331665 2.363651752471924 25.428560256958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916141748428345 2.204843044281006 23.840045928955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924810647964478 2.2687318325042725 24.479799270629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921639680862427 2.1919782161712646 23.711946487426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920598983764648 1.93898344039917 21.181896209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919068336486816 1.9615148305892944 21.407054901123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916053533554077 2.1872622966766357 23.664228439331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920433282852173 1.9464293718338013 21.256338119506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910722494125366 2.168099880218506 23.47207260131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923507690429688 2.3165292739868164 24.957643508911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922314405441284 2.0165822505950928 21.958053588867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792563796043396 2.1080994606018066 22.873558044433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 2.5161428451538086 26.95326805114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925469875335693 2.158468008041382 23.377227783203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927106618881226 2.325096368789673 25.04367446899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926900386810303 2.144644021987915 23.2391300201416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931156158447266 2.2565457820892334 24.35857391357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921324968338013 2.3164968490600586 24.957101821899414
  batch 60 loss: 1.7921324968338013, 2.3164968490600586, 24.957101821899414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927329540252686 2.2787528038024902 24.58026123046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916829586029053 2.2689766883850098 24.481449127197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925363779067993 2.156446933746338 23.357004165649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921236753463745 2.216473340988159 23.956857681274414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926719188690186 1.8131438493728638 19.924110412597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918621301651 2.3480470180511475 25.2723331451416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917747497558594 2.2184765338897705 23.976539611816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79156494140625 2.011157274246216 21.90313720703125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791898250579834 1.8795191049575806 20.58708953857422
Total LOSS train 23.726281444843 valid 22.93477487564087
CE LOSS train 1.7922445150522086 valid 0.4479745626449585
Contrastive LOSS train 2.193403695179866 valid 0.46987977623939514
EPOCH 231:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922216653823853 2.3191869258880615 24.98409080505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915409803390503 2.533557891845703 27.127119064331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924649715423584 2.011599540710449 21.90846061706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923669815063477 2.165297508239746 23.445343017578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920615673065186 2.2667081356048584 24.459142684936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928630113601685 2.288062572479248 24.673490524291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918431758880615 2.313014030456543 24.92198371887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919018268585205 2.047031879425049 22.26222038269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.183311700820923 23.624845504760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917417287826538 1.9816855192184448 21.608596801757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704701423645 2.3397369384765625 25.190074920654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924026250839233 2.4064571857452393 25.85697364807129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929521799087524 2.2270963191986084 24.063915252685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926217317581177 2.2981607913970947 24.774229049682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910878658294678 2.3020896911621094 24.81198501586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927136421203613 2.262389898300171 24.41661262512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925751209259033 2.2533867359161377 24.32644271850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924156188964844 2.2639474868774414 24.4318904876709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923176288604736 1.716017246246338 18.952489852905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915908098220825 2.3518333435058594 25.309925079345703
  batch 20 loss: 1.7915908098220825, 2.3518333435058594, 25.309925079345703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924507856369019 1.9033141136169434 20.825592041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.1875386238098145 23.667823791503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922148704528809 1.9908907413482666 21.701122283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926990985870361 2.1421546936035156 23.21424674987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923879623413086 2.305145025253296 24.84383773803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924991846084595 2.2534687519073486 24.327186584472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928627729415894 2.478705406188965 26.57991600036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918692827224731 2.2191762924194336 23.983633041381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793320655822754 2.346663236618042 25.259952545166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915124893188477 1.9896761178970337 21.688274383544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925605773925781 2.123579502105713 23.02835464477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914048433303833 2.102003574371338 22.811439514160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924305200576782 2.2181098461151123 23.973529815673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925529479980469 2.1976826190948486 23.769378662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925066947937012 2.273801565170288 24.5305233001709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926080226898193 2.3157668113708496 24.95027732849121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792528748512268 2.242370128631592 24.216230392456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916210889816284 2.2308881282806396 24.100502014160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792147159576416 2.4631552696228027 26.4237003326416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916531562805176 2.177401065826416 23.565662384033203
  batch 40 loss: 1.7916531562805176, 2.177401065826416, 23.565662384033203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921687364578247 2.175894021987915 23.551109313964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923870086669922 2.0510833263397217 22.303220748901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204261302948 2.4086155891418457 25.878196716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916133403778076 2.2805373668670654 24.596986770629883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792479157447815 2.1617178916931152 23.409658432006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921619415283203 2.3058996200561523 24.851158142089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920582294464111 2.1126322746276855 22.91838264465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919049263000488 2.3274810314178467 25.066715240478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 1.9430474042892456 21.222078323364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920423746109009 2.0072481632232666 21.86452293395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910716533660889 2.347313165664673 25.264202117919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923492193222046 2.287200927734375 24.664358139038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922297716140747 2.1102004051208496 22.894235610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925618886947632 2.116713762283325 22.959699630737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918378114700317 2.4333131313323975 26.124969482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792544960975647 2.116760730743408 22.96015167236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927086353302002 2.281054973602295 24.603260040283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926876544952393 2.195852756500244 23.7512149810791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931135892868042 2.36177921295166 25.410905838012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921314239501953 2.223362684249878 24.025758743286133
  batch 60 loss: 1.7921314239501953, 2.223362684249878, 24.025758743286133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927310466766357 2.2870595455169678 24.663326263427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916814088821411 2.296466588973999 24.75634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925339937210083 2.0287413597106934 22.07994842529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921218872070312 2.0828981399536133 22.621103286743164
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792669415473938 1.591520071029663 17.707870483398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918621301651 2.2891812324523926 24.683673858642578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774868965149 2.2700703144073486 24.492477416992188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915650606155396 1.9116250276565552 20.90781593322754
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918977737426758 1.9610741138458252 21.402637481689453
Total LOSS train 23.82708303011381 valid 22.87165117263794
CE LOSS train 1.7922434329986572 valid 0.44797444343566895
Contrastive LOSS train 2.20348395384275 valid 0.4902685284614563
EPOCH 232:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792221188545227 2.267972469329834 24.471946716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915416955947876 2.126193046569824 23.0534725189209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792465329170227 1.6627408266067505 18.41987419128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923665046691895 1.8836822509765625 20.629188537597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920613288879395 1.807148814201355 19.863548278808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928622961044312 2.2931272983551025 24.72413444519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918431758880615 2.3970415592193604 25.762258529663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919014692306519 1.9878672361373901 21.67057228088379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917295694351196 1.9911043643951416 21.70277214050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917417287826538 2.1446597576141357 23.238340377807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927031517028809 1.9895398616790771 21.68810272216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924011945724487 2.4947845935821533 26.740245819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929517030715942 2.1055147647857666 22.848098754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926217317581177 2.237217426300049 24.164794921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910877466201782 2.2928807735443115 24.71989631652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792711853981018 2.4461159706115723 26.253870010375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925728559494019 2.2591004371643066 24.383575439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792413353919983 2.127608060836792 23.06849479675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923147678375244 1.678117036819458 18.573484420776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915891408920288 1.9958207607269287 21.749797821044922
  batch 20 loss: 1.7915891408920288, 1.9958207607269287, 21.749797821044922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924482822418213 1.9518932104110718 21.31138038635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924354076385498 2.1262757778167725 23.055192947387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922128438949585 2.0415666103363037 22.2078800201416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926965951919556 2.2206826210021973 23.999521255493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923855781555176 2.2636191844940186 24.428577423095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792496681213379 2.2427480220794678 24.21997833251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928613424301147 2.222660779953003 24.01947021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791869044303894 2.1871531009674072 23.66339874267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793319582939148 2.393409490585327 25.727415084838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915128469467163 2.3905210494995117 25.69672393798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925602197647095 2.4712753295898438 26.505313873291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791405200958252 2.273479461669922 24.526199340820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924304008483887 2.2574665546417236 24.367095947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925528287887573 2.1777937412261963 23.57048988342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792506456375122 2.3508551120758057 25.301057815551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926080226898193 2.253648281097412 24.329092025756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925292253494263 2.3664562702178955 25.45709228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.1226999759674072 23.01862144470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921476364135742 2.2969138622283936 24.761287689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.2599332332611084 24.390987396240234
  batch 40 loss: 1.7916538715362549, 2.2599332332611084, 24.390987396240234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921676635742188 2.3542399406433105 25.33456802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792386531829834 1.8977489471435547 20.76987648010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920417785644531 2.419590473175049 25.987945556640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791613221168518 1.8054425716400146 19.846038818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792478322982788 1.7983850240707397 19.776329040527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921606302261353 2.2079198360443115 23.87135887145996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920570373535156 2.1581802368164062 23.373859405517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919042110443115 2.032989978790283 22.121803283691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603446006775 2.2755727767944336 24.547330856323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792041540145874 2.1119165420532227 22.91120719909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910716533660889 2.3398566246032715 25.189638137817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923485040664673 2.170551061630249 23.497859954833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922290563583374 2.022968292236328 22.02191162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925611734390259 2.122479200363159 23.017353057861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.351720094680786 25.309038162231445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925437688827515 2.060511589050293 22.397659301757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927073240280151 2.3388254642486572 25.18096160888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792686939239502 2.188037633895874 23.673063278198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931123971939087 2.586219310760498 27.655305862426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921311855316162 2.3579657077789307 25.371788024902344
  batch 60 loss: 1.7921311855316162, 2.3579657077789307, 25.371788024902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927305698394775 2.082359790802002 22.6163272857666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916817665100098 2.128845691680908 23.080137252807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925342321395874 2.0765671730041504 22.558204650878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921222448349 2.3646011352539062 25.438133239746094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926689386367798 1.7391083240509033 19.183752059936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 2.0926432609558105 22.71829605102539
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917752265930176 2.077995538711548 22.57172966003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915656566619873 1.8667227029800415 20.458791732788086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918983697891235 1.6943036317825317 18.734935760498047
Total LOSS train 23.492533786480244 valid 21.120938301086426
CE LOSS train 1.7922427379167998 valid 0.4479745924472809
Contrastive LOSS train 2.1700291138428907 valid 0.42357590794563293
EPOCH 233:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922213077545166 2.2066738605499268 23.858959197998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915418148040771 2.576529026031494 27.55683135986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924643754959106 2.102882146835327 22.821287155151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366623878479 2.2372467517852783 24.164833068847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920615673065186 2.022054433822632 22.012605667114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928621768951416 2.246044158935547 24.25330352783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918440103530884 2.3647947311401367 25.439790725708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919026613235474 1.751322627067566 19.30512809753418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730523109436 1.9281659126281738 21.073389053344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917423248291016 1.9400746822357178 21.192489624023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927024364471436 2.288198232650757 24.674684524536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924015522003174 2.557206630706787 27.364469528198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929503917694092 2.1510703563690186 23.303653717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926207780838013 2.2493250370025635 24.285871505737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910876274108887 2.2636842727661133 24.42793083190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927104234695435 2.193633556365967 23.729047775268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925714254379272 2.0920321941375732 22.712894439697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924127578735352 2.2376272678375244 24.168685913085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792314887046814 2.099475145339966 22.787065505981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915898561477661 1.729593276977539 19.087522506713867
  batch 20 loss: 1.7915898561477661, 1.729593276977539, 19.087522506713867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924482822418213 2.0133578777313232 21.926027297973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924354076385498 1.8743195533752441 20.535629272460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922126054763794 1.9805799722671509 21.598012924194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926957607269287 2.1328518390655518 23.121213912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923851013183594 2.467519998550415 26.46758460998535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924962043762207 2.3904144763946533 25.696640014648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928602695465088 2.198115587234497 23.774015426635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918689250946045 2.017538547515869 21.967252731323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793318748474121 2.332672595977783 25.120044708251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915133237838745 2.175894260406494 23.55045509338379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79256010055542 2.375288963317871 25.54545021057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914057970046997 2.3109536170959473 24.900941848754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79243004322052 2.207746982574463 23.86989974975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79255211353302 2.1802468299865723 23.595020294189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 2.2023303508758545 23.81580924987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926077842712402 2.258578300476074 24.37839126586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925292253494263 2.165722131729126 23.449750900268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791622281074524 2.2409918308258057 24.201539993286133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921477556228638 2.278332233428955 24.575469970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.2824559211730957 24.616212844848633
  batch 40 loss: 1.791654348373413, 2.2824559211730957, 24.616212844848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921688556671143 2.2289109230041504 24.08127784729004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792386770248413 1.950544834136963 21.297834396362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920427322387695 2.060572862625122 22.397769927978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916145324707031 1.8071643114089966 19.863258361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924787998199463 2.098707914352417 22.779558181762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792161226272583 2.1659963130950928 23.452125549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920576333999634 1.9439972639083862 21.23202896118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791904330253601 1.943360686302185 21.22551155090332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916042804718018 2.2776427268981934 24.56803321838379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920417785644531 2.0799174308776855 22.591217041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910726070404053 2.347452402114868 25.265596389770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792348027229309 2.3704025745391846 25.496374130249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922298908233643 2.119025468826294 22.982484817504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792561650276184 2.2678749561309814 24.471311569213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918386459350586 2.3436708450317383 25.228546142578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792543888092041 2.211388111114502 23.906423568725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927072048187256 2.3221561908721924 25.01426887512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926862239837646 2.22137188911438 24.006404876708984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793110966682434 2.314540386199951 24.93851661682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792130947113037 2.3574934005737305 25.3670654296875
  batch 60 loss: 1.792130947113037, 2.3574934005737305, 25.3670654296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927294969558716 2.088805675506592 22.680788040161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916826009750366 2.06581711769104 22.449853897094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925336360931396 2.0993127822875977 22.785661697387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921220064163208 2.2069618701934814 23.861740112304688
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926688194274902 1.7476675510406494 19.269344329833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 2.091201066970825 22.703872680664062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917755842208862 1.997009515762329 21.761869430541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915661334991455 1.8762081861495972 20.553647994995117
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791898488998413 1.7482905387878418 19.274805068969727
Total LOSS train 23.479058485764725 valid 21.073548793792725
CE LOSS train 1.7922427269128653 valid 0.44797462224960327
Contrastive LOSS train 2.168681579369765 valid 0.43707263469696045
EPOCH 234:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922214269638062 2.1628611087799072 23.42083168029785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915425300598145 2.3870291709899902 25.661834716796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924638986587524 2.192701816558838 23.719480514526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923661470413208 1.9797919988632202 21.590286254882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79206120967865 1.775214672088623 19.544208526611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792860984802246 2.1357035636901855 23.149898529052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791843295097351 2.3168935775756836 24.960779190063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 2.0732035636901855 22.52393913269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917308807373047 2.2715489864349365 24.507221221923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917426824569702 1.9511802196502686 21.303544998168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702317237854 2.351428985595703 25.306991577148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924009561538696 2.3302218914031982 25.094619750976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929497957229614 2.1491923332214355 23.284873962402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926206588745117 2.125389575958252 23.04651641845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910900115966797 2.3368289470672607 25.159379959106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927113771438599 2.3847556114196777 25.640268325805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925732135772705 2.226426839828491 24.056842803955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924132347106934 2.2808644771575928 24.601058959960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923156023025513 1.9017759561538696 20.810075759887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915911674499512 2.101353406906128 22.805126190185547
  batch 20 loss: 1.7915911674499512, 2.101353406906128, 22.805126190185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924489974975586 1.6980817317962646 18.773265838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924360036849976 1.8496705293655396 20.289142608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922134399414062 2.120955228805542 23.001766204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926961183547974 2.099940538406372 22.79210090637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385220527649 2.3140828609466553 24.93321418762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924963235855103 2.2265236377716064 24.05773162841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928602695465088 2.410109758377075 25.893957138061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918697595596313 1.783182978630066 19.623699188232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933175563812256 2.037257194519043 22.165889739990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915143966674805 2.088768243789673 22.679195404052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925599813461304 2.4650659561157227 26.443220138549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914063930511475 2.3246824741363525 25.038230895996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792427897453308 2.045447826385498 22.246906280517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925500869750977 2.343196392059326 25.22451400756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925041913986206 2.420379400253296 25.99629783630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926055192947388 2.4213972091674805 26.00657844543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792528510093689 2.2920048236846924 24.712575912475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916227579116821 2.1744725704193115 23.536348342895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921476364135742 2.2239999771118164 24.032146453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.322155237197876 25.013206481933594
  batch 40 loss: 1.791654348373413, 2.322155237197876, 25.013206481933594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921669483184814 2.1137218475341797 22.929386138916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923848628997803 2.1511075496673584 23.3034610748291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920408248901367 2.340212821960449 25.194168090820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916145324707031 2.081068754196167 22.60230255126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924782037734985 2.0447425842285156 22.239904403686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792159914970398 2.2642247676849365 24.43440818786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920564413070679 2.207202434539795 23.86408233642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919034957885742 2.0528719425201416 22.320621490478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.355994701385498 25.351551055908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792040467262268 2.256103038787842 24.353071212768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910737991333008 2.4639248847961426 26.430320739746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792348027229309 2.3413712978363037 25.20606231689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792228102684021 2.070756196975708 22.49979019165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792559266090393 2.144681692123413 23.239376068115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918368577957153 2.4370110034942627 26.16194725036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925420999526978 2.289564609527588 24.688186645507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927051782608032 2.5900940895080566 27.693645477294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926840782165527 2.1735870838165283 23.528554916381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931089401245117 2.5729994773864746 27.52310562133789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792129635810852 2.3712196350097656 25.50432586669922
  batch 60 loss: 1.792129635810852, 2.3712196350097656, 25.50432586669922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927274703979492 2.220268964767456 23.995418548583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916812896728516 2.239584445953369 24.187524795532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925323247909546 2.006869077682495 21.861223220825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921206951141357 1.8790003061294556 20.582122802734375
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926666736602783 1.591575264930725 17.708419799804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918611764907837 2.099428653717041 22.78614616394043
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.029644012451172 22.088214874267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79156494140625 1.8792616128921509 20.58418083190918
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918968200683594 1.8895410299301147 20.687307357788086
Total LOSS train 23.69308841411884 valid 21.53646230697632
CE LOSS train 1.7922421932220458 valid 0.44797420501708984
Contrastive LOSS train 2.190084611452543 valid 0.4723852574825287
EPOCH 235:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922192811965942 2.1441731452941895 23.233949661254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915412187576294 2.2038052082061768 23.829593658447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924625873565674 2.1227996349334717 23.02046012878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923650741577148 2.306265354156494 24.855018615722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920608520507812 2.2570862770080566 24.36292266845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928597927093506 2.286982536315918 24.66268539428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918434143066406 2.268282175064087 24.47466468811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919024229049683 1.8758589029312134 20.550491333007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.225881814956665 24.050548553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917423248291016 2.197861909866333 23.770360946655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792701244354248 2.393866539001465 25.731367111206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924004793167114 2.362098455429077 25.41338539123535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929484844207764 2.2347400188446045 24.140348434448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792619228363037 2.3594138622283936 25.38675880432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791089415550232 2.230503559112549 24.09612464904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927089929580688 2.432199239730835 26.114702224731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925711870193481 2.3695287704467773 25.48785972595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 2.211317777633667 23.905590057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923141717910767 2.0732192993164062 22.524507522583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915904521942139 2.0707318782806396 22.49890899658203
  batch 20 loss: 1.7915904521942139, 2.0707318782806396, 22.49890899658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924469709396362 1.9482524394989014 21.27497100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924336194992065 2.2422080039978027 24.214513778686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922114133834839 1.9885427951812744 21.67763900756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926942110061646 2.114178419113159 22.934478759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792384147644043 2.594588279724121 27.738265991210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924950122833252 2.1510508060455322 23.303003311157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792858600616455 2.332789659500122 25.12075424194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918676137924194 2.22623348236084 24.054203033447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933144569396973 2.3619773387908936 25.413087844848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915130853652954 1.910062313079834 20.89213752746582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925574779510498 2.294170379638672 24.73426055908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914056777954102 2.1811461448669434 23.602867126464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924275398254395 2.2495601177215576 24.288028717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925503253936768 2.283618450164795 24.62873649597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925046682357788 2.29709529876709 24.763458251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792604923248291 2.250325918197632 24.29586410522461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925267219543457 2.4730615615844727 26.523141860961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916220426559448 2.0296216011047363 22.08783721923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921464443206787 2.225836753845215 24.050514221191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.2613744735717773 24.405399322509766
  batch 40 loss: 1.7916538715362549, 2.2613744735717773, 24.405399322509766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921669483184814 2.2384722232818604 24.176889419555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923851013183594 2.138646125793457 23.17884635925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920410633087158 2.2262473106384277 24.054515838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791614294052124 2.3145713806152344 24.937328338623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792477011680603 2.07926607131958 22.58513641357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921597957611084 2.2976882457733154 24.76904296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792056679725647 1.7684930562973022 19.476987838745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919038534164429 2.156259298324585 23.3544979095459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916041612625122 2.1531174182891846 23.322778701782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920401096343994 2.2073326110839844 23.865365982055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791073203086853 2.225975751876831 24.050830841064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923463582992554 2.2836217880249023 24.628564834594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922279834747314 2.234546184539795 24.137691497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925589084625244 2.316859245300293 24.961151123046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918373346328735 2.193678617477417 23.72862434387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925422191619873 1.9403427839279175 21.195968627929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704701423645 2.3473520278930664 25.266225814819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.143932580947876 23.23200798034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931073904037476 2.3016316890716553 24.809425354003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921288013458252 2.1050875186920166 22.84300422668457
  batch 60 loss: 1.7921288013458252, 2.1050875186920166, 22.84300422668457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792726755142212 2.150606870651245 23.298795700073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916818857192993 2.0545871257781982 22.337553024291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792531967163086 2.30843186378479 24.876850128173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921209335327148 2.2997331619262695 24.789451599121094
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926664352416992 1.7746269702911377 19.538936614990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918622493743896 2.228389263153076 24.075756072998047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917753458023071 2.153265953063965 23.324434280395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791566252708435 1.8854175806045532 20.645742416381836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791898250579834 1.8646243810653687 20.438142776489258
Total LOSS train 23.838921268169695 valid 22.121018886566162
CE LOSS train 1.7922413715949426 valid 0.4479745626449585
Contrastive LOSS train 2.2046679771863498 valid 0.46615609526634216
EPOCH 236:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922203540802002 2.229011058807373 24.082332611083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915425300598145 2.6252899169921875 28.04444122314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924623489379883 2.2133989334106445 23.92645263671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923647165298462 2.04093337059021 22.201698303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920609712600708 2.1881980895996094 23.674041748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928588390350342 2.1961824893951416 23.754682540893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791843056678772 2.2239646911621094 24.031490325927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791901707649231 2.0585432052612305 22.377334594726562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917295694351196 2.1859536170959473 23.651264190673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917416095733643 2.0680081844329834 22.471824645996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926993370056152 2.399958610534668 25.792285919189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923990488052368 2.3693184852600098 25.48558235168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929461002349854 2.2649030685424805 24.44197654724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926174402236938 2.2722649574279785 24.51526641845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910888195037842 2.132357120513916 23.11465835571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 2.425020933151245 26.042917251586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925693988800049 2.249671459197998 24.28928565979004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924100160598755 2.329404592514038 25.086456298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923132181167603 2.0805411338806152 22.59772491455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915898561477661 2.2188045978546143 23.97963523864746
  batch 20 loss: 1.7915898561477661, 2.2188045978546143, 23.97963523864746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792445421218872 2.1112725734710693 22.90517234802246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792432188987732 2.302675724029541 24.819189071655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922104597091675 2.0538246631622314 22.330455780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79269278049469 2.2362873554229736 24.15556526184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923831939697266 2.1803746223449707 23.596128463745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792494297027588 2.2347426414489746 24.139921188354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928574085235596 2.1127707958221436 22.92056655883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918685674667358 1.6671725511550903 18.463594436645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933138608932495 2.0667243003845215 22.46055793762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915139198303223 1.8759102821350098 20.550615310668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925573587417603 2.48553466796875 26.647903442382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791406273841858 1.8396029472351074 20.187437057495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924275398254395 2.178778886795044 23.580215454101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254949092865 2.285999298095703 24.652542114257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925037145614624 2.411541700363159 25.907920837402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926045656204224 2.349987745285034 25.292482376098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792526364326477 2.2910799980163574 24.703327178955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791622519493103 2.215857744216919 23.950199127197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921465635299683 2.058960437774658 22.381750106811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916539907455444 2.3532190322875977 25.32384490966797
  batch 40 loss: 1.7916539907455444, 2.3532190322875977, 25.32384490966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79216730594635 2.3490841388702393 25.283008575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923845052719116 1.9861252307891846 21.653636932373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792041301727295 2.3826332092285156 25.61837387084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916145324707031 2.0778748989105225 22.570363998413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792475938796997 2.040010929107666 22.192584991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921595573425293 2.2027018070220947 23.819177627563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920564413070679 2.0941739082336426 22.733795166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791904091835022 2.204345226287842 23.835357666015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916038036346436 2.1558825969696045 23.35042953491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920403480529785 2.0023224353790283 21.815263748168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910743951797485 2.3428232669830322 25.21930694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923463582992554 2.3566770553588867 25.35911750793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922282218933105 2.038907051086426 22.181299209594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925589084625244 2.1696674823760986 23.489233016967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79183828830719 2.249465227127075 24.286489486694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925419807434082 2.2442309856414795 24.234851837158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704701423645 2.405712127685547 25.84982681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926839590072632 2.1352155208587646 23.144840240478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793107509613037 2.5483787059783936 27.27689552307129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921302318572998 2.25227427482605 24.31487274169922
  batch 60 loss: 1.7921302318572998, 2.25227427482605, 24.31487274169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927271127700806 2.1399052143096924 23.1917781829834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.193476676940918 23.726449966430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925325632095337 2.271536350250244 24.50789451599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921218872070312 1.9366123676300049 21.158245086669922
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926669120788574 1.7262396812438965 19.055065155029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791862964630127 2.435162305831909 26.14348602294922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917760610580444 2.4445865154266357 26.237642288208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791567087173462 2.1848528385162354 23.640094757080078
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918989658355713 2.107809543609619 22.869993209838867
Total LOSS train 23.72921383197491 valid 24.722804069519043
CE LOSS train 1.7922410928286039 valid 0.4479747414588928
Contrastive LOSS train 2.193697274648226 valid 0.5269523859024048
EPOCH 237:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922203540802002 2.407867908477783 25.870899200439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915427684783936 2.584041118621826 27.631954193115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924612760543823 2.1347882747650146 23.140344619750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792364239692688 2.0350451469421387 22.1428165435791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920602560043335 2.1627607345581055 23.419668197631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928578853607178 2.2766826152801514 24.55968475341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918431758880615 2.2090554237365723 23.882396697998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 2.2429659366607666 24.221561431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791731357574463 2.2096047401428223 23.88777732849121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917428016662598 2.1465795040130615 23.257537841796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926992177963257 2.3168680667877197 24.961380004882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923989295959473 2.4568428993225098 26.36082649230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929438352584839 2.206554651260376 23.858489990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792615532875061 2.4164488315582275 25.957103729248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910889387130737 2.298659086227417 24.777679443359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79270601272583 2.353776216506958 25.330467224121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925692796707153 2.081059217453003 22.60316276550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924103736877441 2.403587579727173 25.828285217285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792313575744629 1.9935111999511719 21.72742462158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915908098220825 2.2941858768463135 24.733449935913086
  batch 20 loss: 1.7915908098220825, 2.2941858768463135, 24.733449935913086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924455404281616 2.1583170890808105 23.37561798095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924330234527588 2.1541264057159424 23.333696365356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922109365463257 2.013610601425171 21.92831802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926928997039795 2.0862104892730713 22.654796600341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923833131790161 2.4844164848327637 26.63654899597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924939393997192 2.215517520904541 23.947668075561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928560972213745 2.3589985370635986 25.382841110229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918678522109985 2.019486665725708 21.98673439025879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793310523033142 2.0826964378356934 22.620275497436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915133237838745 2.282888174057007 24.62039566040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556881904602 2.368147850036621 25.474035263061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914068698883057 2.3495213985443115 25.28662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924270629882812 2.265192985534668 24.44435691833496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925490140914917 1.8805102109909058 20.5976505279541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925034761428833 2.383228302001953 25.624786376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926043272018433 2.2513668537139893 24.306272506713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925257682800293 2.1657278537750244 23.449804306030273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916221618652344 2.120678663253784 22.998409271240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921453714370728 2.3605782985687256 25.39792823791504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916535139083862 2.2634027004241943 24.42568016052246
  batch 40 loss: 1.7916535139083862, 2.2634027004241943, 24.42568016052246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921658754348755 2.2535288333892822 24.32745361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923840284347534 2.067626714706421 22.468650817871094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920396327972412 2.1863324642181396 23.655364990234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916135787963867 2.209151268005371 23.88312530517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924737930297852 2.1198103427886963 22.990577697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921578884124756 2.1422805786132812 23.214963912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920551300048828 2.090834379196167 22.70039939880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791902780532837 2.2980005741119385 24.771907806396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603922843933 2.069361925125122 22.48522186279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920409440994263 2.0314841270446777 22.106884002685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910757064819336 2.3548648357391357 25.339725494384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923460006713867 2.382854700088501 25.620891571044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922277450561523 2.1163668632507324 22.95589828491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792556643486023 2.257620096206665 24.368757247924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918365001678467 2.390204906463623 25.693885803222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792540192604065 2.2254960536956787 24.047500610351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927024364471436 2.420529842376709 25.998001098632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792681336402893 2.157679557800293 23.369476318359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931050062179565 2.2036778926849365 23.829883575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792129635810852 2.3314969539642334 25.107099533081055
  batch 60 loss: 1.792129635810852, 2.3314969539642334, 25.107099533081055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927263975143433 2.1647469997406006 23.440196990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.097187042236328 22.763553619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925313711166382 2.1353960037231445 23.14649200439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921212911605835 2.2798776626586914 24.590898513793945
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926644086837769 1.818613886833191 19.978801727294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918617725372314 2.0922436714172363 22.714298248291016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917749881744385 2.188157320022583 23.67334747314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915664911270142 1.9039987325668335 20.831554412841797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918975353240967 2.005706548690796 21.848962783813477
Total LOSS train 23.992322452251727 valid 22.267040729522705
CE LOSS train 1.7922404711063091 valid 0.44797438383102417
Contrastive LOSS train 2.2200082008655255 valid 0.501426637172699
EPOCH 238:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922189235687256 2.140645742416382 23.19867706298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791542649269104 2.4603271484375 26.394813537597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924602031707764 2.102332353591919 22.81578254699707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923632860183716 2.2604987621307373 24.39735221862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920595407485962 2.1082119941711426 22.874177932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792856216430664 2.1286866664886475 23.079723358154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918422222137451 2.2959325313568115 24.75116729736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919007539749146 1.9080157279968262 20.872058868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.174848794937134 23.54021644592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917414903640747 2.230120897293091 24.09295082092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926976680755615 2.3222029209136963 25.014726638793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923978567123413 2.5160329341888428 26.952728271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929439544677734 2.2404305934906006 24.197250366210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926160097122192 2.3490846157073975 25.283462524414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910908460617065 2.3041489124298096 24.832578659057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792707085609436 2.3397457599639893 25.19016456604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925686836242676 2.266786813735962 24.46043586730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924095392227173 2.321819543838501 25.010604858398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923123836517334 2.158888578414917 23.38119888305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791590690612793 2.180478811264038 23.596378326416016
  batch 20 loss: 1.791590690612793, 2.180478811264038, 23.596378326416016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924443483352661 2.2229623794555664 24.02206802368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792431354522705 2.3164803981781006 24.95723533630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922097444534302 1.9877077341079712 21.669286727905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792691707611084 2.0910210609436035 22.70290184020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923825979232788 2.541058301925659 27.202966690063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924931049346924 2.2191505432128906 23.983999252319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792855143547058 2.4444665908813477 26.237520217895508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918678522109985 2.01849627494812 21.976831436157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933094501495361 2.3099775314331055 24.893085479736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915136814117432 2.166579484939575 23.457307815551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792554497718811 2.159583806991577 23.38839340209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914061546325684 2.191765308380127 23.70905876159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792425274848938 2.2485265731811523 24.277690887451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925472259521484 2.1921961307525635 23.714508056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925019264221191 2.393394947052002 25.726449966430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926028966903687 2.1742308139801025 23.534910202026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925249338150024 2.2974860668182373 24.767385482788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791622519493103 2.0276546478271484 22.06816864013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921457290649414 2.3661892414093018 25.454036712646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 1.940450668334961 21.1961612701416
  batch 40 loss: 1.7916537523269653, 1.940450668334961, 21.1961612701416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921655178070068 2.084559202194214 22.637758255004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923829555511475 2.064469575881958 22.43707847595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920403480529785 2.0940101146698 22.732141494750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916147708892822 1.9445921182632446 21.23753547668457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924748659133911 2.178283214569092 23.575307846069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921595573425293 2.2174429893493652 23.966590881347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792056918144226 2.241485357284546 24.206911087036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919045686721802 2.116400718688965 22.95591163635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.1084468364715576 22.876073837280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920403480529785 2.093954086303711 22.73158073425293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910757064819336 2.2294647693634033 24.085723876953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923446893692017 2.1900594234466553 23.69293975830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922265529632568 1.9837254285812378 21.629480361938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925572395324707 2.093061923980713 22.723175048828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918376922607422 2.268993377685547 24.48177146911621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792540431022644 1.7827986478805542 19.620525360107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702317237854 2.3707082271575928 25.499784469604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926809787750244 2.200575351715088 23.798433303833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931041717529297 2.3746068477630615 25.539173126220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921288013458252 2.3077878952026367 24.87000846862793
  batch 60 loss: 1.7921288013458252, 2.3077878952026367, 24.87000846862793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927242517471313 2.3223276138305664 25.016000747680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916823625564575 2.133659839630127 23.128280639648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925299406051636 2.2367770671844482 24.160301208496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921198606491089 2.225314140319824 24.04526138305664
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792663812637329 1.843949794769287 20.232162475585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791861653327942 2.236876964569092 24.160633087158203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917749881744385 2.1503021717071533 23.294795989990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915664911270142 2.0007646083831787 21.799213409423828
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918972969055176 1.9102870225906372 20.89476776123047
Total LOSS train 23.76548194885254 valid 22.537352561950684
CE LOSS train 1.792239966759315 valid 0.4479743242263794
Contrastive LOSS train 2.197324202610896 valid 0.4775717556476593
EPOCH 239:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922184467315674 2.233631134033203 24.128530502319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791542887687683 2.5903561115264893 27.69510269165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924597263336182 2.2373766899108887 24.166227340698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923628091812134 2.170274019241333 23.495101928710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920594215393066 2.2417943477630615 24.210002899169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928555011749268 2.3122787475585938 24.9156436920166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918423414230347 2.185439109802246 23.64623260498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919009923934937 2.0316548347473145 22.108448028564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.2013652324676514 23.805383682250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917418479919434 2.015458106994629 21.94632339477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926971912384033 2.387427568435669 25.666973114013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923977375030518 2.3919448852539062 25.71184730529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929426431655884 2.2532336711883545 24.325279235839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792615294456482 2.1220803260803223 23.013418197631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910915613174438 2.159449338912964 23.38558578491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927049398422241 2.4092161655426025 25.88486671447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925677299499512 2.299530267715454 24.787870407104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924087047576904 2.461181402206421 26.40422248840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923119068145752 2.021348714828491 22.005800247192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791590929031372 2.205652952194214 23.848121643066406
  batch 20 loss: 1.791590929031372, 2.205652952194214, 23.848121643066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924435138702393 2.108149766921997 22.87394142150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924308776855469 2.0864861011505127 22.657291412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792209267616272 2.0949745178222656 22.741954803466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926905155181885 2.168421745300293 23.47690773010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923816442489624 2.502599000930786 26.818370819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924919128417969 2.0638740062713623 22.431232452392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928537130355835 2.3434176445007324 25.227031707763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918678522109985 2.3757100105285645 25.548967361450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933084964752197 2.137906551361084 23.172374725341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915147542953491 2.2473015785217285 24.264530181884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925550937652588 2.404660701751709 25.839162826538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914071083068848 2.399873971939087 25.790145874023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924251556396484 2.1344900131225586 23.137325286865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925467491149902 2.2137269973754883 23.92981719970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925015687942505 2.211897373199463 23.911474227905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926020622253418 2.392928123474121 25.72188377380371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925243377685547 1.9857369661331177 21.64989471435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916232347488403 1.6806023120880127 18.597646713256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921456098556519 2.424525022506714 26.037395477294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.0625133514404297 22.41678810119629
  batch 40 loss: 1.791654109954834, 2.0625133514404297, 22.41678810119629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792165994644165 2.3309404850006104 25.10157012939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792382836341858 2.1550545692443848 23.342927932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920405864715576 2.3428399562835693 25.220439910888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916152477264404 2.237555503845215 24.16716957092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924741506576538 1.9923518896102905 21.715993881225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792158603668213 2.08520245552063 22.644182205200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792055606842041 1.9747450351715088 21.539505004882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919039726257324 2.144726276397705 23.239166259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916045188903809 2.1795191764831543 23.5867977142334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920397520065308 2.1339337825775146 23.131378173828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910757064819336 2.3872568607330322 25.66364288330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923437356948853 2.1579787731170654 23.37213134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922263145446777 2.054821729660034 22.340444564819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792555570602417 2.1022541522979736 22.815095901489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918370962142944 2.248467206954956 24.27651023864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925390005111694 2.215156316757202 23.944103240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927008867263794 2.4852445125579834 26.6451473236084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680263519287 2.361903190612793 25.411712646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931023836135864 2.4298670291900635 26.091772079467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921282052993774 2.1573572158813477 23.365699768066406
  batch 60 loss: 1.7921282052993774, 2.1573572158813477, 23.365699768066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792723298072815 2.088391065597534 22.676633834838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791682481765747 2.319355010986328 24.985233306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925288677215576 2.2845048904418945 24.637577056884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921198606491089 2.2178866863250732 23.97098731994629
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79266357421875 1.7213245630264282 19.005908966064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 2.2175912857055664 23.967775344848633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917759418487549 2.186877489089966 23.660551071166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915674448013306 2.027492046356201 22.06648826599121
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791898488998413 1.9613686800003052 21.40558624267578
Total LOSS train 23.912105384239783 valid 22.775100231170654
CE LOSS train 1.792239524767949 valid 0.44797462224960327
Contrastive LOSS train 2.2119865802618173 valid 0.4903421700000763
EPOCH 240:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218804359436 2.2996721267700195 24.7889404296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915436029434204 2.5133020877838135 26.924564361572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924597263336182 1.8530299663543701 20.3227596282959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923624515533447 2.251579523086548 24.308156967163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920594215393066 2.062408924102783 22.416147232055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928546667099 2.245816230773926 24.25101661682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791841983795166 2.0219314098358154 22.01115608215332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791900873184204 2.065504789352417 22.446949005126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 2.1062114238739014 22.853845596313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917417287826538 2.044024705886841 22.23198890686035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926963567733765 2.3244736194610596 25.037431716918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397379875183 2.509063243865967 26.88302993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929420471191406 2.3072996139526367 24.865938186645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926148176193237 2.281921863555908 24.611831665039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791092872619629 2.2469780445098877 24.26087188720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704701423645 2.473223924636841 26.524944305419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792567253112793 2.200671911239624 23.799285888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924087047576904 2.303861379623413 24.831022262573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923123836517334 1.8917319774627686 20.709632873535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915922403335571 1.98576819896698 21.649272918701172
  batch 20 loss: 1.7915922403335571, 1.98576819896698, 21.649272918701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924443483352661 1.9570043087005615 21.36248779296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924308776855469 2.0938868522644043 22.731300354003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922093868255615 2.177332639694214 23.565536499023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792689561843872 2.020291805267334 21.995609283447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923811674118042 2.4241678714752197 26.034059524536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924911975860596 2.3008980751037598 24.801471710205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928528785705566 2.1709699630737305 23.502552032470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918680906295776 2.1295559406280518 23.087427139282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933077812194824 2.284268379211426 24.6359920501709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915160655975342 1.8143662214279175 19.935176849365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925546169281006 2.2961156368255615 24.753711700439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914087772369385 2.248978853225708 24.28119659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924253940582275 2.0499885082244873 22.29231071472168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925468683242798 2.2052085399627686 23.844633102416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925010919570923 2.42777156829834 26.07021713256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926015853881836 2.3919432163238525 25.712032318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925221920013428 2.24804949760437 24.27301788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916221618652344 2.1541218757629395 23.332839965820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921438217163086 2.241391658782959 24.20606231689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791653037071228 2.1287641525268555 23.079294204711914
  batch 40 loss: 1.791653037071228, 2.1287641525268555, 23.079294204711914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921640872955322 2.2449281215667725 24.241445541381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792380928993225 2.093618869781494 22.72856903076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920387983322144 2.308823585510254 24.880273818969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916147708892822 2.219996213912964 23.9915771484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792472243309021 2.039778709411621 22.19025993347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921570539474487 2.247121572494507 24.26337242126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920547723770142 2.13033127784729 23.095367431640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79190194606781 1.5429004430770874 17.22090721130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791603684425354 2.1681089401245117 23.472692489624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792038083076477 2.1576342582702637 23.36838150024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910758256912231 2.254643201828003 24.337509155273438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923423051834106 2.2112972736358643 23.905315399169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922248840332031 2.200077533721924 23.792999267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925541400909424 2.238851308822632 24.181068420410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918362617492676 2.4316165447235107 26.108001708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925374507904053 2.2220184803009033 24.01272201538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926989793777466 2.4737963676452637 26.530664443969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792677879333496 1.9623130559921265 21.415809631347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7931008338928223 2.456556558609009 26.358665466308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921274900436401 2.2979819774627686 24.771947860717773
  batch 60 loss: 1.7921274900436401, 2.2979819774627686, 24.771947860717773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927217483520508 2.1734702587127686 23.527423858642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791682243347168 1.8133143186569214 19.924823760986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925275564193726 1.9748469591140747 21.540998458862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921183109283447 2.3087704181671143 24.87982177734375
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926610708236694 1.8339200019836426 20.131860733032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918614149093628 2.1000566482543945 22.79242706298828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917749881744385 2.1771767139434814 23.563541412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915666103363037 1.8968294858932495 20.75986099243164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791897177696228 1.8352248668670654 20.144145965576172
Total LOSS train 23.60151067880484 valid 21.814993858337402
CE LOSS train 1.792238870033851 valid 0.447974294424057
Contrastive LOSS train 2.1809271812438964 valid 0.45880621671676636
EPOCH 241:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922172546386719 2.2165043354034424 23.957260131835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915430068969727 2.511235475540161 26.90389633178711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792458176612854 2.0528714656829834 22.3211727142334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923614978790283 2.190988302230835 23.702245712280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920587062835693 2.2681851387023926 24.473909378051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928522825241089 2.21431827545166 23.93603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918412685394287 2.292264699935913 24.714488983154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919000387191772 2.1859023571014404 23.650924682617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.1280038356781006 23.071767807006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917416095733643 2.2306060791015625 24.097803115844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792695164680481 2.339249610900879 25.185192108154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792396068572998 2.3856353759765625 25.64875030517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929397821426392 2.271401882171631 24.506959915161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926130294799805 2.309583902359009 24.888450622558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791092872619629 2.218045234680176 23.971546173095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927027940750122 2.5155751705169678 26.948455810546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792565941810608 2.526838541030884 27.060951232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924062013626099 2.234076499938965 24.13317108154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923095226287842 1.8125735521316528 19.918045043945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915908098220825 2.1398632526397705 23.190223693847656
  batch 20 loss: 1.7915908098220825, 2.1398632526397705, 23.190223693847656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924416065216064 1.9639583826065063 21.432025909423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924293279647827 1.823976993560791 20.032197952270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922078371047974 1.9055671691894531 20.84787940979004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926875352859497 2.1249136924743652 23.041826248168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923790216445923 2.4786694049835205 26.579072952270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924888134002686 1.7925314903259277 19.717803955078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928498983383179 2.2922933101654053 24.715784072875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866421699524 2.0603814125061035 22.395679473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933043241500854 2.2390546798706055 24.18385124206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915151119232178 2.0580036640167236 22.371551513671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925525903701782 2.342966318130493 25.22221565246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914077043533325 2.4755709171295166 26.547117233276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792424201965332 2.1631417274475098 23.423839569091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925453186035156 2.2248129844665527 24.04067611694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925002574920654 2.3382608890533447 25.175107955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926002740859985 2.2740259170532227 24.532859802246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925220727920532 2.264195680618286 24.434478759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916227579116821 2.1508758068084717 23.30038070678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921441793441772 2.218930959701538 23.981454849243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916536331176758 2.084592819213867 22.63758087158203
  batch 40 loss: 1.7916536331176758, 2.084592819213867, 22.63758087158203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921645641326904 2.3544869422912598 25.337032318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923810482025146 2.0033154487609863 21.82553482055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920386791229248 2.331336259841919 25.10540008544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916147708892822 2.3918793201446533 25.710407257080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924715280532837 2.1829564571380615 23.62203598022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921565771102905 2.203138589859009 23.82354164123535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920544147491455 2.082709312438965 22.61914825439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791902780532837 2.088566780090332 22.677570343017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916043996810913 2.4290387630462646 26.081993103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920383214950562 2.118870258331299 22.98073959350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910771369934082 2.300328254699707 24.79435920715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792341709136963 2.2281830310821533 24.07417106628418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922241687774658 1.9751096963882446 21.54332160949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925530672073364 2.3701729774475098 25.494281768798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918365001678467 2.430661201477051 26.098447799682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925375699996948 1.8449145555496216 20.241683959960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926987409591675 2.22914719581604 24.084169387817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926777601242065 2.0724432468414307 22.517108917236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793099045753479 2.554915428161621 27.342252731323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921266555786133 2.2752137184143066 24.544261932373047
  batch 60 loss: 1.7921266555786133, 2.2752137184143066, 24.544261932373047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927205562591553 2.262113094329834 24.41385269165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791682243347168 2.1869351863861084 23.661033630371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925264835357666 2.224355936050415 24.03608512878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921175956726074 2.099717855453491 22.789297103881836
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926596403121948 1.4544870853424072 16.3375301361084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918609380722046 2.3841071128845215 25.632932662963867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917742729187012 2.2650842666625977 24.442617416381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915666103363037 2.030383586883545 22.09540367126465
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918968200683594 1.9852449893951416 21.644346237182617
Total LOSS train 23.79461376483624 valid 23.453824996948242
CE LOSS train 1.7922379181935237 valid 0.44797420501708984
Contrastive LOSS train 2.200237596951998 valid 0.4963112473487854
EPOCH 242:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922165393829346 2.2443151473999023 24.235368728637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915432453155518 2.5994296073913574 27.78584098815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924578189849854 2.080561399459839 22.598072052001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923611402511597 1.826257586479187 20.054935455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920583486557007 1.9581419229507446 21.373477935791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928521633148193 2.1291449069976807 23.084300994873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791841745376587 2.2024621963500977 23.816463470458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.1652235984802246 23.444137573242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917299270629883 1.9953078031539917 21.744808197021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917414903640747 2.1394567489624023 23.186309814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792694091796875 2.343482494354248 25.227519989013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923954725265503 2.5577611923217773 27.370006561279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929383516311646 2.222679615020752 24.019733428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792612075805664 2.331019163131714 25.10280418395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910938262939453 2.273747205734253 24.528566360473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927021980285645 2.4400041103363037 26.1927433013916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925654649734497 2.298473834991455 24.77730369567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792406439781189 2.333590269088745 25.12830924987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792310357093811 2.099708080291748 22.789392471313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791591763496399 2.168659210205078 23.47818374633789
  batch 20 loss: 1.791591763496399, 2.168659210205078, 23.47818374633789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441487312317 2.1927709579467773 23.720151901245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792428970336914 2.399724245071411 25.789670944213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922077178955078 2.184972047805786 23.64192771911621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926875352859497 2.0775277614593506 22.56796646118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923798561096191 2.558678150177002 27.379159927368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924890518188477 2.1237266063690186 23.029754638671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928497791290283 2.3311657905578613 25.104507446289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866660118103 2.230790376663208 24.099769592285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933036088943481 2.2154226303100586 23.94753074645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915159463882446 2.2159407138824463 23.9509220123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925517559051514 2.250805377960205 24.30060386657715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914080619812012 2.4797439575195312 26.588848114013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924226522445679 2.015437602996826 21.946800231933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925444841384888 2.217198371887207 23.964529037475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792499303817749 2.5177195072174072 26.969694137573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925996780395508 2.2782938480377197 24.575538635253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925220727920532 2.234377145767212 24.136293411254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916234731674194 2.0359983444213867 22.151607513427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921444177627563 2.2976865768432617 24.769010543823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.1786835193634033 23.578489303588867
  batch 40 loss: 1.791654348373413, 2.1786835193634033, 23.578489303588867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921640872955322 2.1494858264923096 23.28702163696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792380928993225 2.107167959213257 22.86406135559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920398712158203 2.163172721862793 23.42376708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791616439819336 2.201930046081543 23.810916900634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 2.249969005584717 24.292163848876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921578884124756 2.24013614654541 24.193519592285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920552492141724 1.948119878768921 21.27325439453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791904091835022 2.0302886962890625 22.094791412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.3323941230773926 25.11554527282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920397520065308 2.218303918838501 23.975078582763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910786867141724 2.2352538108825684 24.143617630004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792341947555542 2.4224603176116943 26.016944885253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922242879867554 2.100964069366455 22.801864624023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925530672073364 2.149661064147949 23.28916358947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918369770050049 2.195984125137329 23.751678466796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925361394882202 2.121364116668701 23.00617790222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926974296569824 2.438493490219116 26.17763328552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926775217056274 2.117953300476074 22.972209930419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930984497070312 2.4329118728637695 26.122217178344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921273708343506 2.248584032058716 24.27796745300293
  batch 60 loss: 1.7921273708343506, 2.248584032058716, 24.27796745300293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792719841003418 2.2634942531585693 24.427661895751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916827201843262 2.2589871883392334 24.381555557250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925260066986084 2.252459764480591 24.317123413085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792117714881897 2.150731086730957 23.299428939819336
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792659878730774 1.798586368560791 19.778522491455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918617725372314 2.2861685752868652 24.653549194335938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917752265930176 2.226388692855835 24.055662155151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915676832199097 2.0244390964508057 22.03595733642578
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918972969055176 1.9042115211486816 20.83401107788086
Total LOSS train 23.98838371863732 valid 22.894794940948486
CE LOSS train 1.792237973213196 valid 0.4479743242263794
Contrastive LOSS train 2.2196145662894615 valid 0.4760528802871704
EPOCH 243:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922168970108032 2.257335662841797 24.36557388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915438413619995 2.5036866664886475 26.828411102294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924572229385376 2.1867997646331787 23.66045570373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923604249954224 2.294002056121826 24.73238182067871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920571565628052 2.261462688446045 24.40668487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928498983383179 2.2534706592559814 24.327556610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791840672492981 2.1186795234680176 22.978635787963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79189932346344 2.068812847137451 22.48002815246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730523109436 2.2653114795684814 24.44484519958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917418479919434 2.203500270843506 23.826745986938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792693018913269 2.322413921356201 25.01683235168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394995689392 2.4623727798461914 26.416122436523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792936086654663 2.064781665802002 22.440752029418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926104068756104 2.289271831512451 24.68532943725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910939455032349 2.265204906463623 24.443143844604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792699933052063 2.3057916164398193 24.850616455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925646305084229 2.3107495307922363 24.90005874633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924063205718994 2.348846435546875 25.28087043762207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923105955123901 1.9844363927841187 21.636674880981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915924787521362 1.8086190223693848 19.87778091430664
  batch 20 loss: 1.7915924787521362, 1.8086190223693848, 19.87778091430664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924408912658691 2.0170624256134033 21.963064193725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924269437789917 2.1166160106658936 22.958587646484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206048965454 2.203986167907715 23.832067489624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926855087280273 2.2165627479553223 23.95831298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923778295516968 2.399094343185425 25.783321380615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924871444702148 2.261693239212036 24.409420013427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928473949432373 2.2844226360321045 24.637073516845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791865587234497 2.171729564666748 23.50916290283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7933003902435303 2.0307698249816895 22.100997924804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915152311325073 2.2105820178985596 23.897335052490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 2.3623178005218506 25.415729522705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914079427719116 2.3997838497161865 25.789247512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792421817779541 2.1852867603302 23.645288467407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542576789856 2.2416656017303467 24.209199905395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792497158050537 2.271052598953247 24.503023147583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925971746444702 2.1742069721221924 23.534666061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925195693969727 2.230708122253418 24.09960174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916227579116821 2.1651806831359863 23.44342803955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921428680419922 2.311525821685791 24.907400131225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916533946990967 2.137277364730835 23.164426803588867
  batch 40 loss: 1.7916533946990967, 2.137277364730835, 23.164426803588867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921628952026367 2.432809591293335 26.120258331298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923789024353027 2.055619716644287 22.34857749938965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920377254486084 2.347484588623047 25.266883850097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916148900985718 2.1849184036254883 23.640798568725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924692630767822 2.0419251918792725 22.211721420288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921547889709473 2.2524805068969727 24.316959381103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79205322265625 2.0876948833465576 22.669002532958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919015884399414 2.325678586959839 25.048686981201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916040420532227 2.3224234580993652 25.015838623046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920376062393188 1.95464289188385 21.33846664428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910789251327515 2.228706121444702 24.078140258789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923424243927002 2.254265546798706 24.334999084472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922253608703613 2.0376007556915283 22.168231964111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925537824630737 2.1817502975463867 23.610055923461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 2.3741495609283447 25.533334732055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792536973953247 2.0613291263580322 22.40582847595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926981449127197 2.492614269256592 26.718841552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926762104034424 2.1886932849884033 23.679609298706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930967807769775 2.459393262863159 26.38702964782715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792127013206482 2.245474338531494 24.246870040893555
  batch 60 loss: 1.792127013206482, 2.245474338531494, 24.246870040893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927188873291016 2.458142042160034 26.3741397857666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.084059953689575 22.632282257080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925257682800293 2.181734561920166 23.60987091064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792117953300476 2.28835129737854 24.675630569458008
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792658805847168 1.9124031066894531 20.916690826416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791861891746521 2.378431558609009 25.5761775970459
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917754650115967 2.342782735824585 25.219602584838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915682792663574 2.1699109077453613 23.490676879882812
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918975353240967 2.1564619541168213 23.356515884399414
Total LOSS train 24.010916988666242 valid 24.410743236541748
CE LOSS train 1.7922369681871855 valid 0.44797438383102417
Contrastive LOSS train 2.2218679941617525 valid 0.5391154885292053
EPOCH 244:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922166585922241 2.3344006538391113 25.13622283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791544795036316 2.4823954105377197 26.615497589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924562692642212 2.1482396125793457 23.274850845336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923593521118164 2.023914098739624 22.03150177001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920570373535156 2.1423323154449463 23.21537971496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792848825454712 2.300774097442627 24.800588607788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918407917022705 2.3636908531188965 25.42875099182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791899561882019 2.1600236892700195 23.392135620117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.072950839996338 22.521236419677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.1349847316741943 23.14158821105957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926911115646362 2.4779086112976074 26.57177734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923938035964966 2.3710505962371826 25.502901077270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929354906082153 2.0304441452026367 22.09737777709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926099300384521 2.3649282455444336 25.441892623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910951375961304 2.2715256214141846 24.506351470947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926992177963257 2.287515163421631 24.6678524017334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925626039505005 2.185128688812256 23.643850326538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924039363861084 2.1761012077331543 23.553417205810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792307734489441 2.065762519836426 22.449932098388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915911674499512 2.0992813110351562 22.784404754638672
  batch 20 loss: 1.7915911674499512, 2.0992813110351562, 22.784404754638672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924389839172363 2.1328203678131104 23.120641708374023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924269437789917 2.2213454246520996 24.005882263183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922059297561646 1.5592321157455444 17.3845272064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926850318908691 1.7594597339630127 19.38728141784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923779487609863 2.518082618713379 26.973203659057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924871444702148 2.098632335662842 22.778812408447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928466796875 2.387535810470581 25.66820526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918665409088135 1.9507404565811157 21.299270629882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932990789413452 2.171908140182495 23.512380599975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791516661643982 2.1370484828948975 23.162002563476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 1.9957845211029053 21.750396728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914092540740967 2.3989782333374023 25.781190872192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924212217330933 2.3339762687683105 25.132184982299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541742324829 2.183204174041748 23.624584197998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792496919631958 2.3170666694641113 24.963163375854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925965785980225 2.0483691692352295 22.276288986206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519450187683 2.0987181663513184 22.779701232910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916232347488403 2.2170145511627197 23.961769104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921429872512817 2.2592427730560303 24.384571075439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.1997389793395996 23.789045333862305
  batch 40 loss: 1.7916538715362549, 2.1997389793395996, 23.789045333862305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921631336212158 2.3681209087371826 25.473373413085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923787832260132 1.8544578552246094 20.336957931518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792038083076477 2.1886744499206543 23.678783416748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916157245635986 2.0623245239257812 22.41486167907715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79246985912323 1.682234287261963 18.614810943603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921558618545532 2.3478071689605713 25.270227432250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920536994934082 2.14605712890625 23.25262451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919025421142578 2.0471765995025635 22.263668060302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791604995727539 2.394890069961548 25.74050521850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920374870300293 1.9799283742904663 21.59132194519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910794019699097 2.3285024166107178 25.07610321044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923399209976196 2.4116194248199463 25.908533096313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922228574752808 2.2469754219055176 24.26197624206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792550802230835 2.178942918777466 23.581979751586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918367385864258 2.243683338165283 24.228668212890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925347089767456 2.1559908390045166 23.352441787719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79269540309906 2.427704095840454 26.06973648071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926750183105469 2.238407850265503 24.176753997802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930947542190552 2.467259407043457 26.465688705444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792125940322876 2.0950653553009033 22.742778778076172
  batch 60 loss: 1.792125940322876, 2.0950653553009033, 22.742778778076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927173376083374 2.3235063552856445 25.027780532836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916834354400635 2.2215044498443604 24.00672721862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925245761871338 2.1602091789245605 23.394617080688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921172380447388 2.2907278537750244 24.69939613342285
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792657494544983 1.7508636713027954 19.301294326782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918615341186523 2.20076060295105 23.799468994140625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917754650115967 2.2134182453155518 23.92595672607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915680408477783 1.950850248336792 21.300071716308594
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918974161148071 1.8507829904556274 20.299726486206055
Total LOSS train 23.652988052368165 valid 22.331305980682373
CE LOSS train 1.7922363941486066 valid 0.4479743540287018
Contrastive LOSS train 2.186075159219595 valid 0.46269574761390686
EPOCH 245:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922160625457764 2.209369421005249 23.885910034179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791544795036316 2.6169800758361816 27.96134376525879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924554347991943 2.2538154125213623 24.330610275268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923591136932373 2.292759418487549 24.719951629638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920575141906738 2.1243274211883545 23.03533172607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928485870361328 2.204570770263672 23.83855628967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918415069580078 2.3189189434051514 24.98103141784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 1.7212456464767456 19.004356384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.007422924041748 21.86595916748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917418479919434 2.0972204208374023 22.763946533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926907539367676 2.0828194618225098 22.62088394165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923938035964966 2.026482343673706 22.057218551635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792934775352478 1.8706637620925903 20.49957275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926098108291626 2.317634344100952 24.96895408630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910966873168945 2.1860148906707764 23.6512451171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926976680755615 2.2542879581451416 24.3355770111084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925615310668945 2.176234483718872 23.55490493774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924034595489502 2.148869752883911 23.28110122680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923084497451782 2.107177257537842 22.86408233642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915922403335571 2.2776684761047363 24.568275451660156
  batch 20 loss: 1.7915922403335571, 2.2776684761047363, 24.568275451660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924381494522095 2.114722967147827 22.939668655395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924258708953857 2.261085271835327 24.403278350830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922052145004272 1.8783283233642578 20.575489044189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.1575942039489746 23.36862564086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923773527145386 2.4738383293151855 26.53076171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792486548423767 2.3592677116394043 25.385164260864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928448915481567 2.202775239944458 23.82059669494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918663024902344 1.9850513935089111 21.642379760742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932976484298706 2.346404790878296 25.25734519958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915176153182983 2.048102617263794 22.27254295349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792550802230835 2.3230202198028564 25.02275276184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914103269577026 2.2109811305999756 23.901222229003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924209833145142 2.291752815246582 24.709949493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792541265487671 2.3431293964385986 25.223834991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924962043762207 2.4242255687713623 26.034751892089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925962209701538 2.2765018939971924 24.557615280151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519450187683 2.295804262161255 24.750560760498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.005418062210083 21.84580421447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921429872512817 2.288971185684204 24.681854248046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.175164222717285 23.543296813964844
  batch 40 loss: 1.7916545867919922, 2.175164222717285, 23.543296813964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792162537574768 2.417891502380371 25.97107696533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923786640167236 2.1387083530426025 23.179462432861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792038083076477 2.253413438796997 24.326171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916160821914673 2.1170928478240967 22.96254539489746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924673557281494 2.184312582015991 23.63559341430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921538352966309 2.3405823707580566 25.19797706604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920522689819336 1.599831461906433 17.790367126464844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919026613235474 2.164930820465088 23.44120979309082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916053533554077 2.36203670501709 25.411972045898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920377254486084 2.1231563091278076 23.023601531982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910791635513306 2.287018060684204 24.661258697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923383712768555 2.2620484828948975 24.412822723388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922216653823853 2.0263166427612305 22.055387496948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925491333007812 2.1586456298828125 23.379005432128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918353080749512 2.337639331817627 25.168228149414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925331592559814 2.165764093399048 23.45017433166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926932573318481 2.2785866260528564 24.57855987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926723957061768 2.1122381687164307 22.915054321289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930935621261597 2.447995901107788 26.273052215576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921258211135864 2.3168797492980957 24.960922241210938
  batch 60 loss: 1.7921258211135864, 2.3168797492980957, 24.960922241210938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927151918411255 2.2618329524993896 24.41104507446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791682243347168 2.085282325744629 22.64450454711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925223112106323 2.217041254043579 23.962934494018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792114496231079 2.349484920501709 25.286964416503906
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926543951034546 1.790729284286499 19.699947357177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.007498264312744 21.86684226989746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791773796081543 1.9034607410430908 20.82638168334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915668487548828 1.5555733442306519 17.347301483154297
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918957471847534 1.5450997352600098 17.24289321899414
Total LOSS train 23.723940717256987 valid 19.320854663848877
CE LOSS train 1.7922358237780058 valid 0.44797393679618835
Contrastive LOSS train 2.193170501635625 valid 0.38627493381500244
EPOCH 246:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922139167785645 1.9364852905273438 21.157066345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915443181991577 2.563809633255005 27.42963981628418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924535274505615 2.1121127605438232 22.91358184814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923576831817627 2.3407416343688965 25.19977569580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920564413070679 2.1897947788238525 23.690004348754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928463220596313 1.9704285860061646 21.497133255004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918407917022705 2.3959858417510986 25.751699447631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918994426727295 2.0454604625701904 22.246503829956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.245959758758545 24.251327514648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917413711547852 1.8866373300552368 20.65811538696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926892042160034 2.4234800338745117 26.027488708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923941612243652 2.490025758743286 26.692651748657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929342985153198 2.0446813106536865 22.239748001098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926095724105835 2.5734686851501465 27.527297973632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910988330841064 2.438230037689209 26.17340087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926976680755615 1.914747953414917 20.94017791748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925611734390259 2.358228921890259 25.374849319458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792402744293213 2.1542468070983887 23.334871292114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923072576522827 1.9773303270339966 21.565610885620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 1.85202956199646 20.3118896484375
  batch 20 loss: 1.7915925979614258, 1.85202956199646, 20.3118896484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.1562182903289795 23.35462188720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792425513267517 2.033578872680664 22.12821388244629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922064065933228 1.9099050760269165 20.89125633239746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926832437515259 2.1550772190093994 23.343454360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923766374588013 2.389043092727661 25.68280792236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924855947494507 2.152411460876465 23.316600799560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928434610366821 2.44399094581604 26.232751846313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791865348815918 2.1946299076080322 23.738162994384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793294072151184 2.1492116451263428 23.285411834716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791516661643982 2.053514003753662 22.326658248901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925474643707275 2.160977840423584 23.402326583862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914094924926758 2.3865420818328857 25.656829833984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924188375473022 2.3126893043518066 24.9193115234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925394773483276 2.1871564388275146 23.664104461669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924941778182983 2.3405914306640625 25.198408126831055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925938367843628 1.8440632820129395 20.233224868774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925162315368652 2.1832780838012695 23.62529754638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791622519493103 2.2099530696868896 23.89115333557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921411991119385 2.163121461868286 23.423355102539062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916531562805176 2.311528444290161 24.906936645507812
  batch 40 loss: 1.7916531562805176, 2.311528444290161, 24.906936645507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921615839004517 2.2341833114624023 24.133995056152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923767566680908 1.997920274734497 21.77157974243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920373678207397 2.293381690979004 24.725854873657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916159629821777 2.1873621940612793 23.665239334106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924672365188599 2.1739413738250732 23.53188133239746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79215407371521 2.1900243759155273 23.692398071289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920527458190918 2.079864740371704 22.590700149536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919018268585205 2.207935333251953 23.87125587463379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916052341461182 2.1781580448150635 23.573184967041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792036771774292 2.019061326980591 21.982648849487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910810708999634 2.3750293254852295 25.54137420654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923377752304077 2.2504422664642334 24.29676055908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792221188545227 2.1539597511291504 23.331817626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925481796264648 2.1427195072174072 23.219741821289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918357849121094 2.4473142623901367 26.264978408813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925323247909546 2.1963181495666504 23.755712509155273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792691946029663 2.525693416595459 27.04962730407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926713228225708 2.1400277614593506 23.192949295043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793090581893921 2.44201397895813 26.21323013305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921245098114014 2.309407949447632 24.88620376586914
  batch 60 loss: 1.7921245098114014, 2.309407949447632, 24.88620376586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927143573760986 2.330662488937378 25.099340438842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916827201843262 2.1583828926086426 23.375511169433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792521595954895 2.079719066619873 22.58971405029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792115330696106 2.15175724029541 23.309688568115234
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926543951034546 1.7297632694244385 19.090286254882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918606996536255 2.3938612937927246 25.7304744720459
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774868965149 2.4064722061157227 25.856496810913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915680408477783 1.958132028579712 21.372888565063477
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918963432312012 1.9366601705551147 21.158498764038086
Total LOSS train 23.737836867112378 valid 23.529589653015137
CE LOSS train 1.7922349966489353 valid 0.4479740858078003
Contrastive LOSS train 2.1945601756756123 valid 0.4841650426387787
EPOCH 247:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922143936157227 2.2330896854400635 24.123111724853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915451526641846 2.608466386795044 27.876209259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792453646659851 1.7175216674804688 18.967670440673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923576831817627 2.1356117725372314 23.148475646972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792056679725647 2.1621012687683105 23.413070678710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928458452224731 2.2780544757843018 24.57339096069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918415069580078 2.1759681701660156 23.551523208618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918999195098877 2.042288303375244 22.21478271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.2141172885894775 23.932903289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917412519454956 2.0711183547973633 22.5029239654541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926877737045288 2.4698729515075684 26.491418838500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923917770385742 2.442192554473877 26.214317321777344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792931079864502 2.4073691368103027 25.866622924804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926069498062134 2.186689615249634 23.659502029418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910981178283691 2.3753464221954346 25.5445613861084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926958799362183 2.299595355987549 24.78864860534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925599813461304 2.125915050506592 23.051712036132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924017906188965 2.2539334297180176 24.331735610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923063039779663 1.9444769620895386 21.237075805664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915923595428467 2.192984104156494 23.721431732177734
  batch 20 loss: 1.7915923595428467, 2.192984104156494, 23.721431732177734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924368381500244 2.051549196243286 22.30792808532715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924245595932007 2.0510003566741943 22.30242919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922046184539795 2.06756329536438 22.467836380004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926816940307617 2.188286781311035 23.675548553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923755645751953 2.1585447788238525 23.377822875976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924848794937134 2.27636981010437 24.556182861328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928428649902344 2.368053436279297 25.473377227783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791866421699524 2.1894428730010986 23.686294555664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793293833732605 2.4063565731048584 25.85685920715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915180921554565 2.172666072845459 23.518178939819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925479412078857 2.341670036315918 25.209247589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914106845855713 2.4352333545684814 26.14374351501465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924182415008545 2.1278319358825684 23.070737838745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925384044647217 1.9192988872528076 20.98552703857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792493462562561 2.4532864093780518 26.32535743713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 2.3117175102233887 24.90976905822754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925159931182861 2.2868988513946533 24.6615047454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916237115859985 2.0425095558166504 22.216718673706055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921419143676758 2.338778257369995 25.17992401123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.290238857269287 24.69404411315918
  batch 40 loss: 1.7916548252105713, 2.290238857269287, 24.69404411315918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921620607376099 2.360135555267334 25.393518447875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923766374588013 1.9600809812545776 21.393186569213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920371294021606 2.3675153255462646 25.467191696166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916167974472046 2.2091801166534424 23.8834171295166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924672365188599 1.7136001586914062 18.928468704223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79215407371521 2.214104175567627 23.933195114135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920526266098022 2.1681466102600098 23.47351837158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791901707649231 2.00439190864563 21.8358211517334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916048765182495 2.264134645462036 24.432950973510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920361757278442 1.991640567779541 21.70844078063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910821437835693 2.357860565185547 25.369688034057617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923372983932495 2.3802785873413086 25.595123291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792221188545227 2.2597711086273193 24.38993263244629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925479412078857 2.37064528465271 25.499000549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918360233306885 2.3419978618621826 25.211814880371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925312519073486 2.1730446815490723 23.522977828979492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926912307739258 2.4439151287078857 26.231842041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926709651947021 2.2203545570373535 23.9962158203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930887937545776 2.4416327476501465 26.20941734313965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921245098114014 2.137686014175415 23.168983459472656
  batch 60 loss: 1.7921245098114014, 2.137686014175415, 23.168983459472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927135229110718 2.259962558746338 24.392337799072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916834354400635 2.2425684928894043 24.217369079589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925211191177368 2.1253466606140137 23.045988082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921152114868164 2.2600326538085938 24.392440795898438
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926517724990845 1.7812120914459229 19.604772567749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918593883514404 2.321009397506714 25.001953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.301952362060547 24.811296463012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915674448013306 1.9823216199874878 21.614782333374023
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918952703475952 1.9401880502700806 21.193775177001953
Total LOSS train 23.925042049701396 valid 23.155451774597168
CE LOSS train 1.7922346390210666 valid 0.4479738175868988
Contrastive LOSS train 2.213280751154973 valid 0.48504701256752014
EPOCH 248:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922128438949585 2.2576091289520264 24.368305206298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915446758270264 2.587451696395874 27.666061401367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79245126247406 2.254166603088379 24.334117889404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923554182052612 2.145282506942749 23.245180130004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920562028884888 2.0068907737731934 21.860965728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792842984199524 2.3212997913360596 25.005840301513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918401956558228 2.127661943435669 23.068458557128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 2.193922758102417 23.73112678527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.04880952835083 22.279823303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791741967201233 2.1256308555603027 23.048051834106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 2.422372579574585 26.01641273498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923901081085205 2.409801483154297 25.890405654907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929283380508423 2.31246018409729 24.917530059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792604684829712 2.238104820251465 24.17365264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910981178283691 2.257880687713623 24.369905471801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792693853378296 2.3289384841918945 25.08207893371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557716369629 2.1577019691467285 23.36957550048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924000024795532 2.1608729362487793 23.40113067626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923048734664917 2.1098122596740723 22.890426635742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915918827056885 1.6543430089950562 18.33502197265625
  batch 20 loss: 1.7915918827056885, 1.6543430089950562, 18.33502197265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792435646057129 1.7075847387313843 18.868282318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924224138259888 2.10676908493042 22.86011505126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922024726867676 2.0142579078674316 21.93478012084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678713798523 2.245276927947998 24.24544906616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792373776435852 2.3850255012512207 25.642627716064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792482614517212 2.2607150077819824 24.399633407592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928400039672852 2.2628893852233887 24.421733856201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918648719787598 2.1298747062683105 23.090612411499023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932897806167603 2.2871172428131104 24.664461135864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915173768997192 2.4149465560913086 25.940982818603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925453186035156 2.4004769325256348 25.797313690185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914104461669922 2.323965549468994 25.031064987182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924168109893799 2.2350871562957764 24.14328956604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925368547439575 2.2671265602111816 24.463802337646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924920320510864 2.3522605895996094 25.31509780883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925913333892822 1.9232730865478516 21.02532196044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514443397522 2.245164394378662 24.244159698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791623592376709 2.3846373558044434 25.637998580932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921409606933594 2.277139902114868 24.563539505004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 1.8815913200378418 20.607568740844727
  batch 40 loss: 1.7916542291641235, 1.8815913200378418, 20.607568740844727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921606302261353 2.2666499614715576 24.458660125732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923755645751953 2.074120044708252 22.5335750579834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920361757278442 2.3031303882598877 24.823339462280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916167974472046 2.017548084259033 21.96709632873535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924656867980957 1.7784618139266968 19.577083587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921534776687622 2.1285345554351807 23.077499389648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792051911354065 2.2699344158172607 24.491395950317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791901707649231 2.060232639312744 22.394227981567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 2.303483009338379 24.826435089111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920358180999756 2.0593345165252686 22.3853816986084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910826206207275 2.3331573009490967 25.122655868530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923359870910645 2.124990463256836 23.042240142822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792220115661621 2.2094883918762207 23.887104034423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925466299057007 2.3175926208496094 24.968473434448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918365001678467 2.235100507736206 24.142841339111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925306558609009 2.118680477142334 22.97933578491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792689561843872 2.434117555618286 26.133865356445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926688194274902 2.1591339111328125 23.384008407592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930868864059448 2.444340229034424 26.236488342285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921240329742432 2.2549400329589844 24.341524124145508
  batch 60 loss: 1.7921240329742432, 2.2549400329589844, 24.341524124145508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927117347717285 2.156062364578247 23.353334426879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.093747854232788 22.729162216186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925198078155518 2.109583616256714 22.888357162475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921146154403687 2.2934341430664062 24.726455688476562
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926517724990845 1.6425490379333496 18.218143463134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918602228164673 2.316148042678833 24.953340530395508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917742729187012 2.2459373474121094 24.251148223876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791568398475647 1.9257408380508423 21.04897689819336
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918959856033325 1.7911839485168457 19.7037353515625
Total LOSS train 23.70216293334961 valid 22.48930025100708
CE LOSS train 1.7922333735686082 valid 0.44797399640083313
Contrastive LOSS train 2.1909929513931274 valid 0.4477959871292114
EPOCH 249:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922128438949585 2.203559637069702 23.827810287475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915449142456055 2.557941436767578 27.370960235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792450189590454 2.123220920562744 23.024658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923550605773926 1.7511260509490967 19.30361557006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920547723770142 2.150585412979126 23.297908782958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928417921066284 2.1234891414642334 23.027732849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918400764465332 2.322312593460083 25.014965057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 2.054041862487793 22.332317352294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917295694351196 2.2417843341827393 24.209571838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.1370327472686768 23.162067413330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926851511001587 2.3112032413482666 24.90471649169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792389988899231 2.535198211669922 27.144372940063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929272651672363 1.8662879467010498 20.455806732177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926043272018433 1.9204621315002441 20.997224807739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791100025177002 2.1329665184020996 23.120765686035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926928997039795 2.2585747241973877 24.37843894958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792557716369629 2.1051387786865234 22.843944549560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923996448516846 2.32106351852417 25.003036499023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923051118850708 1.9619145393371582 21.411449432373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915931940078735 2.370335102081299 25.494943618774414
  batch 20 loss: 1.7915931940078735, 2.370335102081299, 25.494943618774414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924344539642334 2.062584400177002 22.418277740478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924224138259888 2.0918381214141846 22.710803985595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792202353477478 1.8789043426513672 20.58124542236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 2.242051601409912 24.21319580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923732995986938 2.421605110168457 26.008424758911133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792481541633606 1.80125093460083 19.804990768432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928383350372314 1.874131679534912 20.534156799316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918654680252075 1.806071162223816 19.852577209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932887077331543 2.406625270843506 25.859542846679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915186882019043 2.2427048683166504 24.21856689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792545199394226 2.460474729537964 26.397293090820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914118766784668 2.4221370220184326 26.01278305053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924169301986694 2.2534334659576416 24.326751708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925362586975098 2.2273972034454346 24.06650733947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924916744232178 2.5037505626678467 26.829998016357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792590856552124 2.1556475162506104 23.34906578063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925138473510742 2.2403934001922607 24.196449279785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791623830795288 2.20747709274292 23.866395950317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792140245437622 2.3133764266967773 24.925905227661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.30483341217041 24.839988708496094
  batch 40 loss: 1.7916542291641235, 2.30483341217041, 24.839988708496094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921605110168457 2.1487390995025635 23.279550552368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792375087738037 1.9334807395935059 21.12718391418457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792036771774292 2.3881406784057617 25.673442840576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 1.7233232259750366 19.024850845336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924655675888062 2.034968137741089 22.142147064208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921534776687622 2.2866902351379395 24.659055709838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920523881912231 2.105997323989868 22.852025985717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919021844863892 2.0170555114746094 21.96245765686035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791606068611145 2.116051435470581 22.95212173461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920364141464233 1.8068768978118896 19.86080551147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79108464717865 1.962014079093933 21.411224365234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923365831375122 1.9195952415466309 20.988290786743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922204732894897 2.016967296600342 21.961894989013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925454378128052 2.2203681468963623 23.996227264404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918357849121094 2.171996831893921 23.511804580688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925291061401367 2.0082201957702637 21.874732971191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926886081695557 2.299233913421631 24.7850284576416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792668342590332 2.2830400466918945 24.623069763183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930853366851807 2.5949454307556152 27.74254035949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921231985092163 2.3479225635528564 25.27134895324707
  batch 60 loss: 1.7921231985092163, 2.3479225635528564, 25.27134895324707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927101850509644 2.0923218727111816 22.715927124023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916834354400635 1.9778022766113281 21.569705963134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925187349319458 2.080864429473877 22.60116195678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792114019393921 2.357978343963623 25.371898651123047
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926504611968994 1.4556854963302612 16.349506378173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918601036071777 2.2986514568328857 24.77837562561035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917745113372803 2.190375566482544 23.69552993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791568636894226 1.9249303340911865 21.04087257385254
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791895866394043 1.8921631574630737 20.71352767944336
Total LOSS train 23.286880493164062 valid 22.557076454162598
CE LOSS train 1.7922331204781166 valid 0.44797396659851074
Contrastive LOSS train 2.149464717278114 valid 0.47304078936576843
EPOCH 250:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922122478485107 2.323230266571045 25.02451515197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915457487106323 2.493518352508545 26.726730346679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924498319625854 1.989291787147522 21.685367584228516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923548221588135 2.071718692779541 22.509540557861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920550107955933 1.860246181488037 20.39451789855957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928410768508911 2.3077261447906494 24.870101928710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918403148651123 1.9633221626281738 21.425060272216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 1.8615813255310059 20.407712936401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.0949203968048096 22.74093246459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917412519454956 2.0346148014068604 22.137887954711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926841974258423 2.3524539470672607 25.317224502563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923895120620728 2.5709965229034424 27.50235366821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792926549911499 2.163996696472168 23.432893753051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792603850364685 2.266103506088257 24.45363998413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911019325256348 2.19681453704834 23.759246826171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926924228668213 2.3108561038970947 24.90125274658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925571203231812 2.1004955768585205 22.79751205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923990488052368 2.174415111541748 23.536550521850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923049926757812 2.050445079803467 22.296756744384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915936708450317 2.201056957244873 23.80216407775879
  batch 20 loss: 1.7915936708450317, 2.201056957244873, 23.80216407775879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924339771270752 2.159668445587158 23.389118194580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924221754074097 1.6963343620300293 18.755765914916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922027111053467 1.7226684093475342 19.01888656616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 1.9369938373565674 21.162616729736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923732995986938 2.079118490219116 22.583559036254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480707168579 2.1921403408050537 23.713884353637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928367853164673 2.2872474193573 24.665311813354492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 1.9785268306732178 21.577133178710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932850122451782 1.7738300561904907 19.531585693359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915173768997192 2.088029146194458 22.67180824279785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925423383712769 2.422417163848877 26.016712188720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914111614227295 2.3616793155670166 25.408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924145460128784 2.1429898738861084 23.222312927246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925347089767456 2.512246608734131 26.915000915527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924896478652954 2.5351836681365967 27.14432716369629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925883531570435 2.1687347888946533 23.479936599731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925118207931519 2.179903984069824 23.591550827026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916237115859985 2.1658871173858643 23.45049476623535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921392917633057 2.2027065753936768 23.819204330444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916539907455444 2.1596858501434326 23.388513565063477
  batch 40 loss: 1.7916539907455444, 2.1596858501434326, 23.388513565063477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921591997146606 2.3545925617218018 25.338085174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923730611801147 1.9390300512313843 21.182674407958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920340299606323 2.176208257675171 23.55411720275879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916154861450195 2.221628427505493 24.00790023803711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79246187210083 2.2164676189422607 23.957138061523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921509742736816 2.152294874191284 23.315099716186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920501232147217 2.1548421382904053 23.340471267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919001579284668 2.008765459060669 21.879554748535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916046380996704 2.2998170852661133 24.789775848388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920337915420532 2.222020387649536 24.012237548828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910836935043335 2.2922937870025635 24.714021682739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923333644866943 2.254891872406006 24.34125328063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218804359436 2.091146945953369 22.70368766784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925437688827515 2.203392744064331 23.82647132873535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918351888656616 2.328303337097168 25.07486915588379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925275564193726 2.0948173999786377 22.74070167541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792686104774475 2.436553478240967 26.158222198486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792665719985962 2.106973886489868 22.862403869628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793082356452942 2.468827247619629 26.481355667114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921222448349 2.2353882789611816 24.14600372314453
  batch 60 loss: 1.7921222448349, 2.2353882789611816, 24.14600372314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792709469795227 2.3131017684936523 24.92372703552246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916842699050903 2.2379488945007324 24.171175003051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925183773040771 2.03511905670166 22.143709182739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921135425567627 2.3151299953460693 24.94341468811035
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926486730575562 1.6817001104354858 18.609649658203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.4981865882873535 26.773725509643555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 2.41151762008667 25.906951904296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 2.1761434078216553 23.553003311157227
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918956279754639 1.9823473691940308 21.615367889404297
Total LOSS train 23.483778586754433 valid 24.46226215362549
CE LOSS train 1.792232032922598 valid 0.44797390699386597
Contrastive LOSS train 2.169154647680429 valid 0.4955868422985077
EPOCH 251:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922114133834839 2.2432174682617188 24.22438621520996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915462255477905 2.4147024154663086 25.938570022583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792448878288269 2.1920392513275146 23.712841033935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923535108566284 2.275930643081665 24.551658630371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920544147491455 2.337476968765259 25.166824340820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928391695022583 2.238436698913574 24.17720603942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839838027954 2.263166666030884 24.423505783081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918988466262817 2.12788987159729 23.070796966552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917295694351196 2.0324480533599854 22.116209030151367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917404174804688 2.0048298835754395 21.840038299560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926822900772095 2.262871026992798 24.4213924407959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792387843132019 2.404606342315674 25.838449478149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929235696792603 2.2332372665405273 24.125295639038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792601466178894 2.3059065341949463 24.851665496826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791101336479187 2.2404162883758545 24.19526481628418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926902770996094 2.0893256664276123 22.68594741821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925550937652588 1.9860942363739014 21.65349769592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923974990844727 2.2621572017669678 24.413970947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923026084899902 2.0645291805267334 22.43759536743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915927171707153 2.131415605545044 23.105749130249023
  batch 20 loss: 1.7915927171707153, 2.131415605545044, 23.105749130249023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924317121505737 1.9724534749984741 21.516965866088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 2.2457587718963623 24.25000762939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922006845474243 2.0902509689331055 22.69470977783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926757335662842 2.290087938308716 24.693553924560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923717498779297 2.4175643920898438 25.968015670776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924801111221313 2.376338243484497 25.555862426757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928361892700195 2.4197423458099365 25.99026107788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 1.8509860038757324 20.301725387573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932846546173096 2.4479050636291504 26.272335052490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915188074111938 2.300772190093994 24.799240112304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542576789856 2.4100050926208496 25.892595291137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914124727249146 2.3059120178222656 24.85053253173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924145460128784 1.9897276163101196 21.68968963623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925338745117188 2.2423532009124756 24.216066360473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924894094467163 2.315141201019287 24.94390296936035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792588233947754 2.3660569190979004 25.453155517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925119400024414 2.193486452102661 23.727375030517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 2.193037509918213 23.72199821472168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921395301818848 2.30155086517334 24.807647705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.2533419132232666 24.3250732421875
  batch 40 loss: 1.7916545867919922, 2.2533419132232666, 24.3250732421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921593189239502 2.371807336807251 25.51023292541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923731803894043 1.6964064836502075 18.756439208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920349836349487 2.086818218231201 22.66021728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 2.049778938293457 22.289405822753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462706565857 2.045626163482666 22.24872398376465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921515703201294 2.118072986602783 22.972881317138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920507192611694 2.325732707977295 25.049379348754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919009923934937 2.297341823577881 24.76531982421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 1.970508098602295 21.496686935424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920345067977905 2.003962755203247 21.831661224365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791084885597229 2.3872523307800293 25.66360855102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.251340627670288 24.305740356445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922180891036987 2.1148386001586914 22.940603256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925431728363037 1.8238756656646729 20.031299591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.01869797706604 21.97881507873535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925273180007935 1.9390544891357422 21.183073043823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926859855651855 2.3589935302734375 25.38262176513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926658391952515 2.2682790756225586 24.47545623779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930821180343628 2.4831087589263916 26.624168395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921223640441895 2.2248902320861816 24.04102325439453
  batch 60 loss: 1.7921223640441895, 2.2248902320861816, 24.04102325439453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927080392837524 2.299323320388794 24.785940170288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916831970214844 2.1754684448242188 23.546367645263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792517066001892 2.2023942470550537 23.81645965576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.325472593307495 25.046838760375977
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792648434638977 1.855461597442627 20.34726333618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.1564862728118896 23.35672378540039
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 2.1966633796691895 23.758407592773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915685176849365 1.866856336593628 20.460132598876953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918952703475952 1.899470567703247 20.78660011291504
Total LOSS train 23.75956623370831 valid 22.090466022491455
CE LOSS train 1.7922315542514509 valid 0.4479738175868988
Contrastive LOSS train 2.1967334839013906 valid 0.47486764192581177
EPOCH 252:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922110557556152 2.17171311378479 23.509342193603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915457487106323 2.5596823692321777 27.388370513916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924482822418213 1.8480808734893799 20.273256301879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923533916473389 2.2435007095336914 24.227359771728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920539379119873 2.1427512168884277 23.219566345214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928386926651 2.3424065113067627 25.216903686523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918397188186646 2.155639410018921 23.348234176635742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 2.109440326690674 22.886301040649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172945022583 1.7193551063537598 18.985279083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917402982711792 2.0094943046569824 21.88668441772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926820516586304 2.288170099258423 24.67438316345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923873662948608 2.4770071506500244 26.562458038330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792923092842102 2.2891032695770264 24.683956146240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792601227760315 2.277414560317993 24.56674575805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911012172698975 2.2721176147460938 24.512277603149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926898002624512 2.2226922512054443 24.01961326599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.3028178215026855 24.82073402404785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397141456604 2.466134548187256 26.45374298095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923027276992798 2.027435302734375 22.0666561126709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915924787521362 2.1568307876586914 23.359899520874023
  batch 20 loss: 1.7915924787521362, 2.1568307876586914, 23.359899520874023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924315929412842 2.0964386463165283 22.756816864013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419672012329 2.1583642959594727 23.376062393188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922004461288452 1.8159396648406982 19.951597213745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792675256729126 2.1951584815979004 23.744258880615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923716306686401 2.430234670639038 26.09471893310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924796342849731 2.134169578552246 23.13417625427246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928358316421509 2.303873300552368 24.831567764282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918645143508911 2.0855696201324463 22.647560119628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932840585708618 2.0114569664001465 21.907854080200195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915185689926147 2.151179075241089 23.30331039428711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925420999526978 2.3627564907073975 25.420106887817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914118766784668 2.310081958770752 24.892230987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924141883850098 2.3250715732574463 25.043128967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253351688385 2.2334723472595215 24.12725830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924890518188477 2.361069440841675 25.403182983398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925878763198853 2.281634569168091 24.608932495117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925111055374146 2.2168996334075928 23.96150779724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916237115859985 2.225917100906372 24.05079460144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921385765075684 2.2641212940216064 24.433351516723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 1.9189883470535278 20.981538772583008
  batch 40 loss: 1.7916537523269653, 1.9189883470535278, 20.981538772583008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792158603668213 2.219960927963257 23.99176788330078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923725843429565 2.065871000289917 22.451082229614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 2.4291152954101562 26.083187103271484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791616678237915 2.0759172439575195 22.55078887939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924625873565674 2.1543092727661133 23.335556030273438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921509742736816 2.2346253395080566 24.138402938842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920503616333008 2.2240536212921143 24.03258514404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919005155563354 2.046968698501587 22.261587142944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916052341461182 2.17471981048584 23.538803100585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792034387588501 2.0307679176330566 22.099712371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79108464717865 2.2667877674102783 24.458961486816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923334836959839 2.197237968444824 23.764713287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922179698944092 1.9196382761001587 20.98859977722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.29403018951416 24.732845306396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791834831237793 2.473097562789917 26.522811889648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925270795822144 2.190484046936035 23.69736671447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792685866355896 2.319333791732788 24.986024856567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926654815673828 2.26149320602417 24.4075984954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930819988250732 2.3708906173706055 25.50198745727539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921218872070312 2.1430397033691406 23.222518920898438
  batch 60 loss: 1.7921218872070312, 2.1430397033691406, 23.222518920898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927080392837524 2.1337008476257324 23.129716873168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.195183515548706 23.743518829345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925165891647339 2.1505215167999268 23.297731399536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921124696731567 2.1659505367279053 23.451618194580078
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926479578018188 1.8808519840240479 20.601167678833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918596267700195 2.1550447940826416 23.342308044433594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791773796081543 2.276451826095581 24.556293487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791568398475647 2.161571741104126 23.407285690307617
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918950319290161 1.8283559083938599 20.075454711914062
Total LOSS train 23.728036528367262 valid 22.845335483551025
CE LOSS train 1.7922312186314509 valid 0.44797375798225403
Contrastive LOSS train 2.193580539409931 valid 0.45708897709846497
EPOCH 253:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922109365463257 2.241554021835327 24.207752227783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915457487106323 2.5939080715179443 27.730627059936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924480438232422 2.073627233505249 22.52872085571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923530340194702 2.1192221641540527 22.984575271606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920536994934082 2.158390522003174 23.375957489013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928383350372314 2.1081416606903076 22.874256134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.284496545791626 24.636804580688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898250579834 2.136761426925659 23.159513473510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.2805428504943848 24.597156524658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 2.229642629623413 24.088167190551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926818132400513 2.6080474853515625 27.873157501220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923873662948608 2.339860677719116 25.190994262695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929227352142334 2.1719000339508057 23.51192283630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926005125045776 2.21700119972229 23.96261215209961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911009788513184 2.3549091815948486 25.340192794799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792689561843872 2.376715660095215 25.559846878051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792554497718811 2.1925714015960693 23.71826934814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923970222473145 2.401200294494629 25.804399490356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923024892807007 2.0200366973876953 21.9926700592041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915922403335571 2.1481130123138428 23.272722244262695
  batch 20 loss: 1.7915922403335571, 2.1481130123138428, 23.272722244262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792431354522705 2.0657765865325928 22.450197219848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924193143844604 2.208355665206909 23.8759765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922000885009766 2.0128209590911865 21.92041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926747798919678 2.1266047954559326 23.05872344970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923710346221924 2.2097597122192383 23.889968872070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924792766571045 2.2017810344696045 23.81028938293457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928354740142822 2.3389930725097656 25.18276596069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 1.9481757879257202 21.273622512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932838201522827 1.9356352090835571 21.149635314941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915184497833252 2.3729729652404785 25.52124786376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.3347063064575195 25.139604568481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914115190505981 1.8722710609436035 20.514122009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924138307571411 2.2547621726989746 24.340036392211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533278465271 2.11613392829895 22.953872680664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792488932609558 2.252410888671875 24.31659698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925877571105957 2.2198100090026855 23.99068832397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792510986328125 2.2194321155548096 23.986831665039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916233539581299 2.025023937225342 22.0418643951416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921385765075684 2.2276241779327393 24.06838035583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916536331176758 2.24566388130188 24.248291015625
  batch 40 loss: 1.7916536331176758, 2.24566388130188, 24.248291015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921583652496338 2.063868284225464 22.43084144592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372465133667 1.8730169534683228 20.522541046142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792034387588501 2.3034911155700684 24.826946258544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916165590286255 2.137593984603882 23.167556762695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924621105194092 2.001753330230713 21.809993743896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921507358551025 2.1754493713378906 23.54664421081543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920500040054321 1.9652663469314575 21.444713592529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919007539749146 2.1358373165130615 23.1502742767334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.19632625579834 23.754867553710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920345067977905 1.9217664003372192 21.00969886779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910850048065186 2.2438430786132812 24.229515075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923334836959839 1.8628629446029663 20.420963287353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922180891036987 2.143888235092163 23.23110008239746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.1891961097717285 23.68450355529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.412604570388794 25.91788101196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925269603729248 1.9211342334747314 21.003868103027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926859855651855 2.296905517578125 24.761741638183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926656007766724 2.165027141571045 23.44293785095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930817604064941 2.3863744735717773 25.65682601928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921222448349 2.1552212238311768 23.34433364868164
  batch 60 loss: 1.7921222448349, 2.1552212238311768, 23.34433364868164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792708158493042 2.147202968597412 23.264738082885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.202915668487549 23.820838928222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792516827583313 1.9483596086502075 21.276113510131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.177093029022217 23.56304359436035
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792648196220398 1.702212929725647 18.814777374267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.4177443981170654 25.96930503845215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 2.294373035430908 24.735504150390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 2.115574836730957 22.947317123413086
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918952703475952 1.9000182151794434 20.792078018188477
Total LOSS train 23.480626208965596 valid 23.611051082611084
CE LOSS train 1.7922310719123253 valid 0.4479738175868988
Contrastive LOSS train 2.168839509670551 valid 0.47500455379486084
EPOCH 254:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922111749649048 2.387775421142578 25.669965744018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915458679199219 2.680229425430298 28.593839645385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924482822418213 2.2543294429779053 24.335742950439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923531532287598 2.3164141178131104 24.956493377685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920538187026978 2.0926332473754883 22.718385696411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792838454246521 2.2077300548553467 23.870140075683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839599609375 2.4552001953125 26.343841552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 2.0391321182250977 22.18321990966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.1675703525543213 23.467432022094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.1645238399505615 23.436979293823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926818132400513 2.445080280303955 26.243484497070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923877239227295 2.3381311893463135 25.17369842529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929232120513916 2.11323618888855 22.92528533935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926008701324463 2.2859609127044678 24.652210235595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911015748977661 1.986977219581604 21.660873413085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792689561843872 2.055980920791626 22.35249900817871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925549745559692 2.3033511638641357 24.826066970825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792397141456604 2.2681241035461426 24.473636627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923027276992798 2.005645513534546 21.848758697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915927171707153 2.331512689590454 25.106719970703125
  batch 20 loss: 1.7915927171707153, 2.331512689590454, 25.106719970703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924315929412842 2.1087796688079834 22.88022804260254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419672012329 2.164724111557007 23.439661026000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922005653381348 1.986816644668579 21.66036605834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926750183105469 2.1020190715789795 22.8128662109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792371392250061 2.573699951171875 27.52937126159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924795150756836 2.248473644256592 24.277217864990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928353548049927 2.2677268981933594 24.470104217529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918647527694702 2.137211322784424 23.163976669311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932838201522827 2.362170457839966 25.414987564086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915188074111938 2.25339937210083 24.325511932373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925422191619873 2.38116192817688 25.60416030883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791412115097046 2.3133552074432373 24.924964904785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924140691757202 2.216550350189209 23.957918167114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253351688385 2.1756250858306885 23.548784255981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924891710281372 2.5259978771209717 27.05246925354004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925881147384644 2.3741884231567383 25.53447151184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792511224746704 2.2079038619995117 23.871549606323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916237115859985 2.2161710262298584 23.95333480834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921390533447266 2.36562180519104 25.44835662841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.2066867351531982 23.858522415161133
  batch 40 loss: 1.791654109954834, 2.2066867351531982, 23.858522415161133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792159080505371 2.2280125617980957 24.072284698486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923728227615356 2.057708740234375 22.369461059570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920349836349487 2.3131155967712402 24.92319107055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.2444241046905518 24.235858917236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462944984436 2.1136369705200195 22.9288330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921513319015503 2.3022148609161377 24.814298629760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 2.12390398979187 23.031091690063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791900873184204 2.2324416637420654 24.116317749023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.2837862968444824 24.62946891784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 2.06796932220459 22.47172737121582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910852432250977 2.2504827976226807 24.295913696289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.225774049758911 24.050073623657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792218565940857 2.213341236114502 23.925630569458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.0582144260406494 22.37468719482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.2323718070983887 24.115554809570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925273180007935 2.0605225563049316 22.39775276184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792686104774475 2.3923747539520264 25.716434478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926654815673828 2.1404497623443604 23.197162628173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930819988250732 2.299952745437622 24.7926082611084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921223640441895 2.108083963394165 22.872961044311523
  batch 60 loss: 1.7921223640441895, 2.108083963394165, 22.872961044311523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792708158493042 2.162074565887451 23.413454055786133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916837930679321 2.1497647762298584 23.289331436157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792517066001892 2.067974328994751 22.472259521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792112946510315 2.2999026775360107 24.791139602661133
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792648196220398 1.8055572509765625 19.848220825195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918598651885986 1.912440538406372 20.9162654876709
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 1.8210924863815308 20.00269889831543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915692329406738 1.5876978635787964 17.668548583984375
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918957471847534 1.5622916221618652 17.414812088012695
Total LOSS train 24.026366365872896 valid 19.00058126449585
CE LOSS train 1.7922313671845655 valid 0.44797393679618835
Contrastive LOSS train 2.223413502253019 valid 0.3905729055404663
EPOCH 255:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922112941741943 1.999308705329895 21.78529930114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79154634475708 2.3301820755004883 25.093366622924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79244863986969 2.087702751159668 22.669475555419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923535108566284 2.284980535507202 24.64215850830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920541763305664 2.2527756690979004 24.319808959960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928385734558105 2.2744076251983643 24.536914825439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918400764465332 2.4094996452331543 25.886837005615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918986082077026 2.146249294281006 23.254392623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.186814308166504 23.65987205505371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.0855584144592285 22.647323608398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926819324493408 2.36788272857666 25.47150993347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923879623413086 2.4435713291168213 26.228099822998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792922854423523 2.376991033554077 25.562833786010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926013469696045 2.0586049556732178 22.378650665283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911019325256348 2.2932381629943848 24.723482131958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926896810531616 2.408076524734497 25.873455047607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.3490424156188965 25.28297996520996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923972606658936 2.396902322769165 25.76141929626465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923028469085693 2.1662230491638184 23.45453453063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915929555892944 2.1693618297576904 23.485212326049805
  batch 20 loss: 1.7915929555892944, 2.1693618297576904, 23.485212326049805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924315929412842 2.100785970687866 22.800291061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924195528030396 2.124840497970581 23.04082489013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922005653381348 1.7001396417617798 18.793596267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926750183105469 2.06248140335083 22.41748809814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923715114593506 2.0005030632019043 21.79740333557129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924795150756836 2.1392781734466553 23.185260772705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928354740142822 2.369654893875122 25.489383697509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 2.1242589950561523 23.034454345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932833433151245 2.097583532333374 22.769119262695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915194034576416 2.1009323596954346 22.80084228515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 2.3850724697113037 25.643268585205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914127111434937 2.356961250305176 25.361024856567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924144268035889 2.2608823776245117 24.40123748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925338745117188 2.0524377822875977 22.316911697387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924894094467163 2.4265329837799072 26.057819366455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925878763198853 2.153287649154663 23.325464248657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925115823745728 2.0203702449798584 21.996213912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 2.0864100456237793 22.655725479125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921394109725952 2.375762939453125 25.549768447875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.4026474952697754 25.81812858581543
  batch 40 loss: 1.7916545867919922, 2.4026474952697754, 25.81812858581543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921591997146606 2.2600889205932617 24.393049240112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923729419708252 1.8528571128845215 20.320945739746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792035460472107 2.272231340408325 24.51434898376465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.1255476474761963 23.047094345092773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462706565857 2.048306465148926 22.275527954101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921512126922607 2.2620744705200195 24.41289520263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 2.145476818084717 23.2468204498291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919011116027832 2.1545302867889404 23.337203979492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916059494018555 2.2352700233459473 24.144306182861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920348644256592 2.102846384048462 22.820497512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 2.4447760581970215 26.238847732543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.3375754356384277 25.168088912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922179698944092 1.5918240547180176 17.710458755493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542815208435 2.2365431785583496 24.157976150512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918355464935303 2.189249277114868 23.684328079223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925273180007935 2.0500383377075195 22.292911529541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 2.44679856300354 26.260671615600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926656007766724 2.1824686527252197 23.617351531982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930817604064941 2.4370429515838623 26.163511276245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921226024627686 2.2145049571990967 23.937171936035156
  batch 60 loss: 1.7921226024627686, 2.2145049571990967, 23.937171936035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792708396911621 2.2209582328796387 24.00229263305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.1160523891448975 22.952207565307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792517066001892 2.252690076828003 24.31941795349121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792113184928894 2.2483291625976562 24.27540397644043
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792648196220398 1.93162202835083 21.108867645263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918601036071777 2.4132795333862305 25.92465591430664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 2.2314789295196533 24.106563568115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915693521499634 2.0163445472717285 21.955013275146484
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918957471847534 1.952252745628357 21.314422607421875
Total LOSS train 23.759600771390474 valid 23.32516384124756
CE LOSS train 1.7922315010657677 valid 0.44797393679618835
Contrastive LOSS train 2.196736922630897 valid 0.48806318640708923
EPOCH 256:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922114133834839 2.251337766647339 24.30558967590332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915464639663696 2.3319945335388184 25.111492156982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924484014511108 2.1791558265686035 23.58400535583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792353630065918 2.2476305961608887 24.268661499023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792054295539856 2.3218958377838135 25.01101303100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928385734558105 2.061722755432129 22.410066604614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918399572372437 2.376641035079956 25.558250427246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.1028149127960205 22.82004737854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.1570091247558594 23.3618221282959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791740894317627 2.10864520072937 22.878192901611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926818132400513 1.98489511013031 21.641633987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79238760471344 2.4177663326263428 25.970050811767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792923092842102 2.2527406215667725 24.320329666137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926009893417358 2.2872493267059326 24.66509437561035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911020517349243 2.2849111557006836 24.640213012695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926896810531616 2.1956098079681396 23.748788833618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925547361373901 2.157980442047119 23.372358322143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923972606658936 2.2231597900390625 24.02399444580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923027276992798 1.9177182912826538 20.969486236572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915928363800049 1.9991689920425415 21.783283233642578
  batch 20 loss: 1.7915928363800049, 1.9991689920425415, 21.783283233642578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924318313598633 2.1008644104003906 22.801074981689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924195528030396 2.152961015701294 23.32202911376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922008037567139 2.138058662414551 23.172786712646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792675256729126 2.2006123065948486 23.798797607421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923716306686401 2.4035696983337402 25.82806968688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924797534942627 2.335773468017578 25.15021514892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928355932235718 2.1595141887664795 23.387977600097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918648719787598 1.9855936765670776 21.64780044555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932837009429932 2.068615674972534 22.479440689086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519284248352 2.045203685760498 22.24355697631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 2.3779373168945312 25.571914672851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914124727249146 2.32574200630188 25.048831939697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924144268035889 2.263288736343384 24.42530059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253351688385 2.2972843647003174 24.765377044677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924891710281372 2.4769577980041504 26.56206703186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925878763198853 2.3875482082366943 25.66806983947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925111055374146 2.1592209339141846 23.384719848632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.2780869007110596 24.572492599487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921390533447266 2.3242690563201904 25.03483009338379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.11222505569458 22.913904190063477
  batch 40 loss: 1.791654348373413, 2.11222505569458, 22.913904190063477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921589612960815 1.775131344795227 19.543472290039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923728227615356 1.9671014547348022 21.463388442993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920351028442383 2.2213568687438965 24.005603790283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916173934936523 2.1482932567596436 23.274551391601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462706565857 2.217883825302124 23.971302032470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921510934829712 2.099860191345215 22.790752410888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 1.9229695796966553 21.021747589111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919011116027832 2.2472805976867676 24.264705657958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.285062074661255 24.64222526550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 2.137104034423828 23.163074493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910854816436768 2.3677446842193604 25.46853256225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.2489709854125977 24.28204345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922180891036987 2.0221378803253174 22.013595581054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792542815208435 2.1502532958984375 23.295076370239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918355464935303 2.206038475036621 23.85222053527832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792527198791504 2.0410044193267822 22.202571868896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 2.4701037406921387 26.493724822998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926653623580933 2.1015689373016357 22.8083553314209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930816411972046 2.1878325939178467 23.67140769958496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792122483253479 2.303546190261841 24.82758331298828
  batch 60 loss: 1.792122483253479, 2.303546190261841, 24.82758331298828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927080392837524 2.261242151260376 24.405128479003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916836738586426 2.2209057807922363 24.00074005126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925169467926025 2.083878993988037 22.63130760192871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.23144268989563 24.106538772583008
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926479578018188 1.559887409210205 17.391521453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.2139618396759033 23.93147850036621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917745113372803 2.169548511505127 23.487258911132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915692329406738 1.866589069366455 20.45745849609375
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918955087661743 1.9269040822982788 21.060935974121094
Total LOSS train 23.627920033381535 valid 22.234282970428467
CE LOSS train 1.7922314405441284 valid 0.4479738771915436
Contrastive LOSS train 2.18356886276832 valid 0.4817260205745697
EPOCH 257:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922111749649048 2.3442115783691406 25.23432731628418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79154634475708 2.4443328380584717 26.234874725341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79244863986969 2.1251962184906006 23.044410705566406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792353630065918 2.2994868755340576 24.78722381591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920541763305664 2.2674810886383057 24.46686553955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928383350372314 2.1372153759002686 23.164993286132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918399572372437 2.2085986137390137 23.877826690673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.1746175289154053 23.538074493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.231712579727173 24.108856201171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.121422052383423 23.00596046447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926818132400513 2.5356647968292236 27.149330139160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923877239227295 2.397995710372925 25.7723445892334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792922854423523 2.1293387413024902 23.08631134033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926007509231567 2.2718937397003174 24.511537551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911020517349243 2.3168323040008545 24.95942497253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926898002624512 2.341273307800293 25.20542335510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925550937652588 2.2436931133270264 24.2294864654541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923974990844727 2.1494154930114746 23.28655242919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792303204536438 2.153252601623535 23.3248291015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915934324264526 2.1739518642425537 23.531112670898438
  batch 20 loss: 1.7915934324264526, 2.1739518642425537, 23.531112670898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924318313598633 1.7191541194915771 18.98397445678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924197912216187 2.342027187347412 25.212692260742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922008037567139 1.8417798280715942 20.209999084472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926753759384155 2.040181875228882 22.194494247436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923716306686401 2.439286231994629 26.18523406982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924797534942627 2.305544853210449 24.847929000854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928355932235718 2.458094835281372 26.373783111572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918651103973389 1.990246057510376 21.694324493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793283462524414 2.251582622528076 24.309110641479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915195226669312 2.182652235031128 23.6180419921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 2.132323980331421 23.11578369140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914127111434937 2.153841495513916 23.32982635498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924145460128784 2.1632227897644043 23.42464256286621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925337553024292 2.1629951000213623 23.4224853515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924894094467163 2.2852821350097656 24.64531135559082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925881147384644 2.353015899658203 25.32274627685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925113439559937 2.10811185836792 22.87363052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 2.191166877746582 23.703292846679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921391725540161 2.2401206493377686 24.19334602355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.2439751625061035 24.23140525817871
  batch 40 loss: 1.7916545867919922, 2.2439751625061035, 24.23140525817871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921591997146606 2.285447120666504 24.646631240844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923730611801147 2.072434902191162 22.5167236328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920355796813965 2.3033573627471924 24.82560920715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617751121521 2.3360390663146973 25.152008056640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792462706565857 2.117887258529663 22.971336364746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921512126922607 1.9586433172225952 21.378583908081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792050838470459 1.9549907445907593 21.34195899963379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919009923934937 2.029855251312256 22.0904541015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791606068611145 2.323848247528076 25.03009033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920349836349487 2.1110427379608154 22.902462005615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910857200622559 1.9826627969741821 21.617713928222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792333722114563 2.310203790664673 24.894371032714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922180891036987 2.0209827423095703 22.002044677734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.1387476921081543 23.180021286010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918351888656616 2.3696374893188477 25.488210678100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925267219543457 2.138554334640503 23.178070068359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 2.39247989654541 25.717485427856445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926651239395142 2.013087034225464 21.92353630065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930810451507568 2.3836233615875244 25.629314422607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792122483253479 2.231365203857422 24.10577392578125
  batch 60 loss: 1.792122483253479, 2.231365203857422, 24.10577392578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927074432373047 2.222327470779419 24.015981674194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916836738586426 2.097921848297119 22.77090072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925163507461548 2.0213630199432373 22.006147384643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921123504638672 2.333827257156372 25.13038444519043
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792647361755371 1.775882363319397 19.551471710205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918596267700195 2.140934467315674 23.201202392578125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.075953722000122 22.551311492919922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 1.827086329460144 20.06243133544922
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918950319290161 1.7413705587387085 19.20560073852539
Total LOSS train 23.73044776916504 valid 21.255136489868164
CE LOSS train 1.7922314753899207 valid 0.44797375798225403
Contrastive LOSS train 2.1938216081032387 valid 0.4353426396846771
EPOCH 258:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922109365463257 2.1103687286376953 22.895898818969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915459871292114 2.4450907707214355 26.242454528808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924481630325317 2.111180305480957 22.904251098632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923531532287598 2.2781221866607666 24.57357406616211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920541763305664 2.2405314445495605 24.197368621826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928380966186523 2.3217902183532715 25.0107421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918399572372437 2.424959897994995 26.041439056396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.177358388900757 23.56548309326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 2.1842591762542725 23.634323120117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791740894317627 2.0884287357330322 22.676027297973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926814556121826 2.44008731842041 26.193553924560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923872470855713 2.3919527530670166 25.7119140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929222583770752 2.3077552318573 24.87047576904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926006317138672 2.362830877304077 25.420909881591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911022901535034 2.4013512134552 25.80461311340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926894426345825 2.600249767303467 27.795188903808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792554259300232 2.385179042816162 25.644346237182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923970222473145 2.3422293663024902 25.214691162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923024892807007 1.9677239656448364 21.46954345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593313217163 2.0169920921325684 21.961515426635742
  batch 20 loss: 1.791593313217163, 2.0169920921325684, 21.961515426635742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924312353134155 1.9601902961730957 21.394332885742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924190759658813 2.033564329147339 22.128063201904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922004461288452 1.8520863056182861 20.31306266784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926745414733887 2.1453211307525635 23.245885848999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923712730407715 2.3778650760650635 25.571022033691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924792766571045 2.1723906993865967 23.516386032104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928351163864136 2.28403902053833 24.633224487304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918648719787598 2.051368474960327 22.30554962158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932827472686768 2.147360324859619 23.26688575744629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915194034576416 2.0689635276794434 22.481155395507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.413877010345459 25.931312561035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791412591934204 2.229320764541626 24.084619522094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924139499664307 2.282412052154541 24.616533279418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533278465271 2.229166269302368 24.084196090698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792488694190979 2.470647096633911 26.498958587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792587399482727 2.2538304328918457 24.330890655517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925108671188354 2.3159093856811523 24.95160484313965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 2.2202367782592773 23.99399185180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792138934135437 2.159796714782715 23.390106201171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.0532219409942627 22.32387351989746
  batch 40 loss: 1.7916544675827026, 2.0532219409942627, 22.32387351989746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792158603668213 2.309910774230957 24.891265869140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923723459243774 1.823721170425415 20.029582977294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 1.9393330812454224 21.185365676879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.296088695526123 24.752506256103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924619913101196 2.2087020874023438 23.87948226928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792150855064392 2.212033271789551 23.91248321533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 1.9894907474517822 21.68695831298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919007539749146 1.987241506576538 21.664316177368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916059494018555 2.2463760375976562 24.255367279052734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920345067977905 2.132622718811035 23.118261337280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791085958480835 2.2398736476898193 24.189823150634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923331260681152 2.1824464797973633 23.616798400878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922180891036987 2.0091254711151123 21.883472442626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.2747082710266113 24.53962516784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918356657028198 2.385836601257324 25.65020179748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925270795822144 2.280909538269043 24.601621627807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792685627937317 2.1909077167510986 23.701763153076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926653623580933 2.156562089920044 23.358285903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930814027786255 2.4922711849212646 26.71579360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921226024627686 2.1786372661590576 23.578495025634766
  batch 60 loss: 1.7921226024627686, 2.1786372661590576, 23.578495025634766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927075624465942 2.175506353378296 23.547771453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916842699050903 2.0107553005218506 21.89923858642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925167083740234 1.9812053442001343 21.604570388793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.317493438720703 24.9670467376709
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926478385925293 1.8381192684173584 20.17384147644043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918602228164673 2.331519842147827 25.107059478759766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917747497558594 2.0636539459228516 22.428314208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915695905685425 1.8870011568069458 20.66158103942871
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918955087661743 1.9070333242416382 20.862228393554688
Total LOSS train 23.819906264085034 valid 22.264795780181885
CE LOSS train 1.7922312296353853 valid 0.4479738771915436
Contrastive LOSS train 2.202767495008615 valid 0.47675833106040955
EPOCH 259:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922114133834839 2.1492457389831543 23.284669876098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915468215942383 2.4975318908691406 26.766864776611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924482822418213 2.2173943519592285 23.96639060974121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923532724380493 2.2849740982055664 24.642093658447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920544147491455 2.1341142654418945 23.133197784423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928379774093628 2.240838050842285 24.201217651367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918401956558228 2.3171513080596924 24.96335220336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918983697891235 2.2133069038391113 23.92496681213379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.027557611465454 22.067306518554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917410135269165 2.0716660022735596 22.508399963378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926814556121826 2.3758857250213623 25.551538467407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79238760471344 2.4271817207336426 26.0642032623291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929222583770752 2.3250327110290527 25.043251037597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926006317138672 2.4875543117523193 26.66814422607422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911025285720825 2.4436633586883545 26.227737426757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926894426345825 2.2993006706237793 24.78569793701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925543785095215 2.2689127922058105 24.4816837310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923967838287354 2.295165538787842 24.74405288696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923024892807007 2.07608962059021 22.553199768066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915931940078735 2.089085340499878 22.68244743347168
  batch 20 loss: 1.7915931940078735, 2.089085340499878, 22.68244743347168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924312353134155 2.1434693336486816 23.227123260498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419195175171 2.1469805240631104 23.262224197387695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922002077102661 1.8602815866470337 20.395015716552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926744222640991 2.3008320331573486 24.800994873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792371153831482 2.5770349502563477 27.562721252441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924787998199463 2.309647560119629 24.888954162597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928346395492554 2.335644245147705 25.149276733398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918649911880493 2.010572671890259 21.89759063720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932825088500977 2.358130693435669 25.374588012695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519284248352 2.2086267471313477 23.87778663635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925419807434082 2.052600145339966 22.31854248046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914127111434937 2.4417855739593506 26.20926856994629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924139499664307 2.1097779273986816 22.89019203186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533040046692 2.3841538429260254 25.634071350097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924884557724 2.4815216064453125 26.607704162597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925870418548584 2.13041615486145 23.09674835205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925106287002563 2.094007730484009 22.732587814331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916240692138672 2.1161415576934814 22.953039169311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792138695716858 2.298480749130249 24.776947021484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.201401948928833 23.805673599243164
  batch 40 loss: 1.7916542291641235, 2.201401948928833, 23.805673599243164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792158603668213 2.222142457962036 24.013582229614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372226715088 1.9619001150131226 21.411373138427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920348644256592 2.263880491256714 24.43083953857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916172742843628 2.1105549335479736 22.897165298461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924621105194092 2.1456141471862793 23.24860382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921507358551025 2.164679765701294 23.438947677612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 2.329561948776245 25.087671279907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919011116027832 2.2812132835388184 24.604034423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 2.2629075050354004 24.420679092407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792034387588501 2.080202102661133 22.59405517578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791085958480835 2.315202236175537 24.9431095123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923331260681152 2.2529618740081787 24.32195281982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922176122665405 2.025607109069824 22.048288345336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925423383712769 2.1640875339508057 23.43341636657715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918355464935303 2.406155824661255 25.8533935546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925268411636353 2.276472330093384 24.557249069213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926851511001587 2.3884077072143555 25.676761627197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792664885520935 2.2725303173065186 24.517969131469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930810451507568 2.4609861373901367 26.402942657470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921221256256104 2.2515015602111816 24.30713653564453
  batch 60 loss: 1.7921221256256104, 2.2515015602111816, 24.30713653564453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927073240280151 2.295884609222412 24.751554489135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.152418851852417 23.315872192382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925162315368652 1.9732455015182495 21.52497100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.1038477420806885 22.830589294433594
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926476001739502 1.5701550245285034 17.494197845458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.108060121536255 22.872461318969727
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917742729187012 2.0533316135406494 22.325090408325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915693521499634 1.9098972082138062 20.890541076660156
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918951511383057 1.7386950254440308 19.178844451904297
Total LOSS train 24.028428033682015 valid 21.316734313964844
CE LOSS train 1.7922311287659864 valid 0.4479737877845764
Contrastive LOSS train 2.2236197031461273 valid 0.4346737563610077
EPOCH 260:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922109365463257 2.125032663345337 23.042537689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915462255477905 2.5135657787323 26.927204132080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924476861953735 2.1527600288391113 23.32004737854004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923530340194702 2.1491100788116455 23.2834529876709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920539379119873 2.2005536556243896 23.797590255737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928375005722046 2.199336290359497 23.78619956970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918399572372437 2.1706483364105225 23.498323440551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898250579834 1.9989596605300903 21.781496047973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917298078536987 2.16898250579834 23.48155403137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917406558990479 2.1081395149230957 22.87313461303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792681097984314 2.421333074569702 26.006011962890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923871278762817 2.549863338470459 27.2910213470459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929219007492065 2.248788595199585 24.280807495117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600393295288 2.226996898651123 24.062570571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911022901535034 2.4187257289886475 25.97835922241211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926887273788452 2.4060299396514893 25.85298728942871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925541400909424 2.175755500793457 23.55010986328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923965454101562 2.3414056301116943 25.206453323364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923023700714111 2.113990545272827 22.932209014892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915929555892944 2.121753692626953 23.009130477905273
  batch 20 loss: 1.7915929555892944, 2.121753692626953, 23.009130477905273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924309968948364 2.077531576156616 22.567747116088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419195175171 2.155694007873535 23.3493595123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922002077102661 1.9650386571884155 21.44258689880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926744222640991 2.115750789642334 22.950183868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923710346221924 2.465961456298828 26.45198631286621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924787998199463 2.162764310836792 23.420122146606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928345203399658 2.4935107231140137 26.727943420410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 2.2232861518859863 24.02472496032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793282389640808 2.218085289001465 23.97413444519043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915197610855103 2.235835552215576 24.14987564086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925416231155396 2.381775140762329 25.610292434692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914127111434937 1.9669842720031738 21.461254119873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924139499664307 2.164433717727661 23.436750411987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925328016281128 2.156752824783325 23.36005973815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924884557724 2.209381580352783 23.886302947998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925870418548584 2.131427526473999 23.106863021850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925106287002563 2.311728000640869 24.9097900390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 2.140045404434204 23.19207763671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921388149261475 2.1649909019470215 23.442049026489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.217247486114502 23.964128494262695
  batch 40 loss: 1.7916544675827026, 2.217247486114502, 23.964128494262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921587228775024 2.2718570232391357 24.51072883605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923723459243774 1.9614169597625732 21.40654182434082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920349836349487 2.236826181411743 24.160295486450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916176319122314 2.0507540702819824 22.29916000366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924622297286987 2.2113468647003174 23.905929565429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792150855064392 2.247530698776245 24.267457962036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920502424240112 2.1346890926361084 23.138940811157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919011116027832 1.999458909034729 21.786489486694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.1260011196136475 23.051616668701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920347452163696 2.145766496658325 23.249698638916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910866737365723 2.3832974433898926 25.624059677124023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923333644866943 2.3015379905700684 24.807714462280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79221773147583 2.021282434463501 22.005041122436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925424575805664 2.139500379562378 23.187545776367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835904121399 2.2226133346557617 24.017969131469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925266027450562 1.962347388267517 21.416000366210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926852703094482 2.3407320976257324 25.20000648498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926647663116455 2.2032294273376465 23.824960708618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930805683135986 2.4284818172454834 26.077899932861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921220064163208 2.300128698348999 24.79340934753418
  batch 60 loss: 1.7921220064163208, 2.300128698348999, 24.79340934753418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927072048187256 2.1493794918060303 23.286502838134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916837930679321 1.9960280656814575 21.751964569091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925162315368652 2.0049383640289307 21.841899871826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921127080917358 2.248661994934082 24.278732299804688
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926476001739502 1.7643768787384033 19.436416625976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918599843978882 2.304675340652466 24.838613510131836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 2.2546606063842773 24.338380813598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569471359253 1.9290608167648315 21.082176208496094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918953895568848 1.9622089862823486 21.413984298706055
Total LOSS train 23.734098698542667 valid 22.918288707733154
CE LOSS train 1.7922310425685002 valid 0.4479738473892212
Contrastive LOSS train 2.1941867699989905 valid 0.49055224657058716
EPOCH 261:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922110557556152 2.254831552505493 24.340526580810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915464639663696 2.0467538833618164 22.259084701538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924479246139526 2.2117996215820312 23.910444259643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923531532287598 2.135214328765869 23.144495010375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920539379119873 2.2145495414733887 23.937549591064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928375005722046 2.3210296630859375 25.00313377380371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839838027954 2.269075632095337 24.482595443725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918988466262817 2.2733726501464844 24.525625228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.2392799854278564 24.18453025817871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917410135269165 2.135712146759033 23.148860931396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926814556121826 2.2238388061523438 24.031068801879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923873662948608 2.2685606479644775 24.47799301147461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792921781539917 2.114903688430786 22.941957473754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792600154876709 2.310593843460083 24.89853858947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911028861999512 2.3205621242523193 24.99672508239746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926889657974243 2.239729166030884 24.189979553222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925541400909424 2.2822959423065186 24.615514755249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923967838287354 2.3899548053741455 25.691944122314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923026084899902 2.0781733989715576 22.574037551879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915935516357422 2.2084972858428955 23.87656593322754
  batch 20 loss: 1.7915935516357422, 2.2084972858428955, 23.87656593322754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924312353134155 2.212207078933716 23.914501190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792419195175171 2.1589691638946533 23.382110595703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922003269195557 2.0319201946258545 22.11140251159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926745414733887 2.2012224197387695 23.804899215698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923710346221924 2.641796588897705 28.210336685180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924792766571045 2.1498708724975586 23.291187286376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928346395492554 2.3771984577178955 25.5648193359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918648719787598 1.9952442646026611 21.744306564331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932822704315186 2.3590688705444336 25.383970260620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915196418762207 2.0820486545562744 22.61200523376465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.3041152954101562 24.833694458007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914131879806519 2.354241371154785 25.333826065063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924137115478516 2.2922561168670654 24.714975357055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925326824188232 2.1552226543426514 23.344758987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924883365631104 2.306015968322754 24.85264778137207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925869226455688 2.16545033454895 23.44709014892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792510747909546 2.229611873626709 24.08863067626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 2.146348237991333 23.25510597229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921388149261475 2.2147908210754395 23.940046310424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 2.0672049522399902 22.46370506286621
  batch 40 loss: 1.7916542291641235, 2.0672049522399902, 22.46370506286621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921587228775024 2.3644518852233887 25.436677932739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792372465133667 1.9660027027130127 21.4523983001709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920348644256592 2.376577615737915 25.557809829711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916178703308105 2.16115403175354 23.40315818786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79246187210083 1.9703583717346191 21.496044158935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921507358551025 2.1469030380249023 23.261180877685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920492887496948 2.1919102668762207 23.711151123046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.061344623565674 22.405345916748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.1635658740997314 23.427263259887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033076286316 1.9155704975128174 20.947736740112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910852432250977 2.2505428791046143 24.296512603759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923319339752197 2.4325060844421387 26.117393493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922166585922241 2.2445268630981445 24.237485885620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925409078598022 2.0713882446289062 22.506423950195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918339967727661 2.3030951023101807 24.822784423828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925255298614502 2.096538782119751 22.75791358947754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683720588684 2.399261474609375 25.78529930114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926632165908813 1.967841625213623 21.471080780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930793762207031 2.4541656970977783 26.334735870361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792121410369873 2.3334758281707764 25.126880645751953
  batch 60 loss: 1.792121410369873, 2.3334758281707764, 25.126880645751953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927058935165405 2.2138638496398926 23.93134307861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.2177860736846924 23.96954345703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925152778625488 2.0262248516082764 22.054763793945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921115159988403 2.140171766281128 23.193830490112305
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926459312438965 1.5016422271728516 16.80906867980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918589115142822 2.2235465049743652 24.027324676513672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.1627185344696045 23.41895866394043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791568636894226 1.8092103004455566 19.883670806884766
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.7542455196380615 19.3343505859375
Total LOSS train 23.78521564190204 valid 21.666076183319092
CE LOSS train 1.7922307711381178 valid 0.4479736387729645
Contrastive LOSS train 2.1992985101846547 valid 0.4385613799095154
EPOCH 262:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922098636627197 2.2063632011413574 23.85584259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915462255477905 2.2216897010803223 24.008441925048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924466133117676 1.8085469007492065 19.877914428710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923519611358643 2.247515916824341 24.26751136779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792053461074829 2.153508186340332 23.32713508605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928364276885986 2.187575340270996 23.668590545654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918392419815063 2.1906609535217285 23.698448181152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.124162197113037 23.03352165222168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.143467426300049 23.226402282714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 2.1521151065826416 23.312891006469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792679786682129 2.424996852874756 26.042648315429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923859357833862 2.3222992420196533 25.015377044677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929205894470215 2.131112575531006 23.104047775268555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792599081993103 2.2546987533569336 24.33958625793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911021709442139 2.2630879878997803 24.421981811523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926877737045288 2.3931384086608887 25.72407341003418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925528287887573 2.296247959136963 24.75503158569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923955917358398 2.361602306365967 25.40842056274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792301058769226 2.167254686355591 23.464847564697266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 2.0253634452819824 22.04522705078125
  batch 20 loss: 1.7915925979614258, 2.0253634452819824, 22.04522705078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79243004322052 2.122598886489868 23.01841926574707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924178838729858 2.2409181594848633 24.20159912109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921992540359497 2.0651233196258545 22.44343376159668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926734685897827 2.113842725753784 22.931100845336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923698425292969 2.5544371604919434 27.336742401123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 2.0868735313415527 22.66121482849121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928334474563599 2.369666337966919 25.4894962310791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864037513733 2.005540132522583 21.847265243530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932807207107544 2.1111936569213867 22.90521812438965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915186882019043 2.1895394325256348 23.686912536621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925399541854858 2.50415301322937 26.834070205688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914122343063354 2.3245325088500977 25.0367374420166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924124002456665 2.251103639602661 24.303447723388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925317287445068 2.3539204597473145 25.331735610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924870252609253 2.3014729022979736 24.807214736938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925859689712524 2.2951762676239014 24.744348526000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925093173980713 2.202881097793579 23.821319580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916237115859985 2.0038092136383057 21.829715728759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921379804611206 2.2352092266082764 24.144229888916016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 2.1480231285095215 23.271886825561523
  batch 40 loss: 1.7916537523269653, 2.1480231285095215, 23.271886825561523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921576499938965 2.32340931892395 25.0262508392334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792371392250061 1.9919549226760864 21.71192169189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920336723327637 2.280064344406128 24.59267807006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916167974472046 2.233870267868042 24.130319595336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924607992172241 2.3214666843414307 25.00712776184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921499013900757 2.2529520988464355 24.321672439575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920496463775635 2.090092658996582 22.692975997924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918999195098877 2.29524827003479 24.744382858276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.1320390701293945 23.111995697021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920335531234741 1.9517196416854858 21.30923080444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910856008529663 2.2429709434509277 24.220796585083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923322916030884 2.211479425430298 23.90712547302246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922170162200928 2.041668176651001 22.208898544311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925411462783813 2.1517443656921387 23.309986114501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.3785862922668457 25.577695846557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925255298614502 2.237626075744629 24.168787002563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683720588684 2.323828935623169 25.030973434448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792663335800171 2.0953705310821533 22.746368408203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930793762207031 2.6448073387145996 28.241153717041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921212911605835 2.2429511547088623 24.221633911132812
  batch 60 loss: 1.7921212911605835, 2.2429511547088623, 24.221633911132812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792705774307251 2.137681722640991 23.169523239135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.192345380783081 23.715137481689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925153970718384 2.3114333152770996 24.906848907470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921115159988403 2.241795778274536 24.21006965637207
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926459312438965 1.8478319644927979 20.270965576171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918590307235718 2.235013484954834 24.14199447631836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.196857452392578 23.760347366333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 1.8111697435379028 19.90326690673828
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894793510437 1.812572717666626 19.917621612548828
Total LOSS train 23.904593365009013 valid 21.93080759048462
CE LOSS train 1.7922300320405227 valid 0.44797369837760925
Contrastive LOSS train 2.2112363173411445 valid 0.4531431794166565
EPOCH 263:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922098636627197 2.1164045333862305 22.956254959106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915462255477905 2.5627596378326416 27.41914176940918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924467325210571 1.8408015966415405 20.200462341308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923517227172852 2.068808078765869 22.480430603027344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792053461074829 2.2550477981567383 24.342531204223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792836308479309 2.119929313659668 22.992130279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.143649101257324 23.228330612182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.2587506771087646 24.379405975341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.1507351398468018 23.29907989501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 2.1163198947906494 22.954938888549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680025100708 2.256531238555908 24.35799217224121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923862934112549 2.3087377548217773 24.879764556884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929205894470215 2.1002423763275146 22.795345306396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792599081993103 2.297644853591919 24.769046783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911022901535034 2.259497880935669 24.386079788208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792688012123108 2.319194793701172 24.984636306762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925530672073364 2.1255431175231934 23.047985076904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923954725265503 2.1631903648376465 23.424299240112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923012971878052 2.216163396835327 23.953935623168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915928363800049 2.182560920715332 23.617202758789062
  batch 20 loss: 1.7915928363800049, 2.182560920715332, 23.617202758789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924301624298096 2.1448538303375244 23.240968704223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924182415008545 2.174309730529785 23.53551483154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921993732452393 2.107377290725708 22.8659725189209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926735877990723 2.246013879776001 24.252811431884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792370080947876 2.384075164794922 25.633121490478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924779653549194 2.2072601318359375 23.865079879760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928334474563599 2.224670886993408 24.039541244506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.020352840423584 21.995393753051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932809591293335 2.205448865890503 23.84777069091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519045829773 2.249645948410034 24.287979125976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.4428746700286865 26.22128677368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791412353515625 2.447573661804199 26.267148971557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924127578735352 2.240858793258667 24.201000213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925316095352173 2.2780418395996094 24.57295036315918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924875020980835 2.4137868881225586 25.930356979370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925859689712524 2.3191637992858887 24.984224319458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925094366073608 2.293454170227051 24.72705078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 1.9676679372787476 21.468303680419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921380996704102 2.4343101978302 26.135238647460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 2.196011781692505 23.751771926879883
  batch 40 loss: 1.7916537523269653, 2.196011781692505, 23.751771926879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921576499938965 1.907321572303772 20.865373611450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923712730407715 1.7327550649642944 19.119922637939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920340299606323 2.294961452484131 24.741649627685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 2.151663303375244 23.30824851989746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924610376358032 2.0140693187713623 21.933155059814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921497821807861 2.296818971633911 24.760339736938477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920492887496948 2.069779634475708 22.489845275878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.357570171356201 25.367603302001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916052341461182 2.17815899848938 23.57319450378418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920335531234741 2.0689918994903564 22.481952667236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910858392715454 2.3480310440063477 25.27139663696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923321723937988 2.2179360389709473 23.971691131591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922167778015137 1.9626649618148804 21.418867111206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925410270690918 2.1553051471710205 23.345592498779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918344736099243 2.203458547592163 23.826419830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792525291442871 2.1673927307128906 23.466453552246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.376912832260132 25.561811447143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926632165908813 2.0790019035339355 22.582683563232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793078899383545 2.5321080684661865 27.114160537719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921212911605835 2.256807565689087 24.360197067260742
  batch 60 loss: 1.7921212911605835, 2.256807565689087, 24.360197067260742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927056550979614 2.2862277030944824 24.654983520507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.2795910835266113 24.58759307861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925153970718384 1.8471304178237915 20.263818740844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921115159988403 2.18949818611145 23.68709373474121
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926456928253174 1.4074922800064087 15.867568016052246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918591499328613 2.238163709640503 24.17349624633789
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791773796081543 2.158801794052124 23.379791259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 2.0273280143737793 22.064849853515625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918949127197266 1.8262585401535034 20.054479598999023
Total LOSS train 23.6756014750554 valid 22.41815423965454
CE LOSS train 1.792230090728173 valid 0.44797372817993164
Contrastive LOSS train 2.1883371334809523 valid 0.45656463503837585
EPOCH 264:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922101020812988 2.214817523956299 23.940383911132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79154634475708 2.5242562294006348 27.034107208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924466133117676 2.06453537940979 22.43779945373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923519611358643 2.077633857727051 22.56869125366211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79205322265625 2.0034866333007812 21.826919555664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79283607006073 2.3008508682250977 24.80134391784668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.3721299171447754 25.513137817382812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898250579834 2.1568498611450195 23.360397338867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.007096767425537 21.86269760131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917404174804688 2.0079567432403564 21.871307373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926799058914185 2.267709493637085 24.469776153564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923859357833862 2.4290952682495117 26.083337783813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792920470237732 2.165107250213623 23.443994522094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792599081993103 2.3384079933166504 25.176677703857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791102409362793 2.248807430267334 24.279178619384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926877737045288 2.37597918510437 25.552480697631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925527095794678 2.3104865550994873 24.897418975830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923954725265503 2.322073459625244 25.01312828063965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923009395599365 1.7702487707138062 19.494789123535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 1.9809280633926392 21.600872039794922
  batch 20 loss: 1.7915925979614258, 1.9809280633926392, 21.600872039794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792429804801941 1.9585814476013184 21.378244400024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924176454544067 2.2658958435058594 24.45137596130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921990156173706 2.0796327590942383 22.588525772094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792672872543335 2.130469560623169 23.097368240356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923698425292969 2.085374116897583 22.64611053466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924777269363403 1.8481589555740356 20.27406883239746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928332090377808 1.9417834281921387 21.210668563842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864037513733 2.253730297088623 24.32916831970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932802438735962 2.437655448913574 26.16983413696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519045829773 2.172065496444702 23.512174606323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.4197516441345215 25.99005699157715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791412591934204 2.5006320476531982 26.797733306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792412281036377 2.203277349472046 23.825185775756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925312519073486 2.147202491760254 23.264556884765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924869060516357 2.206784725189209 23.860334396362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925856113433838 2.3567934036254883 25.360519409179688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925090789794922 2.063250780105591 22.425016403198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791623830795288 2.099463701248169 22.7862606048584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792137622833252 2.105870485305786 22.850841522216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916539907455444 1.8727431297302246 20.519086837768555
  batch 40 loss: 1.7916539907455444, 1.8727431297302246, 20.519086837768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792157530784607 2.2442686557769775 24.234844207763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923707962036133 1.7895877361297607 19.688247680664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920336723327637 2.333252429962158 25.124557495117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 1.9788974523544312 21.580591201782227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924606800079346 2.0736799240112305 22.529260635375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921501398086548 2.1431150436401367 23.22330093383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049527168274 2.038731098175049 22.179359436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791900396347046 2.101285696029663 22.804758071899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.0520918369293213 22.312522888183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920337915420532 2.0691404342651367 22.48343849182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791085958480835 2.098987579345703 22.780961990356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792331576347351 2.2561466693878174 24.353797912597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216420173645 2.0095572471618652 21.88779067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925407886505127 2.1614956855773926 23.40749740600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918347120285034 2.0690712928771973 22.482545852661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792525291442871 2.089564323425293 22.688167572021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926832437515259 2.529305934906006 27.085742950439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926628589630127 1.7403473854064941 19.196136474609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930781841278076 2.527533769607544 27.06841468811035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921209335327148 2.2189879417419434 23.98200225830078
  batch 60 loss: 1.7921209335327148, 2.2189879417419434, 23.98200225830078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927051782608032 2.3112146854400635 24.90485191345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.1529533863067627 23.321216583251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514681816101 2.342162847518921 25.214143753051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921117544174194 2.355741024017334 25.349523544311523
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926461696624756 1.6536494493484497 18.32914161682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918591499328613 2.392146348953247 25.713321685791016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917735576629639 2.2093346118927 23.88511848449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 1.9933457374572754 21.725025177001953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.9944381713867188 21.736276626586914
Total LOSS train 23.39659057030311 valid 23.26493549346924
CE LOSS train 1.7922299531789927 valid 0.4479736387729645
Contrastive LOSS train 2.1604360598784225 valid 0.4986095428466797
EPOCH 265:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922096252441406 2.215367317199707 23.94588279724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915462255477905 2.532080888748169 27.112354278564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924463748931885 2.2441155910491943 24.23360252380371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923519611358643 2.076176643371582 22.554119110107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79205322265625 2.1817548274993896 23.609601974487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928361892700195 2.2206850051879883 23.99968719482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.283388614654541 24.62572479248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918977737426758 2.2494115829467773 24.286014556884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.0609333515167236 22.40106201171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917402982711792 2.138888120651245 23.180622100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792679786682129 2.377624273300171 25.568923950195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923861742019653 2.2428483963012695 24.220870971679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792919635772705 2.2286136150360107 24.079055786132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925984859466553 2.2970290184020996 24.762889862060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911025285720825 2.288877248764038 24.67987632751465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926870584487915 2.2410354614257812 24.203041076660156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925522327423096 2.1781258583068848 23.573810577392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923952341079712 2.389960527420044 25.691999435424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923011779785156 2.2321410179138184 24.113712310791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915927171707153 1.8050363063812256 19.841957092285156
  batch 20 loss: 1.7915927171707153, 1.8050363063812256, 19.841957092285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924294471740723 1.9894315004348755 21.686744689941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417287826538 2.2417585849761963 24.210002899169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792198657989502 1.8747843503952026 20.540040969848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926723957061768 2.121150493621826 23.004179000854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923694849014282 2.5216281414031982 27.008651733398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924771308898926 2.2841970920562744 24.63444709777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792832374572754 2.426116704940796 26.054000854492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918636798858643 2.232863664627075 24.120500564575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932794094085693 2.3792734146118164 25.586013793945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915188074111938 1.9767630100250244 21.55914878845215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 2.443194627761841 26.224485397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914122343063354 2.1640465259552 23.43187713623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924120426177979 2.065831422805786 22.450725555419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253089427948 2.249938488006592 24.291915893554688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924864292144775 2.522641897201538 27.018905639648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925852537155151 2.372446298599243 25.517047882080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792508602142334 2.242586851119995 24.2183780670166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916234731674194 2.22617769241333 24.05340003967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921373844146729 2.4171974658966064 25.964111328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 2.2547404766082764 24.339059829711914
  batch 40 loss: 1.7916537523269653, 2.2547404766082764, 24.339059829711914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921570539474487 2.2720096111297607 24.512252807617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923705577850342 2.1739113330841064 23.531482696533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033314704895 2.183046817779541 23.622501373291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916167974472046 2.1568684577941895 23.360300064086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924600839614868 2.0600202083587646 22.392662048339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921494245529175 2.0955731868743896 22.747880935668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 1.924304485321045 21.03509521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.209416627883911 23.886066436767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916052341461182 2.348299741744995 25.27460289001465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920329570770264 2.1483094692230225 23.275127410888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910858392715454 2.4645931720733643 26.4370174407959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923314571380615 2.3993406295776367 25.785737991333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216181755066 1.9261265993118286 21.053482055664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925406694412231 2.1079111099243164 22.871652603149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918342351913452 2.2572181224823 24.364015579223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792524814605713 1.7612723112106323 19.40524673461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926830053329468 2.3571064472198486 25.363746643066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792662501335144 2.0708062648773193 22.50072479248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930779457092285 2.462347984313965 26.41655731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921208143234253 2.2513558864593506 24.305679321289062
  batch 60 loss: 1.7921208143234253, 2.2513558864593506, 24.305679321289062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927049398422241 2.3419454097747803 25.212160110473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916830778121948 2.1724982261657715 23.516666412353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514443397522 2.171574831008911 23.508262634277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921110391616821 2.317000389099121 24.962114334106445
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926454544067383 1.9185528755187988 20.978172302246094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918591499328613 2.2686798572540283 24.478656768798828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.2722971439361572 24.51474380493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 2.037745475769043 22.169023513793945
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.9170966148376465 20.962862014770508
Total LOSS train 23.95257929288424 valid 23.03132152557373
CE LOSS train 1.7922296157250037 valid 0.4479736387729645
Contrastive LOSS train 2.2160349625807543 valid 0.4792741537094116
EPOCH 266:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922096252441406 2.237612009048462 24.1683292388916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79154634475708 2.4515225887298584 26.306772232055664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924463748931885 2.02170991897583 22.009544372558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923516035079956 2.1238489151000977 23.030839920043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792052984237671 2.2290737628936768 24.08279037475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928353548049927 2.2643868923187256 24.436704635620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.1921794414520264 23.713634490966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918976545333862 2.1375248432159424 23.167144775390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.3506240844726562 25.297969818115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 2.1212525367736816 23.00426483154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678713798523 2.5718445777893066 27.511123657226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385458946228 2.55727219581604 27.3651065826416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792919397354126 2.062483549118042 22.417755126953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792598009109497 2.2565534114837646 24.35813331604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911025285720825 2.242957830429077 24.22068214416504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926865816116333 2.2427594661712646 24.22028160095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79255211353302 2.237074613571167 24.163299560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394995689392 2.36161470413208 25.408540725708008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792300820350647 2.092594861984253 22.718250274658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 2.1526238918304443 23.317832946777344
  batch 20 loss: 1.7915925979614258, 2.1526238918304443, 23.317832946777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924292087554932 2.2020957469940186 23.813386917114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417049407959 2.2952170372009277 24.74458885192871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921983003616333 2.1386756896972656 23.178955078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792672038078308 2.1339433193206787 23.132104873657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 2.2773327827453613 24.565696716308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924771308898926 2.103519916534424 22.827674865722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928321361541748 2.391371488571167 25.706546783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918636798858643 1.627866506576538 18.07052993774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932792901992798 2.1548655033111572 23.341934204101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915188074111938 2.027106285095215 22.06258201599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925394773483276 2.3965578079223633 25.75811767578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914124727249146 2.2841174602508545 24.632587432861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924118041992188 2.2544901371002197 24.337312698364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 2.2309556007385254 24.10208511352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792486310005188 2.4503068923950195 26.295555114746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792584776878357 2.3601226806640625 25.39381217956543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925083637237549 2.3556265830993652 25.34877586364746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916232347488403 2.072026252746582 22.511886596679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921372652053833 2.4049313068389893 25.841449737548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916537523269653 2.407104253768921 25.86269760131836
  batch 40 loss: 1.7916537523269653, 2.407104253768921, 25.86269760131836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921574115753174 2.041809558868408 22.21025276184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792370319366455 1.611122727394104 17.903596878051758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920334339141846 2.0662994384765625 22.455028533935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 2.2401177883148193 24.192794799804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924602031707764 2.103010892868042 22.822568893432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792149543762207 2.2290642261505127 24.08279037475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920489311218262 2.084033966064453 22.632389068603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.2198235988616943 23.990137100219727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.1791796684265137 23.583402633666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033076286316 2.0562775135040283 22.354806900024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910860776901245 2.1997323036193848 23.788408279418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923312187194824 2.4684948921203613 26.477279663085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922163009643555 2.01108455657959 21.903060913085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.2055914402008057 23.848453521728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918343544006348 2.336986541748047 25.161699295043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925245761871338 2.0006511211395264 21.799036026000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926826477050781 2.29502010345459 24.742883682250977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792662501335144 1.9252941608428955 21.045602798461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793077826499939 2.3642172813415527 25.435251235961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921206951141357 2.1797754764556885 23.589874267578125
  batch 60 loss: 1.7921206951141357, 2.1797754764556885, 23.589874267578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927048206329346 2.175703525543213 23.549739837646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916831970214844 2.19569993019104 23.748682022094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925143241882324 2.0747642517089844 22.540157318115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921110391616821 2.2146425247192383 23.938535690307617
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926450967788696 1.6133527755737305 17.926172256469727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918590307235718 2.1754324436187744 23.54618263244629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917735576629639 2.2088286876678467 23.88006019592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915687561035156 1.9050482511520386 20.842050552368164
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.779650092124939 19.588396072387695
Total LOSS train 23.725690548236553 valid 21.96417236328125
CE LOSS train 1.792229461669922 valid 0.4479736387729645
Contrastive LOSS train 2.193346117093013 valid 0.44491252303123474
EPOCH 267:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922093868255615 2.14567494392395 23.248958587646484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791546106338501 2.401369571685791 25.805240631103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924460172653198 2.254931926727295 24.341766357421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923511266708374 2.180081844329834 23.593170166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920527458190918 2.004427671432495 21.83633041381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928352355957031 2.1476588249206543 23.269424438476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 2.307957410812378 24.871414184570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918978929519653 2.1558125019073486 23.35002326965332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 1.9787859916687012 21.57958984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 1.9720345735549927 21.512086868286133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926785945892334 2.262117624282837 24.413854598999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923856973648071 2.2169651985168457 23.9620361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929192781448364 1.8294107913970947 20.087026596069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925986051559448 1.8125669956207275 19.91826820373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911033630371094 1.8914073705673218 20.705177307128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926865816116333 2.3706276416778564 25.49896240234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79255211353302 2.249344825744629 24.286001205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394757270813 2.16672682762146 23.45966339111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792300820350647 1.9575833082199097 21.368133544921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915928363800049 1.912741780281067 20.91901206970215
  batch 20 loss: 1.7915928363800049, 1.912741780281067, 20.91901206970215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924294471740723 2.084840774536133 22.640836715698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417287826538 2.115546703338623 22.947885513305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792198657989502 2.0614070892333984 22.406269073486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792672038078308 2.18099308013916 23.602602005004883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923693656921387 2.558666229248047 27.379032135009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924772500991821 1.9888577461242676 21.681053161621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928322553634644 2.213310480117798 23.925935745239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918639183044434 2.141531467437744 23.207178115844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932789325714111 2.0188326835632324 21.98160743713379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791519284248352 1.9045684337615967 20.837203979492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 2.440574884414673 26.198287963867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914129495620728 2.361969470977783 25.41110610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924120426177979 2.0920910835266113 22.713321685791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 2.1668038368225098 23.460567474365234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792486548423767 2.253312110900879 24.325607299804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792585015296936 2.1997387409210205 23.78997230529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792508602142334 2.191523790359497 23.707746505737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.295344352722168 24.745067596435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921372652053833 2.248687505722046 24.27901268005371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 2.0525989532470703 22.317644119262695
  batch 40 loss: 1.7916538715362549, 2.0525989532470703, 22.317644119262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921574115753174 2.2188031673431396 23.98019027709961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923707962036133 1.9748742580413818 21.541114807128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920335531234741 2.0383262634277344 22.175296783447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916172742843628 2.283665418624878 24.628271102905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924602031707764 2.168069362640381 23.473154067993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792149543762207 2.341822624206543 25.210376739501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.13916015625 23.183650970458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918999195098877 1.9684220552444458 21.47612190246582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.3570637702941895 25.362241744995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033314704895 2.268958330154419 24.481616973876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910866737365723 2.417804002761841 25.969125747680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792331337928772 2.3659703731536865 25.452035903930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216420173645 2.2276158332824707 24.068374633789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.1344871520996094 23.13741111755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918344736099243 2.3559136390686035 25.350969314575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925245761871338 2.3122479915618896 24.91500473022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926826477050781 2.4184317588806152 25.977001190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926623821258545 2.1079020500183105 22.87168312072754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930774688720703 2.2592060565948486 24.3851375579834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921209335327148 2.1174979209899902 22.96710205078125
  batch 60 loss: 1.7921209335327148, 2.1174979209899902, 22.96710205078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704701423645 2.41845440864563 25.977249145507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.194795846939087 23.739641189575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925143241882324 1.743519902229309 19.22771453857422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921111583709717 2.13370680809021 23.129179000854492
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926450967788696 1.8200550079345703 19.993194580078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918591499328613 2.359586238861084 25.38772201538086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791773796081543 2.3188588619232178 24.980361938476562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915688753128052 2.003232955932617 21.823898315429688
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.8620002269744873 20.411897659301758
Total LOSS train 23.419337492722732 valid 23.150969982147217
CE LOSS train 1.7922295570373534 valid 0.4479736387729645
Contrastive LOSS train 2.1627107876997727 valid 0.4655000567436218
EPOCH 268:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792209506034851 2.2471303939819336 24.263513565063477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791546106338501 2.4205682277679443 25.997228622436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924460172653198 2.0616064071655273 22.408510208129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923510074615479 2.3444783687591553 25.23713493347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920526266098022 2.0845322608947754 22.637374877929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792834997177124 2.2040011882781982 23.832847595214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 1.9360026121139526 21.151865005493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918977737426758 2.168452024459839 23.476417541503906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 1.9457645416259766 21.249374389648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 2.1616244316101074 23.40798568725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678713798523 2.282851219177246 24.621191024780273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385458946228 2.3246870040893555 25.039255142211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929190397262573 2.201251745223999 23.805437088012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925981283187866 2.407782793045044 25.870426177978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911031246185303 2.2790839672088623 24.58194351196289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926865816116333 2.383084297180176 25.6235294342041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79255211353302 2.357196569442749 25.364519119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923948764801025 2.510450839996338 26.896902084350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923007011413574 1.9061243534088135 20.853544235229492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915925979614258 2.0488626956939697 22.28022003173828
  batch 20 loss: 1.7915925979614258, 2.0488626956939697, 22.28022003173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924292087554932 2.174121379852295 23.53364372253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417287826538 2.257819414138794 24.3706111907959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921985387802124 2.1833481788635254 23.62567901611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792671799659729 2.1504313945770264 23.296985626220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 2.3904964923858643 25.69733428955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792477011680603 2.2069103717803955 23.86157989501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928320169448853 1.9538975954055786 21.33180809020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918641567230225 1.9164429903030396 20.956295013427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932789325714111 2.1458444595336914 23.251724243164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915194034576416 2.216174602508545 23.953266143798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 2.3822667598724365 25.61520767211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914128303527832 2.3155508041381836 24.94692039489746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924120426177979 2.2465224266052246 24.25763702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253089427948 2.29325532913208 24.725082397460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924866676330566 1.998069405555725 21.77318000793457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925851345062256 2.3696165084838867 25.488750457763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925087213516235 2.280174493789673 24.594253540039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 1.7852734327316284 19.644357681274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792137622833252 2.3182358741760254 24.97449493408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.1004204750061035 22.79585838317871
  batch 40 loss: 1.791654109954834, 2.1004204750061035, 22.79585838317871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921574115753174 2.2770960330963135 24.56311798095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923705577850342 1.959938645362854 21.391756057739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.368980646133423 25.481840133666992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916173934936523 2.292057514190674 24.71219253540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924604415893555 2.1672492027282715 23.464954376220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921496629714966 2.1362664699554443 23.154815673828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920492887496948 1.9678466320037842 21.470516204833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.106328010559082 22.855180740356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 2.1641249656677246 23.4328556060791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920337915420532 2.203686237335205 23.828895568847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910871505737305 2.251983642578125 24.310924530029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792331576347351 2.2677552700042725 24.469884872436523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216420173645 2.1863577365875244 23.655794143676758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925405502319336 2.1605286598205566 23.3978271484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918347120285034 2.2570855617523193 24.362689971923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925246953964233 2.017777919769287 21.970304489135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926827669143677 2.5883302688598633 27.67598533630371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926626205444336 2.2442548274993896 24.235210418701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930774688720703 2.4442527294158936 26.235605239868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921210527420044 2.366955041885376 25.461671829223633
  batch 60 loss: 1.7921210527420044, 2.366955041885376, 25.461671829223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927051782608032 2.2651236057281494 24.443941116333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916839122772217 2.3098347187042236 24.890029907226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925148010253906 2.200836181640625 23.80087661743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921115159988403 2.193220376968384 23.724315643310547
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926453351974487 1.7246756553649902 19.03940200805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.330641746520996 25.098278045654297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917742729187012 2.264552116394043 24.43729591369629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 2.0392415523529053 22.18398666381836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918952703475952 1.9703136682510376 21.495031356811523
Total LOSS train 23.804991971529446 valid 23.303647994995117
CE LOSS train 1.792229617558993 valid 0.4479738175868988
Contrastive LOSS train 2.2012762289780836 valid 0.4925784170627594
EPOCH 269:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922098636627197 2.2596983909606934 24.38919448852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915470600128174 2.5831408500671387 27.622957229614258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792446494102478 2.283295154571533 24.625396728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923517227172852 2.2300729751586914 24.093082427978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920531034469604 2.1597707271575928 23.389760971069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928354740142822 2.1657421588897705 23.45025634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918397188186646 2.2583508491516113 24.375347137451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 2.104006052017212 22.831958770751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.0789709091186523 22.581439971923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 1.9817911386489868 21.609651565551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926791906356812 2.125898599624634 23.051664352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923859357833862 2.260361433029175 24.395999908447266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929192781448364 2.3103439807891846 24.896358489990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925984859466553 2.1954431533813477 23.74703025817871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911038398742676 2.044910192489624 22.240205764770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926865816116333 2.023561477661133 22.028301239013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925524711608887 2.3621857166290283 25.414409637451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923952341079712 2.2318179607391357 24.11057472229004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792301058769226 1.9950095415115356 21.74239730834961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915934324264526 2.1762592792510986 23.55418586730957
  batch 20 loss: 1.7915934324264526, 2.1762592792510986, 23.55418586730957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924295663833618 1.9933338165283203 21.725767135620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924174070358276 1.7857762575149536 19.65018081665039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921990156173706 1.521685004234314 17.009048461914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926721572875977 2.2689931392669678 24.48260498046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923694849014282 2.4509222507476807 26.301591873168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924772500991821 2.0871400833129883 22.663877487182617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928322553634644 2.1977643966674805 23.770475387573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.2073776721954346 23.86564064025879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932789325714111 2.240043878555298 24.19371795654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915199995040894 2.1441521644592285 23.23303985595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792540192604065 2.407684087753296 25.869380950927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79141366481781 2.3169641494750977 24.961055755615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792412281036377 2.2083990573883057 23.876401901245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792531132698059 2.29427170753479 24.735248565673828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924870252609253 2.2048914432525635 23.841400146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925854921340942 1.9561917781829834 21.354503631591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925090789794922 2.09794020652771 22.77191162109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916250228881836 2.199126958847046 23.782894134521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921382188796997 2.2384305000305176 24.176443099975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916545867919922 2.315640449523926 24.94805908203125
  batch 40 loss: 1.7916545867919922, 2.315640449523926, 24.94805908203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921580076217651 2.1878161430358887 23.670320510864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792371392250061 1.6442373991012573 18.234745025634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920342683792114 2.195730686187744 23.749340057373047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916179895401 2.22725248336792 24.064144134521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924606800079346 2.071305274963379 22.50551414489746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921499013900757 2.314225435256958 24.934404373168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049765586853 2.013864278793335 21.930692672729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919007539749146 2.0038115978240967 21.83001708984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916063070297241 2.278127431869507 24.5728816986084
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920340299606323 2.1900486946105957 23.692520141601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910878658294678 2.2254366874694824 24.045455932617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923316955566406 2.3479788303375244 25.272119522094727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922168970108032 1.946875810623169 21.260974884033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925405502319336 1.9398773908615112 21.191314697265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918355464935303 2.1412696838378906 23.204532623291016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925249338150024 2.085554599761963 22.648069381713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926831245422363 2.3726420402526855 25.51910400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926628589630127 2.200028896331787 23.792953491210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930775880813599 2.4796035289764404 26.589113235473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921216487884521 2.335581064224243 25.147932052612305
  batch 60 loss: 1.7921216487884521, 2.335581064224243, 25.147932052612305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927052974700928 2.275054693222046 24.54325294494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916841506958008 2.236551523208618 24.15719985961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925148010253906 2.2101614475250244 23.894128799438477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921116352081299 2.2850289344787598 24.64240074157715
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926453351974487 1.8390066623687744 20.182710647583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791859745979309 2.105316162109375 22.845022201538086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 2.0503809452056885 22.295583724975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 1.805704951286316 19.848617553710938
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918951511383057 1.6878414154052734 18.67030906677246
Total LOSS train 23.48675783597506 valid 20.914883136749268
CE LOSS train 1.792230039376479 valid 0.4479737877845764
Contrastive LOSS train 2.1694527809436503 valid 0.42196035385131836
EPOCH 270:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922098636627197 2.1159563064575195 22.951772689819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791547179222107 2.4126765727996826 25.91831398010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924461364746094 2.15342116355896 23.326658248901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923516035079956 2.0149292945861816 21.94164276123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920533418655396 2.234632730484009 24.13838005065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928352355957031 2.130455255508423 23.097387313842773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918397188186646 2.183316946029663 23.625009536743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918983697891235 2.2078232765197754 23.87013053894043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 2.2518301010131836 24.31003189086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791740894317627 2.135768413543701 23.149425506591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678952217102 2.414109230041504 25.93377113342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923858165740967 2.5199716091156006 26.992101669311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929191589355469 2.1947829723358154 23.74074935913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925982475280762 2.123987913131714 23.03247833251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911040782928467 2.352654457092285 25.31764793395996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926867008209229 2.230048894882202 24.093175888061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79255211353302 2.1850693225860596 23.643245697021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792394995689392 2.221956729888916 24.011960983276367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792301058769226 1.9999184608459473 21.791484832763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915935516357422 2.230039119720459 24.09198570251465
  batch 20 loss: 1.7915935516357422, 2.230039119720459, 24.09198570251465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924294471740723 2.0774290561676025 22.56671905517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417287826538 2.1635589599609375 23.428007125854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921991348266602 2.118447780609131 22.97667694091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792672038078308 2.160426139831543 23.39693260192871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923694849014282 2.463524103164673 26.427610397338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924772500991821 2.0597217082977295 22.389694213867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928318977355957 1.9442006349563599 21.234838485717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 1.787577509880066 19.667638778686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793278694152832 2.0337576866149902 22.130855560302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520118713379 2.292462110519409 24.716140747070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925397157669067 2.340987205505371 25.202411651611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791413426399231 2.411628007888794 25.90769386291504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924121618270874 2.185861825942993 23.651029586791992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925310134887695 2.1706440448760986 23.49897003173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924867868423462 2.2500131130218506 24.292617797851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792585015296936 2.372954845428467 25.52213478088379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925087213516235 2.1510274410247803 23.302783966064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916244268417358 2.037423849105835 22.165863037109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792137622833252 2.2353618144989014 24.145755767822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.175966739654541 23.551321029663086
  batch 40 loss: 1.7916544675827026, 2.175966739654541, 23.551321029663086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792157530784607 2.353877544403076 25.330934524536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923704385757446 2.0452969074249268 22.245338439941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.1389365196228027 23.181400299072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916178703308105 2.2133445739746094 23.925064086914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792460322380066 2.0218076705932617 22.010536193847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921496629714966 2.3242552280426025 25.03470230102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049527168274 1.9963418245315552 21.755468368530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791900396347046 2.1955795288085938 23.747695922851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916061878204346 2.2162795066833496 23.954402923583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.2241604328155518 24.03363800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910877466201782 2.3081612586975098 24.872699737548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923316955566406 2.267512798309326 24.46746063232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922167778015137 2.1519827842712402 23.31204605102539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792540431022644 2.382397413253784 25.616514205932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.408538341522217 25.877220153808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925245761871338 2.001052141189575 21.80304527282715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926827669143677 2.3260130882263184 25.052814483642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926628589630127 2.0617480278015137 22.410144805908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930773496627808 2.5774917602539062 27.567995071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921212911605835 2.400639295578003 25.79851531982422
  batch 60 loss: 1.7921212911605835, 2.400639295578003, 25.79851531982422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927049398422241 2.285329580307007 24.6460018157959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.2538559436798096 24.330242156982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514443397522 2.1456797122955322 23.249311447143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921111583709717 2.252361536026001 24.315725326538086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926453351974487 1.7532413005828857 19.325057983398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79185950756073 2.124112367630005 23.032981872558594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 2.128317356109619 23.074947357177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 1.836569905281067 20.157268524169922
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918952703475952 1.6220531463623047 18.012426376342773
Total LOSS train 23.800261922983022 valid 21.069406032562256
CE LOSS train 1.792229855977572 valid 0.4479738175868988
Contrastive LOSS train 2.200803201015179 valid 0.40551328659057617
EPOCH 271:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922097444534302 2.1413469314575195 23.205678939819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791547179222107 2.456845283508301 26.360000610351562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924456596374512 2.1001336574554443 22.79378318786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923510074615479 2.0711236000061035 22.503585815429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792052984237671 2.2886767387390137 24.678821563720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928345203399658 2.1816823482513428 23.60965919494629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.337702751159668 25.168867111206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918977737426758 2.1345746517181396 23.137645721435547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.22135591506958 24.005287170410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.044881820678711 22.240558624267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926782369613647 2.331906795501709 25.11174774169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923853397369385 2.324599266052246 25.03837776184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79291832447052 2.03743577003479 22.16727638244629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925976514816284 2.436138391494751 26.153980255126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911041975021362 2.2179789543151855 23.97089385986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926863431930542 2.350052833557129 25.293214797973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925519943237305 2.0414187908172607 22.206741333007812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923948764801025 2.2992136478424072 24.784530639648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923005819320679 2.0847465991973877 22.639766693115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915934324264526 2.090890645980835 22.70050048828125
  batch 20 loss: 1.7915934324264526, 2.090890645980835, 22.70050048828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924288511276245 2.2612671852111816 24.405099868774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924169301986694 2.151258945465088 23.30500602722168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792198657989502 2.092533826828003 22.71753692626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792671799659729 2.0400781631469727 22.193452835083008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923692464828491 2.3730149269104004 25.522518157958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924768924713135 1.9720367193222046 21.51284408569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928317785263062 2.3765921592712402 25.558753967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 2.058478832244873 22.37665367126465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932778596878052 2.278871774673462 24.581995010375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520118713379 2.139669895172119 23.188217163085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925392389297485 2.379608631134033 25.588624954223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79141366481781 2.3789215087890625 25.580629348754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924116849899292 2.2143964767456055 23.936376571655273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 2.079890012741089 22.5914306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792486310005188 2.4366519451141357 26.159006118774414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925846576690674 2.347834587097168 25.270931243896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925083637237549 1.8870265483856201 20.66277503967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916247844696045 2.0820984840393066 22.612607955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792137622833252 2.2807812690734863 24.59994888305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.2338016033172607 24.129671096801758
  batch 40 loss: 1.7916544675827026, 2.2338016033172607, 24.129671096801758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792157530784607 2.172689914703369 23.51905632019043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792370319366455 1.9265400171279907 21.057769775390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.2367706298828125 24.159740447998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617751121521 2.1397674083709717 23.189292907714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924599647521973 2.1187903881073 22.980363845825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921496629714966 2.1383981704711914 23.176132202148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049527168274 2.0920073986053467 22.71212387084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.073369264602661 22.525592803955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916064262390137 2.1228272914886475 23.019880294799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920336723327637 2.018723249435425 21.979267120361328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910881042480469 2.336550235748291 25.15658950805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792331337928772 2.255108118057251 24.343412399291992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792216420173645 2.2779219150543213 24.571435928344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925405502319336 2.1991376876831055 23.783916473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.312074899673462 24.91258430480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925244569778442 2.213761329650879 23.930137634277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926825284957886 2.601210355758667 27.804786682128906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926623821258545 2.1434149742126465 23.2268123626709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930766344070435 2.4610161781311035 26.40323829650879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921208143234253 2.2279653549194336 24.071773529052734
  batch 60 loss: 1.7921208143234253, 2.2279653549194336, 24.071773529052734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792704463005066 2.327075242996216 25.06345558166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916843891143799 1.6508712768554688 18.300397872924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925140857696533 2.0460474491119385 22.252988815307617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921112775802612 2.1461193561553955 23.25330352783203
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926445007324219 1.8010684251785278 19.803329467773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918593883514404 2.3149826526641846 24.94168472290039
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.2427287101745605 24.21906280517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 1.9784387350082397 21.575958251953125
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894793510437 1.9670780897140503 21.462675094604492
Total LOSS train 23.684036607008714 valid 23.049845218658447
CE LOSS train 1.792229591883146 valid 0.44797369837760925
Contrastive LOSS train 2.1891806987615734 valid 0.4917695224285126
EPOCH 272:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792209267616272 2.3035929203033447 24.82813835144043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915470600128174 2.4284472465515137 26.076021194458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924455404281616 2.107621669769287 22.868663787841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923510074615479 2.1800756454467773 23.593107223510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920527458190918 2.1603636741638184 23.39569091796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928342819213867 2.038712978363037 22.17996597290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918392419815063 2.38887095451355 25.68054962158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 1.8816490173339844 20.608388900756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.2492411136627197 24.28413963317871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791740894317627 2.1918978691101074 23.71072006225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 2.3086538314819336 24.87921714782715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923851013183594 2.3711941242218018 25.50432586669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929179668426514 2.1685259342193604 23.47817611694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925976514816284 2.19762921333313 23.768888473510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911044359207153 2.180323362350464 23.59433937072754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792686104774475 2.4348301887512207 26.140987396240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925515174865723 2.364563465118408 25.43818473815918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923945188522339 2.125988245010376 23.052276611328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923005819320679 1.9990925788879395 21.783226013183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593313217163 1.9872405529022217 21.663999557495117
  batch 20 loss: 1.791593313217163, 1.9872405529022217, 21.663999557495117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792428731918335 2.10380220413208 22.8304500579834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924168109893799 2.216454029083252 23.95695686340332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921984195709229 1.897763729095459 20.76983642578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926714420318604 1.9468191862106323 21.260862350463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 2.413698434829712 25.929353713989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924765348434448 2.1701653003692627 23.494129180908203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928309440612793 2.408046245574951 25.873294830322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791864275932312 2.0075221061706543 21.86708641052246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932772636413574 2.186326742172241 23.656545639038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915199995040894 2.2068047523498535 23.85956573486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792539119720459 2.428039789199829 26.07293701171875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914135456085205 2.2630555629730225 24.42197036743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241144657135 2.0930755138397217 22.723167419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.257608652114868 24.368616104125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924859523773193 2.480933904647827 26.601825714111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925841808319092 2.3842241764068604 25.634824752807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925078868865967 2.1578543186187744 23.371049880981445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 2.080881357192993 22.60043716430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921372652053833 2.3712446689605713 25.50458335876465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.2166569232940674 23.958223342895508
  batch 40 loss: 1.791654348373413, 2.2166569232940674, 23.958223342895508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921571731567383 2.235387086868286 24.146026611328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923702001571655 1.9945433139801025 21.737802505493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920339107513428 2.356940269470215 25.36143684387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617751121521 2.374894380569458 25.54056167602539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924597263336182 2.0945398807525635 22.737857818603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792149305343628 2.2868916988372803 24.66106605529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920489311218262 2.1174752712249756 22.9668025970459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.086803913116455 22.65993881225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916059494018555 2.383931875228882 25.630924224853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920335531234741 2.1414785385131836 23.206819534301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910878658294678 2.3070642948150635 24.861730575561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923308610916138 2.377117872238159 25.56351089477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922159433364868 2.069331407546997 22.48552894592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925395965576172 2.0665061473846436 22.45760154724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918351888656616 2.2645821571350098 24.43765640258789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523980140686 2.0250487327575684 22.043012619018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926818132400513 2.326972246170044 25.06240463256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926619052886963 2.049921751022339 22.291879653930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793076515197754 2.3821945190429688 25.615020751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921209335327148 2.2633883953094482 24.426006317138672
  batch 60 loss: 1.7921209335327148, 2.2633883953094482, 24.426006317138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927042245864868 2.167246103286743 23.465164184570312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916841506958008 2.1852214336395264 23.643898010253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925137281417847 1.8118234872817993 19.910747528076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921115159988403 2.1608006954193115 23.40011978149414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792644739151001 1.5218061208724976 17.010705947875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918596267700195 2.1736321449279785 23.528179168701172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 2.3183581829071045 24.975357055664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915700674057007 2.025672674179077 22.048297882080078
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918950319290161 1.9746066331863403 21.537961959838867
Total LOSS train 23.701676119290866 valid 23.022449016571045
CE LOSS train 1.792229360800523 valid 0.44797375798225403
Contrastive LOSS train 2.1909446734648483 valid 0.4936516582965851
EPOCH 273:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922096252441406 2.347411870956421 25.266328811645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915472984313965 2.1433801651000977 23.22534942626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924456596374512 2.0470058917999268 22.26250457763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923511266708374 2.1162898540496826 22.955249786376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792052984237671 2.232145071029663 24.11350440979004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928342819213867 2.1963131427764893 23.755966186523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 2.300791025161743 24.79974937438965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918978929519653 2.191634178161621 23.708240509033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917299270629883 2.068424701690674 22.475975036621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.206042766571045 23.852169036865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 2.1218819618225098 23.011497497558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792385220527649 2.115581512451172 22.948200225830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929182052612305 2.139401912689209 23.186939239501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925974130630493 2.2075793743133545 23.868391036987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791104793548584 2.1943070888519287 23.734176635742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926857471466064 2.1693966388702393 23.486652374267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925516366958618 2.3283612728118896 25.07616424560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923945188522339 2.2181077003479004 23.97347068786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792300820350647 1.890180230140686 20.694103240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915937900543213 2.1137776374816895 22.92936897277832
  batch 20 loss: 1.7915937900543213, 2.1137776374816895, 22.92936897277832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792428970336914 2.0135462284088135 21.92789077758789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792417049407959 2.1887292861938477 23.679710388183594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792198657989502 1.9990721940994263 21.782920837402344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926714420318604 2.1235055923461914 23.027727127075195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 2.510242223739624 26.894792556762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792476773262024 2.2080259323120117 23.87273597717285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928311824798584 2.293100118637085 24.723833084106445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918646335601807 2.0794739723205566 22.58660316467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932772636413574 2.3977925777435303 25.771203994750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915195226669312 2.238494873046875 24.176467895507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925392389297485 2.429100275039673 26.083541870117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914130687713623 2.3506784439086914 25.29819679260254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924106121063232 2.2557170391082764 24.349580764770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925293445587158 2.311246633529663 24.904996871948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792484998703003 2.4447500705718994 26.2399845123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925831079483032 2.371384620666504 25.50642967224121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925071716308594 2.4587137699127197 26.3796443939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916240692138672 2.134962797164917 23.141252517700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921364307403564 2.384033679962158 25.63247299194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916538715362549 1.9638584852218628 21.430238723754883
  batch 40 loss: 1.7916538715362549, 1.9638584852218628, 21.430238723754883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921562194824219 1.9767850637435913 21.560007095336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79236900806427 1.9262988567352295 21.05535888671875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920327186584473 2.1278088092803955 23.070119857788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916169166564941 2.044844388961792 22.240060806274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924585342407227 2.0250730514526367 22.043190002441406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792148470878601 2.186079978942871 23.6529483795166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 2.1047849655151367 22.83989715576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918994426727295 2.235353946685791 24.145437240600586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.298022508621216 24.77182960510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792032241821289 1.9248255491256714 21.040287017822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79108726978302 2.448333501815796 26.274423599243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923305034637451 2.3267621994018555 25.059951782226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922152280807495 1.9986766576766968 21.778982162475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925387620925903 2.1523449420928955 23.315988540649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.4242501258850098 26.034334182739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925233840942383 2.1854336261749268 23.64685821533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926808595657349 2.526092052459717 27.05360221862793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926605939865112 2.034766912460327 22.140329360961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930750846862793 2.4245223999023438 26.038299560546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921202182769775 2.169670343399048 23.48882293701172
  batch 60 loss: 1.7921202182769775, 2.169670343399048, 23.48882293701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927030324935913 2.2701914310455322 24.494617462158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683316230774 2.132544994354248 23.11713409423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925128936767578 2.073117256164551 22.523685455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792110562324524 2.352238893508911 25.314498901367188
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926431894302368 1.6797065734863281 18.58970832824707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918589115142822 1.8232231140136719 20.024089813232422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917735576629639 1.73276948928833 19.11946678161621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915695905685425 1.47071373462677 16.498706817626953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918943166732788 1.490602970123291 16.69792366027832
Total LOSS train 23.723455370389498 valid 18.085046768188477
CE LOSS train 1.7922289609909057 valid 0.4479735791683197
Contrastive LOSS train 2.1931226436908426 valid 0.37265074253082275
Saved best model. Old loss 18.33022975921631 and new best loss 18.085046768188477
EPOCH 274:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922085523605347 1.920917272567749 21.001380920410156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915470600128174 2.3724355697631836 25.51590347290039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924445867538452 2.090454578399658 22.696989059448242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923500537872314 2.255347490310669 24.3458251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920525074005127 2.1183266639709473 22.975318908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928329706192017 2.088576555252075 22.678598403930664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918388843536377 2.347224235534668 25.264081954956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918974161148071 2.069488286972046 22.486780166625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.224747896194458 24.039207458496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 1.9843982458114624 21.635723114013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926769256591797 2.2463176250457764 24.2558536529541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923840284347534 2.4380404949188232 26.172788619995117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929167747497559 2.1303951740264893 23.09686851501465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792596459388733 2.179612636566162 23.58872413635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911043167114258 2.1197688579559326 22.988792419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926849126815796 2.4545888900756836 26.338573455810547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 2.062025308609009 22.412803649902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923935651779175 2.301877498626709 24.811168670654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922993898391724 2.0944294929504395 22.73659324645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593313217163 2.2129952907562256 23.921546936035156
  batch 20 loss: 1.791593313217163, 2.2129952907562256, 23.921546936035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792427897453308 2.0034584999084473 21.827011108398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792415738105774 2.2050530910491943 23.842947006225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921977043151855 2.0088050365448 21.8802490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926702499389648 1.9751293659210205 21.543964385986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923681735992432 2.545607566833496 27.248443603515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924754619598389 2.1877715587615967 23.670190811157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928298711776733 2.4087307453155518 25.880136489868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918637990951538 2.1467177867889404 23.259042739868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932758331298828 2.2949059009552 24.742334365844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915194034576416 2.1354751586914062 23.146270751953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925376892089844 2.4131243228912354 25.92378044128418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914133071899414 2.363755702972412 25.428970336914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924103736877441 2.047560214996338 22.26801109313965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925291061401367 2.4070701599121094 25.863231658935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924848794937134 2.2846643924713135 24.639127731323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925831079483032 2.0169689655303955 21.96227264404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925070524215698 2.225928544998169 24.05179214477539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 2.038516044616699 22.17678451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136311531067 2.2916674613952637 24.708812713623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 1.82510507106781 20.042705535888672
  batch 40 loss: 1.791654109954834, 1.82510507106781, 20.042705535888672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792156457901001 2.1174278259277344 22.966434478759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79236900806427 1.8860440254211426 20.652809143066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920325994491577 2.2525134086608887 24.317167282104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916170358657837 2.152456045150757 23.316177368164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924585342407227 2.0490870475769043 22.283329010009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921483516693115 2.3196306228637695 24.988454818725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920483350753784 1.8941525220870972 20.733572006225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791899561882019 2.051273822784424 22.304636001586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.1124393939971924 22.915998458862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920321226119995 1.9039762020111084 20.83179473876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910875082015991 2.4290571212768555 26.0816593170166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792330265045166 2.3221096992492676 25.013425827026367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792215347290039 2.0073182582855225 21.865398406982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925385236740112 2.311546564102173 24.908002853393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918344736099243 2.282707691192627 24.61890983581543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925233840942383 2.178864002227783 23.581161499023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926807403564453 2.372389554977417 25.516576766967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926603555679321 2.1836423873901367 23.62908363342285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930747270584106 2.556346893310547 27.356544494628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921199798583984 2.088880777359009 22.680927276611328
  batch 60 loss: 1.7921199798583984, 2.088880777359009, 22.680927276611328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927029132843018 2.261195659637451 24.404661178588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916837930679321 2.0155861377716064 21.94754409790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925130128860474 2.115969181060791 22.95220375061035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921104431152344 2.3412368297576904 25.204479217529297
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926430702209473 1.9492369890213013 21.28501319885254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918586730957031 2.076998233795166 22.561840057373047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.1064701080322266 22.856473922729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915695905685425 1.852819561958313 20.319765090942383
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918941974639893 1.819278597831726 19.98468017578125
Total LOSS train 23.591162549532378 valid 21.430689811706543
CE LOSS train 1.7922284896557148 valid 0.4479735493659973
Contrastive LOSS train 2.179893420292781 valid 0.4548196494579315
EPOCH 275:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922085523605347 2.051506519317627 22.30727195739746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915467023849487 2.5448546409606934 27.240093231201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924444675445557 2.224360227584839 24.036046981811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923496961593628 1.6983089447021484 18.77543830871582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792052149772644 2.1327807903289795 23.11985969543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928327322006226 2.2411394119262695 24.204227447509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.393651008605957 25.728349685668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918975353240967 2.1779932975769043 23.57183074951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.135172128677368 23.143449783325195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 2.2006468772888184 23.79821014404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926766872406006 2.2816262245178223 24.608938217163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923839092254639 2.4144816398620605 25.93720054626465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929165363311768 2.0357868671417236 22.150785446166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925961017608643 2.2170372009277344 23.962968826293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911045551300049 2.3127810955047607 24.918916702270508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926846742630005 2.072347640991211 22.51616096496582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925502061843872 2.1408591270446777 23.201143264770508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792393445968628 2.3996803760528564 25.789196014404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922993898391724 1.9484881162643433 21.277179718017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593074798584 1.985946536064148 21.651058197021484
  batch 20 loss: 1.791593074798584, 1.985946536064148, 21.651058197021484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924277782440186 2.1145880222320557 22.93830680847168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924158573150635 2.112292528152466 22.915340423583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921977043151855 2.2360446453094482 24.152645111083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926703691482544 2.283996820449829 24.632638931274414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792367696762085 2.458310604095459 26.37547492980957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924754619598389 2.3213770389556885 25.006244659423828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928297519683838 2.202497720718384 23.817806243896484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918637990951538 1.9466615915298462 21.258480072021484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932755947113037 2.260993480682373 24.40321159362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915198802947998 2.022846221923828 22.019981384277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792537808418274 2.2721855640411377 24.514392852783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914139032363892 2.4578354358673096 26.369768142700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924103736877441 2.141655683517456 23.208967208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925291061401367 2.4046218395233154 25.838748931884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924847602844238 2.28484845161438 24.640968322753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925827503204346 2.363342523574829 25.426008224487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925066947937012 2.3236405849456787 25.028913497924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916241884231567 2.1986258029937744 23.777881622314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136788368225 2.2606217861175537 24.39835548400879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.263620615005493 24.427860260009766
  batch 40 loss: 1.791654348373413, 2.263620615005493, 24.427860260009766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792156457901001 2.217829465866089 23.97045135498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923691272735596 2.0959908962249756 22.75227928161621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920328378677368 2.296481132507324 24.75684356689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.1478867530822754 23.270484924316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924582958221436 2.144331216812134 23.235769271850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921483516693115 2.1919658184051514 23.711807250976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920483350753784 2.0803184509277344 22.595232009887695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918990850448608 2.0852608680725098 22.644506454467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791605830192566 2.3561854362487793 25.35346031188965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792032241821289 1.9844985008239746 21.63701820373535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910882234573364 2.2059338092803955 23.850425720214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792330026626587 2.244718313217163 24.239513397216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922147512435913 2.029827117919922 22.090486526489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925381660461426 2.2216544151306152 24.009082794189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.3123908042907715 24.915742874145508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792522668838501 1.9949302673339844 21.741825103759766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680263519287 2.4699695110321045 26.49237632751465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926603555679321 2.3296782970428467 25.08944320678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793074131011963 2.354616641998291 25.3392391204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921197414398193 2.1363275051116943 23.1553955078125
  batch 60 loss: 1.7921197414398193, 2.1363275051116943, 23.1553955078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702078819275 2.1025044918060303 22.817747116088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916831970214844 2.2551872730255127 24.343555450439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925119400024414 2.180473804473877 23.597248077392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792109727859497 2.133486747741699 23.126977920532227
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79264235496521 1.756068468093872 19.35332679748535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918585538864136 2.279446601867676 24.58632469177246
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917732000350952 2.2401952743530273 24.1937255859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915692329406738 2.0108654499053955 21.900222778320312
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918941974639893 1.8503382205963135 20.295276641845703
Total LOSS train 23.772008602435772 valid 22.743887424468994
CE LOSS train 1.7922283557745127 valid 0.4479735493659973
Contrastive LOSS train 2.197978025216323 valid 0.46258455514907837
EPOCH 276:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922080755233765 2.2822840213775635 24.615047454833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915465831756592 2.4743340015411377 26.53488540649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924439907073975 1.8424113988876343 20.2165584564209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923495769500732 2.1711156368255615 23.50350570678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920520305633545 2.1827855110168457 23.619905471801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928322553634644 2.108767032623291 22.88050079345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.319441318511963 24.986251831054688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791897177696228 2.231234073638916 24.104236602783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917290925979614 1.9134485721588135 20.92621421813965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 2.2212748527526855 24.00448989868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926762104034424 2.3953561782836914 25.746238708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923835515975952 2.3404242992401123 25.196626663208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79291570186615 2.213955879211426 23.93247413635254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925957441329956 2.2708308696746826 24.500904083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911045551300049 2.257298469543457 24.364089965820312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926839590072632 2.3626255989074707 25.4189395904541
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925496101379395 1.9559828042984009 21.35237693786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923928499221802 2.0820491313934326 22.612884521484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922990322113037 2.1066670417785645 22.85896873474121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593074798584 2.1682701110839844 23.474294662475586
  batch 20 loss: 1.791593074798584, 2.1682701110839844, 23.474294662475586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924270629882812 1.9592410326004028 21.384838104248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792414903640747 2.158785820007324 23.380273818969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921968698501587 1.648496389389038 18.27716064453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926691770553589 1.84853196144104 20.27798843383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923673391342163 2.3771612644195557 25.563980102539062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924747467041016 2.0929975509643555 22.722450256347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928290367126465 2.153211832046509 23.324947357177734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918630838394165 1.9828499555587769 21.620361328125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932746410369873 2.254671573638916 24.339988708496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915196418762207 2.223374366760254 24.0252628326416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792536973953247 2.369690179824829 25.489439010620117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914131879806519 2.334388017654419 25.135292053222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792409896850586 2.0424203872680664 22.21661376953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925282716751099 2.1345646381378174 23.138174057006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792484164237976 2.3555140495300293 25.347625732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925825119018555 2.2873599529266357 24.666183471679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925059795379639 2.35546612739563 25.347166061401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.1086010932922363 22.877634048461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921364307403564 2.2333099842071533 24.12523651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.28755521774292 24.667207717895508
  batch 40 loss: 1.791654109954834, 2.28755521774292, 24.667207717895508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921561002731323 2.0484039783477783 22.276195526123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792368769645691 1.8752994537353516 20.54536247253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920325994491577 2.1228644847869873 23.02067756652832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916172742843628 2.0956122875213623 22.747739791870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924580574035645 2.124053955078125 23.032997131347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921481132507324 2.104078531265259 22.83293342590332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 2.02244234085083 22.01647186279297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791899561882019 2.0371713638305664 22.163612365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916054725646973 2.2239882946014404 24.0314884185791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920325994491577 2.207364320755005 23.86567497253418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791088342666626 2.4383625984191895 26.174713134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923295497894287 2.413586378097534 25.928194046020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922145128250122 1.9881969690322876 21.674184799194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925382852554321 2.1461195945739746 23.253734588623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.2486977577209473 24.278810501098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925221920013428 2.2362310886383057 24.15483283996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926799058914185 2.3918299674987793 25.710981369018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792659878730774 2.239470958709717 24.18737030029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930735349655151 2.318253755569458 24.975610733032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921196222305298 2.119175434112549 22.98387336730957
  batch 60 loss: 1.7921196222305298, 2.119175434112549, 22.98387336730957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927018404006958 2.1286544799804688 23.079246520996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.2258403301239014 24.050086975097656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925117015838623 2.2224340438842773 24.0168514251709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921099662780762 2.315229654312134 24.944406509399414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792642593383789 1.693771481513977 18.730358123779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918586730957031 2.1532795429229736 23.32465362548828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917734384536743 2.1818928718566895 23.610700607299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915693521499634 1.9768635034561157 21.560203552246094
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918938398361206 1.8892008066177368 20.683900833129883
Total LOSS train 23.499286563579854 valid 22.294864654541016
CE LOSS train 1.7922280036486111 valid 0.44797345995903015
Contrastive LOSS train 2.1707058723156267 valid 0.4723002016544342
EPOCH 277:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792207956314087 2.180152177810669 23.59372901916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915464639663696 2.5636794567108154 27.428340911865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792443871498108 2.1999716758728027 23.79216194152832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923494577407837 2.14302921295166 23.222640991210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920516729354858 2.1571204662323 23.363256454467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928317785263062 2.3550918102264404 25.34375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.4458277225494385 26.2501163482666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918972969055176 2.0642552375793457 22.4344482421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172945022583 2.175562620162964 23.54735565185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 2.072274684906006 22.514488220214844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792676329612732 2.404691219329834 25.839590072631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923836708068848 2.3472981452941895 25.265363693237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929155826568604 2.2436680793762207 24.229595184326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925958633422852 2.0840954780578613 22.633548736572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911049127578735 2.2714903354644775 24.50600814819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683720588684 2.3575544357299805 25.368228912353516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925496101379395 2.0306951999664307 22.09950065612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923928499221802 2.3105318546295166 24.8977108001709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922989130020142 2.0155200958251953 21.947500228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915929555892944 2.247347354888916 24.265066146850586
  batch 20 loss: 1.7915929555892944, 2.247347354888916, 24.265066146850586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924269437789917 1.9648873805999756 21.441301345825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792414903640747 2.191333532333374 23.705751419067383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921969890594482 2.023268461227417 22.02488136291504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926691770553589 2.1702582836151123 23.49525260925293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923673391342163 2.2290148735046387 24.082517623901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924748659133911 2.2642407417297363 24.43488121032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928287982940674 2.306851863861084 24.86134910583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 2.1105916500091553 22.897781372070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932740449905396 2.1632814407348633 23.426088333129883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915198802947998 2.033425807952881 22.125778198242188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792536735534668 2.38401460647583 25.63268280029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79141366481781 2.3744518756866455 25.535932540893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792409896850586 2.0989229679107666 22.781639099121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925281524658203 2.1820919513702393 23.613447189331055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924840450286865 2.398493528366089 25.777420043945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925822734832764 2.3276989459991455 25.069570541381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925060987472534 2.1631672382354736 23.424177169799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916239500045776 2.0207207202911377 21.998830795288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136311531067 2.048060417175293 22.272741317749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.256598711013794 24.357641220092773
  batch 40 loss: 1.791654109954834, 2.256598711013794, 24.357641220092773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921558618545532 2.299306631088257 24.78522300720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923686504364014 1.4605869054794312 16.398237228393555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920329570770264 2.0896434783935547 22.688467025756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916173934936523 2.184110164642334 23.632720947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924580574035645 1.983844518661499 21.630903244018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921483516693115 2.16945219039917 23.486671447753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920480966567993 1.9941307306289673 21.733354568481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918994426727295 2.1469614505767822 23.261512756347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916057109832764 2.33001708984375 25.09177589416504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920324802398682 2.1434736251831055 23.226768493652344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910887002944946 2.272784471511841 24.518932342529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923295497894287 2.279379367828369 24.586122512817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922146320343018 1.9614709615707397 21.406925201416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792537808418274 2.223015546798706 24.022693634033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.170041084289551 23.492244720458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925220727920532 2.1482317447662354 23.274839401245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680025100708 2.40338397026062 25.826520919799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926596403121948 2.1366539001464844 23.159198760986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930734157562256 2.5933332443237305 27.72640609741211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921197414398193 2.2894792556762695 24.686912536621094
  batch 60 loss: 1.7921197414398193, 2.2894792556762695, 24.686912536621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927017211914062 2.1311800479888916 23.104501724243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.145329236984253 23.244976043701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925119400024414 2.2547266483306885 24.339778900146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921099662780762 2.174429178237915 23.536401748657227
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926422357559204 1.87424898147583 20.535131454467773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918587923049927 2.3351104259490967 25.142963409423828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.123934745788574 23.03112030029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 1.9621632099151611 21.41320037841797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918943166732788 2.090240240097046 22.694297790527344
Total LOSS train 23.675373634925254 valid 23.070395469665527
CE LOSS train 1.7922279834747314 valid 0.4479735791683197
Contrastive LOSS train 2.1883145644114568 valid 0.5225600600242615
EPOCH 278:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792207956314087 2.321171522140503 25.003923416137695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915468215942383 2.424091339111328 26.032459259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792443871498108 2.301017999649048 24.802623748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923492193222046 2.25772762298584 24.369625091552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920516729354858 2.339301824569702 25.185070037841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928317785263062 2.357093095779419 25.36376190185547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.3700108528137207 25.491947174072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918972969055176 2.089543342590332 22.68733024597168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172945022583 2.0076799392700195 21.868528366088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 1.9651731252670288 21.443471908569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926757335662842 2.375653028488159 25.549205780029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923834323883057 2.4155735969543457 25.948118209838867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929153442382812 2.3100554943084717 24.893470764160156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925951480865479 2.241715669631958 24.20975112915039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791105031967163 2.2334587574005127 24.12569236755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683482170105 2.5488154888153076 27.280838012695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254949092865 2.2812323570251465 24.604873657226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923928499221802 2.4310195446014404 26.102588653564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922991514205933 2.067532539367676 22.46762466430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791593074798584 1.8594862222671509 20.386455535888672
  batch 20 loss: 1.791593074798584, 1.8594862222671509, 20.386455535888672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924269437789917 1.9176584482192993 20.969011306762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792414903640747 2.214172840118408 23.93414306640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921969890594482 2.098970890045166 22.781904220581055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926689386367798 2.1726467609405518 23.519136428833008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923671007156372 2.4457125663757324 26.249494552612305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924747467041016 2.3486244678497314 25.278718948364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928284406661987 2.3272712230682373 25.065540313720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 2.294517755508423 24.73703956604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932738065719604 2.244563102722168 24.23890495300293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915199995040894 2.222921848297119 24.020736694335938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792536973953247 2.3301684856414795 25.094223022460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914141416549683 2.381134510040283 25.602758407592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792410135269165 2.174481153488159 23.537221908569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925282716751099 2.2073974609375 23.86650276184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924844026565552 2.2808797359466553 24.601282119750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792582392692566 2.0783944129943848 22.57652473449707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925063371658325 2.2434020042419434 24.22652816772461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916244268417358 2.0943779945373535 22.735403060913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921364307403564 2.3638319969177246 25.430458068847656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 2.262601137161255 24.417665481567383
  batch 40 loss: 1.791654109954834, 2.262601137161255, 24.417665481567383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921561002731323 2.2641372680664062 24.433528900146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923686504364014 1.9261205196380615 21.053573608398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033076286316 2.2608046531677246 24.40007972717285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916178703308105 2.038839817047119 22.180015563964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924582958221436 2.174962043762207 23.542078018188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921481132507324 2.138793706893921 23.180086135864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920483350753784 1.9356746673583984 21.148794174194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.071575164794922 22.507652282714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916061878204346 2.250765800476074 24.299264907836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920327186584473 1.881913423538208 20.61116600036621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910890579223633 2.1043553352355957 22.834640502929688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923297882080078 2.1650278568267822 23.442607879638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922149896621704 2.1031672954559326 22.823888778686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925379276275635 2.1922695636749268 23.715232849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918343544006348 1.9285197257995605 21.0770320892334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925223112106323 2.011229991912842 21.904823303222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680025100708 2.286973714828491 24.662418365478516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926596403121948 1.9040589332580566 20.833248138427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930734157562256 2.4902396202087402 26.695470809936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921202182769775 2.3459765911102295 25.25188636779785
  batch 60 loss: 1.7921202182769775, 2.3459765911102295, 25.25188636779785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792702078819275 2.3567183017730713 25.35988426208496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916841506958008 2.4490630626678467 26.28231430053711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925125360488892 2.234121799468994 24.133729934692383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921103239059448 2.3484785556793213 25.27689552307129
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.79264235496521 1.7250598669052124 19.043241500854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918591499328613 2.2424304485321045 24.216163635253906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.25175142288208 24.309288024902344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915701866149902 1.8376222848892212 20.16779327392578
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894793510437 1.834370493888855 20.13559913635254
Total LOSS train 23.867601717435395 valid 22.207211017608643
CE LOSS train 1.7922280733401958 valid 0.44797369837760925
Contrastive LOSS train 2.207537375963651 valid 0.45859262347221375
EPOCH 279:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922083139419556 2.192176342010498 23.713972091674805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915476560592651 2.3197367191314697 24.988914489746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792444109916687 2.0007271766662598 21.799715042114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923496961593628 2.1065566539764404 22.8579158782959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920522689819336 2.2094008922576904 23.886062622070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928320169448853 2.198155641555786 23.77438735961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791839361190796 2.3068089485168457 24.859928131103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.2127299308776855 23.919198989868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917299270629883 2.09380841255188 22.729812622070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.12483549118042 23.040096282958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792676329612732 2.474154233932495 26.53421974182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923840284347534 2.3199217319488525 24.991600036621094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929154634475708 2.1116106510162354 22.909021377563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925955057144165 2.1858363151550293 23.650959014892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911059856414795 2.2913713455200195 24.704818725585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792683720588684 2.3316726684570312 25.109411239624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792549729347229 2.160930633544922 23.40185546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923929691314697 2.41093373298645 25.901729583740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792299509048462 2.0925662517547607 22.71796226501465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915942668914795 2.187640428543091 23.667997360229492
  batch 20 loss: 1.7915942668914795, 2.187640428543091, 23.667997360229492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79242742061615 1.666819453239441 18.460620880126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924156188964844 2.218857765197754 23.980993270874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921977043151855 2.0865583419799805 22.65778160095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926695346832275 2.0303292274475098 22.09596061706543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923678159713745 2.355854034423828 25.350908279418945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924752235412598 2.1605939865112305 23.398414611816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928290367126465 2.288029670715332 24.673126220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918643951416016 2.1383092403411865 23.174957275390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793274164199829 2.3654420375823975 25.447694778442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915208339691162 1.9179530143737793 20.971052169799805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925376892089844 2.4021682739257812 25.814220428466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914148569107056 2.490631341934204 26.69772720336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924102544784546 2.2041826248168945 23.83423614501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925286293029785 2.1059634685516357 22.852163314819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924847602844238 2.16593861579895 23.45186996459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792582631111145 2.3107595443725586 24.900178909301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925066947937012 2.2456984519958496 24.249492645263672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916252613067627 2.2685253620147705 24.476879119873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921366691589355 2.380260705947876 25.594743728637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 1.9537620544433594 21.329275131225586
  batch 40 loss: 1.79165518283844, 1.9537620544433594, 21.329275131225586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921565771102905 2.2759392261505127 24.55154800415039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923688888549805 1.9172415733337402 20.964786529541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792033314704895 2.2249467372894287 24.041501998901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 2.0891544818878174 22.683162689208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924582958221436 1.6341809034347534 18.134265899658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792148470878601 2.27465558052063 24.53870391845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048692703247 2.0405619144439697 22.197668075561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7919002771377563 2.1901259422302246 23.693161010742188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 2.153384208679199 23.325448989868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920329570770264 2.0129594802856445 21.921627044677734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791089653968811 2.200277805328369 23.793867111206055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923301458358765 2.465789556503296 26.450225830078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79221510887146 1.6987183094024658 18.77939796447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792538046836853 2.1008474826812744 22.80101203918457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791834831237793 2.516965389251709 26.961490631103516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925225496292114 2.206406831741333 23.856590270996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792680025100708 2.44815993309021 26.274280548095703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926596403121948 1.9751940965652466 21.544601440429688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930731773376465 2.3537728786468506 25.33080291748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921202182769775 2.241370677947998 24.205827713012695
  batch 60 loss: 1.7921202182769775, 2.241370677947998, 24.205827713012695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927019596099854 2.3602473735809326 25.39517593383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916842699050903 2.1300227642059326 23.0919132232666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792512059211731 2.270487070083618 24.49738311767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921103239059448 2.3019936084747314 24.81204605102539
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926422357559204 1.4886093139648438 16.678735733032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918592691421509 2.120839834213257 23.00025749206543
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917745113372803 2.023409605026245 22.02587127685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915701866149902 1.8270151615142822 20.061721801757812
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894793510437 1.7482529878616333 19.274425506591797
Total LOSS train 23.586109220064603 valid 21.090569019317627
CE LOSS train 1.792228469481835 valid 0.44797369837760925
Contrastive LOSS train 2.1793880701065063 valid 0.4370632469654083
EPOCH 280:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922084331512451 2.120429039001465 22.996498107910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915477752685547 2.435943365097046 26.150981903076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792443871498108 2.063051462173462 22.422958374023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923494577407837 2.2014272212982178 23.806621551513672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920523881912231 2.12502384185791 23.04229164123535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928317785263062 2.1239161491394043 23.031993865966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918392419815063 1.9305249452590942 21.097089767456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.082331895828247 22.615217208862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.2793610095977783 24.58534049987793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.2014782428741455 23.806522369384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926758527755737 2.39156436920166 25.70831871032715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792382836341858 2.3156042098999023 24.94842529296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929152250289917 2.3292815685272217 25.085731506347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792595386505127 2.314819812774658 24.940792083740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911063432693481 2.3700220584869385 25.4913272857666
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926833629608154 2.4247548580169678 26.040231704711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925490140914917 2.3415327072143555 25.207876205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923929691314697 2.347054958343506 25.262943267822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792298674583435 2.1544699668884277 23.336999893188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915937900543213 2.083010673522949 22.621700286865234
  batch 20 loss: 1.7915937900543213, 2.083010673522949, 22.621700286865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924267053604126 2.027655601501465 22.06898307800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924145460128784 2.108457088470459 22.876985549926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921971082687378 1.9924979209899902 21.71717643737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926684617996216 2.1644999980926514 23.43766975402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923671007156372 2.0117251873016357 21.90962028503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792474389076233 2.1575891971588135 23.368366241455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928277254104614 2.2860727310180664 24.653554916381836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 1.9431722164154053 21.223587036132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793272852897644 2.1774649620056152 23.567922592163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520595550537 2.268481731414795 24.47633934020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925366163253784 2.062532901763916 22.417863845825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914143800735474 2.4877219200134277 26.66863441467285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924096584320068 2.074831962585449 22.540729522705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925280332565308 2.1599414348602295 23.391942977905273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924838066101074 2.496626853942871 26.758752822875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925819158554077 2.299712896347046 24.789710998535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925056219100952 2.249882459640503 24.291330337524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916247844696045 1.7908798456192017 19.700422286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792136549949646 2.2028920650482178 23.82105827331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.190016269683838 23.691816329956055
  batch 40 loss: 1.7916548252105713, 2.190016269683838, 23.691816329956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921562194824219 2.3878655433654785 25.67081069946289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923686504364014 2.150242805480957 23.294795989990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920328378677368 2.3304362297058105 25.09639549255371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916181087493896 2.188181161880493 23.673429489135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924578189849854 2.142242670059204 23.21488380432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792148232460022 2.148951768875122 23.281665802001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048454284668 1.960662841796875 21.398677825927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.116090774536133 22.952808380126953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 2.1102700233459473 22.894306182861328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920325994491577 2.053065538406372 22.32268714904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910900115966797 2.2555971145629883 24.347061157226562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923295497894287 2.2343297004699707 24.1356258392334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922148704528809 2.2419731616973877 24.211946487426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792537808418274 2.190089464187622 23.693431854248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918347120285034 2.3216712474823 25.008546829223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925219535827637 1.9740291833877563 21.532814025878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926795482635498 2.0879762172698975 22.672441482543945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926592826843262 2.1063544750213623 22.856204986572266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930725812911987 2.448664426803589 26.27971649169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921199798583984 2.150637149810791 23.298490524291992
  batch 60 loss: 1.7921199798583984, 2.150637149810791, 23.298490524291992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927014827728271 2.078864336013794 22.581344604492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.1032180786132812 22.823863983154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925117015838623 2.0770461559295654 22.562973022460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792110562324524 2.3313915729522705 25.10602569580078
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926419973373413 1.8286232948303223 20.078874588012695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79185950756073 1.8890565633773804 20.682424545288086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917746305465698 1.818648099899292 19.978256225585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915706634521484 1.5310816764831543 17.102386474609375
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918949127197266 1.5015833377838135 16.807727813720703
Total LOSS train 23.60864847623385 valid 18.642698764801025
CE LOSS train 1.7922280550003051 valid 0.44797372817993164
Contrastive LOSS train 2.181642039005573 valid 0.37539583444595337
EPOCH 281:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922085523605347 1.9159413576126099 20.951622009277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915483713150024 2.377004861831665 25.561595916748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924439907073975 2.0690653324127197 22.483097076416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923493385314941 2.134343385696411 23.13578224182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792051911354065 2.1445343494415283 23.237394332885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928311824798584 2.3005223274230957 24.798053741455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918394804000854 2.3756818771362305 25.54865837097168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918978929519653 2.073293685913086 22.52483558654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 2.1068718433380127 22.860448837280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.1759097576141357 23.550838470458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926756143569946 2.505988359451294 26.852558135986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923833131790161 2.4238035678863525 26.030418395996094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929147481918335 2.2159013748168945 23.951929092407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925950288772583 2.298222064971924 24.77481460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911065816879272 2.3398613929748535 25.189720153808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926830053329468 2.360610008239746 25.39878273010254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925492525100708 1.987911343574524 21.671663284301758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923924922943115 2.1218438148498535 23.01082992553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922991514205933 2.1412174701690674 23.2044734954834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915942668914795 2.2038605213165283 23.830198287963867
  batch 20 loss: 1.7915942668914795, 2.2038605213165283, 23.830198287963867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924269437789917 2.087256908416748 22.664997100830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792414665222168 2.2831084728240967 24.62350082397461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921969890594482 2.086273431777954 22.654930114746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926685810089111 2.255887269973755 24.35154151916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366862297058 2.606506824493408 27.857433319091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792474389076233 2.3375701904296875 25.168176651000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928276062011719 2.2961816787719727 24.7546443939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918637990951538 2.165301561355591 23.44487953186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793272614479065 2.2136495113372803 23.929767608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915207147598267 2.0582094192504883 22.373615264892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925364971160889 2.3914923667907715 25.707460403442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914146184921265 2.3357434272766113 25.148847579956055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924093008041382 2.144049644470215 23.232906341552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925275564193726 2.256314754486084 24.355676651000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924836874008179 2.5069308280944824 26.861793518066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925819158554077 2.2656471729278564 24.449052810668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925056219100952 2.196035146713257 23.752857208251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916247844696045 2.009706735610962 21.888690948486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921361923217773 2.1541013717651367 23.333148956298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.1145708560943604 22.937362670898438
  batch 40 loss: 1.7916544675827026, 2.1145708560943604, 22.937362670898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921558618545532 2.2222535610198975 24.014692306518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923682928085327 2.0342936515808105 22.135305404663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920328378677368 2.1520962715148926 23.3129940032959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916183471679688 2.3089349269866943 24.88096809387207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924575805664062 2.2518413066864014 24.310871124267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921478748321533 2.2317862510681152 24.11001205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920479774475098 2.05228853225708 22.314931869506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918998003005981 2.279541492462158 24.58731460571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916063070297241 2.2339699268341064 24.131305694580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920325994491577 1.9870244264602661 21.662277221679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910903692245483 2.238548755645752 24.176576614379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923294305801392 2.3238120079040527 25.03045082092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922147512435913 2.124964714050293 23.04186248779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925372123718262 2.222212791442871 24.014665603637695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791835069656372 2.3507277965545654 25.299114227294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792521595954895 2.0642974376678467 22.435497283935547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926790714263916 2.4501540660858154 26.294219970703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792658805847168 1.9669893980026245 21.46255111694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79307222366333 2.1863062381744385 23.6561336517334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921197414398193 2.313807487487793 24.930194854736328
  batch 60 loss: 1.7921197414398193, 2.313807487487793, 24.930194854736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7927008867263794 2.278367280960083 24.576374053955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916842699050903 1.9953473806381226 21.745159149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792511224746704 2.1228418350219727 23.02092933654785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921104431152344 2.2659904956817627 24.452014923095703
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926417589187622 1.919634461402893 20.98898696899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918592691421509 2.211674928665161 23.908607482910156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917745113372803 2.206230640411377 23.854080200195312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915704250335693 1.9155197143554688 20.946767807006836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.965384602546692 21.445741653442383
Total LOSS train 23.886836946927584 valid 22.538799285888672
CE LOSS train 1.7922279596328736 valid 0.4479736387729645
Contrastive LOSS train 2.2094609040480395 valid 0.491346150636673
EPOCH 282:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792208194732666 2.18149995803833 23.607206344604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915476560592651 2.38765811920166 25.668128967285156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924433946609497 2.262423038482666 24.41667366027832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923493385314941 2.098170757293701 22.774057388305664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920516729354858 2.0613465309143066 22.405515670776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792830467224121 2.160944700241089 23.402278900146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 2.3881635665893555 25.67347526550293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918974161148071 2.2106728553771973 23.898624420166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791730284690857 2.1520094871520996 23.311826705932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917406558990479 1.9749605655670166 21.541345596313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926748991012573 2.3276901245117188 25.069576263427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923833131790161 2.328552007675171 25.077903747558594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929143905639648 2.299992561340332 24.79283905029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925946712493896 2.4533326625823975 26.3259220123291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911068201065063 2.1517958641052246 23.309066772460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926828861236572 2.273980140686035 24.53248405456543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925490140914917 2.2528035640716553 24.320585250854492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923922538757324 2.2859015464782715 24.651409149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922989130020142 2.138667583465576 23.17897605895996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791594386100769 2.0262699127197266 22.054292678833008
  batch 20 loss: 1.791594386100769, 2.0262699127197266, 22.054292678833008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924264669418335 2.142014503479004 23.21257209777832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924147844314575 2.0605597496032715 22.398014068603516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792197346687317 1.8889743089675903 20.681941986083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926685810089111 2.1710240840911865 23.502910614013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366862297058 2.4562418460845947 26.35478401184082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924742698669434 2.0951929092407227 22.744403839111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792827844619751 2.382352113723755 25.616348266601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918639183044434 2.041654109954834 22.208406448364258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932720184326172 2.1247353553771973 23.040624618530273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791520118713379 2.075183629989624 22.543357849121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925351858139038 2.379981756210327 25.59235382080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914150953292847 2.313664674758911 24.92806053161621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924093008041382 2.136632204055786 23.15873146057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925280332565308 1.7310773134231567 19.103302001953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924836874008179 2.2369351387023926 24.161834716796875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925803661346436 2.2900404930114746 24.69298553466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925058603286743 2.289229154586792 24.684797286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791624665260315 2.151648998260498 23.308115005493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921355962753296 2.2743985652923584 24.536121368408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 1.920188546180725 20.993539810180664
  batch 40 loss: 1.791654348373413, 1.920188546180725, 20.993539810180664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921552658081055 2.29764461517334 24.768600463867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923672199249268 2.1953964233398438 23.7463321685791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920318841934204 2.1587233543395996 23.3792667388916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.125889301300049 23.05051040649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792456030845642 2.0626261234283447 22.418716430664062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921475172042847 2.285322904586792 24.645376205444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920478582382202 1.6536275148391724 18.328323364257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 1.5800185203552246 17.592084884643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916063070297241 2.2056849002838135 23.84845542907715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920315265655518 1.9418690204620361 21.210721969604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910897731781006 2.1787288188934326 23.578378677368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923284769058228 2.125310182571411 23.045429229736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922136783599854 1.972424864768982 21.516462326049805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925360202789307 2.040496587753296 22.19750213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918336391448975 2.1998329162597656 23.790163040161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925207614898682 2.1442081928253174 23.234601974487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926777601242065 2.006194591522217 21.854623794555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792657732963562 1.9978337287902832 21.770994186401367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930710315704346 2.5763590335845947 27.55666160583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921191453933716 2.1996490955352783 23.788610458374023
  batch 60 loss: 1.7921191453933716, 2.1996490955352783, 23.788610458374023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792699933052063 2.293818235397339 24.73088264465332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.0546836853027344 22.338520050048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925105094909668 2.1353135108947754 23.145645141601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921091318130493 2.168332099914551 23.47542953491211
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926400899887085 1.676784634590149 18.560487747192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.435471296310425 26.14657211303711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917733192443848 2.41616153717041 25.953388214111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791569709777832 2.075366735458374 22.545238494873047
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918939590454102 2.1506316661834717 23.29821014404297
Total LOSS train 23.30843344468337 valid 24.485852241516113
CE LOSS train 1.7922274662898137 valid 0.44797348976135254
Contrastive LOSS train 2.1516205824338472 valid 0.5376579165458679
EPOCH 283:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206883430481 2.2756922245025635 24.549129486083984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915475368499756 2.502600908279419 26.817556381225586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924420833587646 2.323303699493408 25.02547836303711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923479080200195 2.0426995754241943 22.219345092773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920513153076172 2.3044373989105225 24.83642578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928295135498047 1.9404593706130981 21.197423934936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918386459350586 2.272794008255005 24.519779205322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918970584869385 1.953413486480713 21.326030731201172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.092125177383423 22.712980270385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 2.057459592819214 22.366336822509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926740646362305 2.447200059890747 26.26467514038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923818826675415 2.575932502746582 27.551706314086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929130792617798 2.2657110691070557 24.450023651123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925937175750732 2.085951805114746 22.652111053466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911065816879272 2.220268726348877 23.993793487548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926814556121826 1.9877662658691406 21.67034339904785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925477027893066 2.3704488277435303 25.49703598022461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923914194107056 2.38423752784729 25.634765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792297601699829 2.056572198867798 22.35801887512207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915937900543213 1.9171489477157593 20.963083267211914
  batch 20 loss: 1.7915937900543213, 1.9171489477157593, 20.963083267211914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924253940582275 2.14566969871521 23.249122619628906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792413353919983 2.326312303543091 25.0555362701416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921966314315796 1.8841259479522705 20.633455276489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792667269706726 2.0211453437805176 22.004119873046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923656702041626 2.5753376483917236 27.54574203491211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924730777740479 2.354576349258423 25.33823585510254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928264141082764 2.330057382583618 25.093399047851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918634414672852 2.098031997680664 22.77218246459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932707071304321 2.325078248977661 25.044052124023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915204763412476 2.35613751411438 25.352895736694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925350666046143 2.452393054962158 26.316465377807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791414499282837 2.408435821533203 25.87577247619629
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924083471298218 2.3338308334350586 25.13071632385254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925264835357666 2.3262362480163574 25.054889678955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924823760986328 2.354120969772339 25.33369255065918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580485343933 2.4337117671966553 26.129697799682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925044298171997 2.2841596603393555 24.63410186767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916244268417358 2.108104705810547 22.872671127319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921353578567505 2.292976140975952 24.72189712524414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916542291641235 1.7998707294464111 19.790361404418945
  batch 40 loss: 1.7916542291641235, 1.7998707294464111, 19.790361404418945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792155146598816 2.055863380432129 22.350788116455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923671007156372 2.079481363296509 22.587181091308594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920317649841309 2.3680198192596436 25.472230911254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916176319122314 2.199599266052246 23.78761100769043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924561500549316 2.2074692249298096 23.86714744567871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921473979949951 2.161006450653076 23.402212142944336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 1.6714171171188354 18.50621795654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918990850448608 1.9924789667129517 21.71668815612793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 2.3506503105163574 25.298110961914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920316457748413 2.1742770671844482 23.53480339050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910903692245483 2.157740592956543 23.36849594116211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923282384872437 2.186840057373047 23.660728454589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792214035987854 1.9230595827102661 21.022809982299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925361394882202 2.061993360519409 22.4124698638916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918339967727661 2.1932272911071777 23.72410774230957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925208806991577 2.2670037746429443 24.46255874633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 2.523921489715576 27.03189468383789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926573753356934 1.677817702293396 18.57083511352539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930704355239868 2.559943437576294 27.39250373840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792119026184082 2.302420139312744 24.81631851196289
  batch 60 loss: 1.792119026184082, 2.302420139312744, 24.81631851196289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926996946334839 2.289346933364868 24.686168670654297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.241316318511963 24.204845428466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925106287002563 2.283583641052246 24.628347396850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921091318130493 2.109246253967285 22.884571075439453
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926394939422607 1.8554219007492065 20.34685707092285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.139864683151245 23.190505981445312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.210745334625244 23.89922523498535
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915700674057007 2.095543622970581 22.747007369995117
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918943166732788 1.8408442735671997 20.20033836364746
Total LOSS train 23.819870053804838 valid 22.50926923751831
CE LOSS train 1.7922269436029288 valid 0.4479735791683197
Contrastive LOSS train 2.2027643258755023 valid 0.4602110683917999
EPOCH 284:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206883430481 2.282646417617798 24.618671417236328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915472984313965 2.62709641456604 28.062511444091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924416065216064 2.0681402683258057 22.473844528198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923475503921509 2.1457738876342773 23.250085830688477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920507192611694 2.3679206371307373 25.47125816345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928287982940674 2.224778175354004 24.040611267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918390035629272 2.3576605319976807 25.368444442749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918972969055176 2.1246957778930664 23.038854598999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.236529588699341 24.157024383544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 1.9597954750061035 21.389694213867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926735877990723 2.5420289039611816 27.212961196899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792381763458252 2.530404567718506 27.09642791748047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929120063781738 2.2734904289245605 24.527816772460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 2.307861566543579 24.87120819091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911065816879272 2.137242555618286 23.163532257080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792681097984314 2.355924129486084 25.3519229888916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925474643707275 2.2501626014709473 24.294172286987305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391061782837 2.2291922569274902 24.084314346313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922972440719604 2.1046512126922607 22.838809967041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915935516357422 2.2129287719726562 23.920881271362305
  batch 20 loss: 1.7915935516357422, 2.2129287719726562, 23.920881271362305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924250364303589 2.237783908843994 24.170263290405273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924129962921143 2.0703437328338623 22.495851516723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921956777572632 2.06705904006958 22.462785720825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926663160324097 2.031891107559204 22.111576080322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923656702041626 2.4314677715301514 26.107044219970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924726009368896 2.242844343185425 24.220916748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928255796432495 2.234100818634033 24.133832931518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791862964630127 2.2168612480163574 23.96047592163086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932699918746948 2.3824784755706787 25.61805534362793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915204763412476 2.111372947692871 22.905250549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925348281860352 1.969901442527771 21.49155044555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914146184921265 2.259290933609009 24.384323120117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924079895019531 2.184323787689209 23.63564682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925258874893188 2.3571197986602783 25.363723754882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924820184707642 2.372579336166382 25.51827621459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580008506775 2.132683515548706 23.119415283203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792503833770752 2.0148227214813232 21.940731048583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916243076324463 1.7713369131088257 19.504993438720703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921350002288818 2.3853280544281006 25.645416259765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916539907455444 2.34710431098938 25.262697219848633
  batch 40 loss: 1.7916539907455444, 2.34710431098938, 25.262697219848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921547889709473 2.292896032333374 24.721115112304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366623878479 1.9213312864303589 21.005678176879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920315265655518 2.292330026626587 24.71533203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791617512702942 2.2782974243164062 24.57459259033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924556732177734 1.7100322246551514 18.892778396606445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792146921157837 1.9604547023773193 21.39669418334961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792047142982483 1.733116865158081 19.12321662902832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.0297329425811768 22.0892276763916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916055917739868 2.181217908859253 23.603784561157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920310497283936 2.1580023765563965 23.372055053710938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910898923873901 2.2519850730895996 24.310941696166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923275232315063 2.136314868927002 23.155475616455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922134399414062 2.049074649810791 22.282958984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792535424232483 2.165285587310791 23.445390701293945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918338775634766 2.296841621398926 24.760250091552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925198078155518 1.883445382118225 20.62697410583496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926769256591797 2.3941407203674316 25.73408317565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926570177078247 2.275517702102661 24.547834396362305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930694818496704 2.4120705127716064 25.913774490356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921183109283447 2.1738412380218506 23.53053092956543
  batch 60 loss: 1.7921183109283447, 2.1738412380218506, 23.53053092956543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926989793777466 2.1590540409088135 23.38323974609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 2.2174911499023438 23.966594696044922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925097942352295 2.1094672679901123 22.887182235717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921088933944702 2.2493233680725098 24.285341262817383
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926394939422607 1.556753158569336 17.360170364379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.0931198596954346 22.72305679321289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917734384536743 2.069512367248535 22.486896514892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915698289871216 1.8383560180664062 20.17513084411621
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791893482208252 1.7088472843170166 18.8803653717041
Total LOSS train 23.6457090817965 valid 21.066362380981445
CE LOSS train 1.7922265162834754 valid 0.447973370552063
Contrastive LOSS train 2.185348253983718 valid 0.42721182107925415
EPOCH 285:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922065258026123 2.068366050720215 22.475866317749023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915470600128174 2.5085880756378174 26.87742805480957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924412488937378 2.091495990753174 22.707399368286133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923473119735718 2.1730520725250244 23.52286720275879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920504808425903 2.2756593227386475 24.54864501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928282022476196 2.1981892585754395 23.77471923828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.356844186782837 25.360279083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918962240219116 2.1563875675201416 23.355772018432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917290925979614 1.9959301948547363 21.75102996826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 1.9831212759017944 21.622953414916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792673110961914 2.2681593894958496 24.474267959594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923815250396729 2.4388175010681152 26.180557250976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929116487503052 2.245422601699829 24.24713706970215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 2.277479887008667 24.567392349243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911070585250854 2.3409111499786377 25.200218200683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926807403564453 2.2351233959198 24.1439151763916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925469875335693 2.142575740814209 23.218305587768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923907041549683 2.4863476753234863 26.655866622924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792297124862671 2.0133779048919678 21.926076889038086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915936708450317 2.136491537094116 23.156509399414062
  batch 20 loss: 1.7915936708450317, 2.136491537094116, 23.156509399414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924246788024902 2.1259703636169434 23.0521297454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924129962921143 2.2061352729797363 23.8537654876709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921956777572632 2.027269124984741 22.06488800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926661968231201 2.0420427322387695 22.213092803955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923654317855835 2.3821187019348145 25.61355209350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 1.9765790700912476 21.558263778686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928252220153809 2.018373727798462 21.9765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918627262115479 1.9328898191452026 21.120759963989258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932692766189575 2.3243374824523926 25.036643981933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915207147598267 2.2637667655944824 24.429189682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925344705581665 2.4518537521362305 26.311071395874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791414737701416 2.434817314147949 26.13958740234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924078702926636 2.310516119003296 24.89756965637207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925258874893188 2.1781005859375 23.573532104492188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924820184707642 2.214312791824341 23.935609817504883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925798892974854 2.2135403156280518 23.927982330322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925037145614624 2.2430479526519775 24.22298240661621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791624903678894 1.7653756141662598 19.44537925720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921353578567505 2.0000953674316406 21.793088912963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916544675827026 2.2928292751312256 24.719947814941406
  batch 40 loss: 1.7916544675827026, 2.2928292751312256, 24.719947814941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921546697616577 2.310023307800293 24.89238739013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923667430877686 1.9494671821594238 21.287036895751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920316457748413 2.1815297603607178 23.607330322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916181087493896 2.188237190246582 23.67399024963379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792455792427063 2.0313775539398193 22.106231689453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792147159576416 1.780057430267334 19.592721939086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792047381401062 2.0102038383483887 21.894086837768555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918988466262817 2.0113792419433594 21.905691146850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916061878204346 2.3966147899627686 25.757755279541016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920315265655518 2.15678334236145 23.359865188598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910906076431274 2.395260810852051 25.743698120117188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923275232315063 2.3077619075775146 24.86994743347168
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922134399414062 2.1055071353912354 22.8472843170166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792535424232483 2.221569299697876 24.008228302001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918338775634766 2.2641897201538086 24.433731079101562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925196886062622 2.028263807296753 22.075159072875977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926768064498901 2.3223867416381836 25.016544342041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926565408706665 2.098519802093506 22.777854919433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930691242218018 2.4600725173950195 26.393795013427734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921186685562134 2.1300928592681885 23.093046188354492
  batch 60 loss: 1.7921186685562134, 2.1300928592681885, 23.093046188354492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792698621749878 2.2716898918151855 24.509597778320312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916837930679321 2.343816041946411 25.229843139648438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925094366073608 2.1729555130004883 23.522064208984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921088933944702 2.1648287773132324 23.440397262573242
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792639136314392 1.7231876850128174 19.02451515197754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.347001314163208 25.261871337890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917734384536743 2.166461706161499 23.456390380859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915701866149902 1.978345513343811 21.57502555847168
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918938398361206 1.9907824993133545 21.699718475341797
Total LOSS train 23.61100936302772 valid 22.99825143814087
CE LOSS train 1.792226424584022 valid 0.44797345995903015
Contrastive LOSS train 2.181878293477572 valid 0.4976956248283386
EPOCH 286:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922065258026123 2.30635666847229 24.855772018432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915476560592651 2.3855366706848145 25.646913528442383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924413681030273 2.2604634761810303 24.397075653076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923475503921509 2.105882167816162 22.85116958618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920507192611694 2.25985050201416 24.39055633544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928283214569092 2.2384443283081055 24.177270889282227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79183828830719 2.307330846786499 24.86514663696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791896939277649 2.2055983543395996 23.847881317138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79172945022583 2.122757911682129 23.01930809020996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 1.9706099033355713 21.497838973999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926727533340454 2.4766499996185303 26.559173583984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923816442489624 2.311594247817993 24.908323287963867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792911410331726 2.179521322250366 23.588125228881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 2.2847518920898438 24.640111923217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911075353622437 2.2248289585113525 24.039396286010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926807403564453 2.421130895614624 26.003990173339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925471067428589 2.060861825942993 22.401165008544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391061782837 2.221909523010254 24.011486053466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922974824905396 2.0925564765930176 22.71786117553711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79159414768219 2.1298422813415527 23.090017318725586
  batch 20 loss: 1.79159414768219, 2.1298422813415527, 23.090017318725586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924246788024902 2.0385918617248535 22.178342819213867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924131155014038 2.3213489055633545 25.005903244018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921959161758423 2.03739595413208 22.166154861450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926663160324097 2.223496675491333 24.027631759643555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923654317855835 2.3513920307159424 25.306285858154297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924726009368896 2.233799695968628 24.130470275878906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928252220153809 2.1726043224334717 23.518869400024414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 2.3234329223632812 25.02619171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793269157409668 2.3370208740234375 25.16347885131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915210723876953 2.344106435775757 25.232585906982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925349473953247 2.4776291847229004 26.56882667541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914154529571533 2.201138973236084 23.802806854248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924078702926636 2.230644702911377 24.098854064941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792526125907898 2.238813877105713 24.1806640625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924822568893433 2.498466968536377 26.777151107788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792580008506775 2.2547318935394287 24.33989906311035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925041913986206 2.247494697570801 24.2674503326416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916254997253418 2.1293933391571045 23.085559844970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921355962753296 2.401325225830078 25.805387496948242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 2.3599345684051514 25.391000747680664
  batch 40 loss: 1.79165518283844, 2.3599345684051514, 25.391000747680664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792155146598816 2.3108673095703125 24.900827407836914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366862297058 2.068228006362915 22.474645614624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79203200340271 2.228698253631592 24.079015731811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916185855865479 2.1476809978485107 23.268428802490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792455792427063 2.123135805130005 23.023813247680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921472787857056 2.2678139209747314 24.470285415649414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 1.8973931074142456 20.76597785949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918994426727295 2.1758217811584473 23.55011558532715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916067838668823 2.2502248287200928 24.293855667114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920318841934204 2.095259189605713 22.7446231842041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910913228988647 2.339372158050537 25.184814453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923280000686646 2.219414710998535 23.986474990844727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922134399414062 2.1549508571624756 23.34172248840332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792535424232483 2.251086473464966 24.30340003967285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918342351913452 2.233860492706299 24.13043785095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925200462341309 2.0740160942077637 22.532682418823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926769256591797 2.424129009246826 26.033967971801758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926567792892456 1.9496710300445557 21.289365768432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930690050125122 2.229926109313965 24.092330932617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792119026184082 2.291764259338379 24.709762573242188
  batch 60 loss: 1.792119026184082, 2.291764259338379, 24.709762573242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926990985870361 2.1830520629882812 23.623220443725586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791684627532959 2.103351354598999 22.825199127197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792509913444519 1.7098199129104614 18.890708923339844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792109489440918 2.260183095932007 24.393939971923828
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926394939422607 1.6900416612625122 18.693056106567383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918589115142822 2.2349653244018555 24.141511917114258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.092957019805908 22.721343994140625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915709018707275 1.9366564750671387 21.15813636779785
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791894555091858 1.7495722770690918 19.28761863708496
Total LOSS train 23.89515031667856 valid 21.827152729034424
CE LOSS train 1.7922267015163715 valid 0.4479736387729645
Contrastive LOSS train 2.2102923668347874 valid 0.43739306926727295
EPOCH 287:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79220712184906 2.117553472518921 22.967742919921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791548490524292 2.401386022567749 25.805408477783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441725730896 2.1032259464263916 22.8247013092041
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79234778881073 2.1339406967163086 23.13175392150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920514345169067 2.1352384090423584 23.14443588256836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928283214569092 2.075127363204956 22.54410171508789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.335040807723999 25.142248153686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918975353240967 2.099479913711548 22.78669548034668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917304039001465 2.2075419425964355 23.867151260375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.164280891418457 23.43454933166504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926729917526245 2.499667167663574 26.789344787597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923816442489624 2.6573333740234375 28.36571502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929115295410156 2.1618452072143555 23.41136360168457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925927639007568 2.2870826721191406 24.663419723510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791108250617981 2.2233872413635254 24.024980545043945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926807403564453 2.221789836883545 24.01058006286621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925471067428589 1.8748582601547241 20.54113006591797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391061782837 2.3927080631256104 25.719470977783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922974824905396 1.8772798776626587 20.565095901489258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915946245193481 2.3566133975982666 25.357728958129883
  batch 20 loss: 1.7915946245193481, 2.3566133975982666, 25.357728958129883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924249172210693 2.1815121173858643 23.607545852661133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924132347106934 2.334258794784546 25.13500213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792196273803711 2.1419551372528076 23.211748123168945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926661968231201 1.8596782684326172 20.389448165893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792365550994873 2.224832773208618 24.040693283081055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924724817276 2.239468812942505 24.18716049194336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928249835968018 2.1663401126861572 23.456226348876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918636798858643 2.095759868621826 22.74946403503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932686805725098 2.21864652633667 23.979734420776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 2.145057439804077 23.242095947265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925349473953247 1.9147658348083496 20.940195083618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914156913757324 2.369371175765991 25.48512840270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924078702926636 2.0610105991363525 22.40251350402832
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925257682800293 2.2637429237365723 24.429954528808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924820184707642 2.4828076362609863 26.62055778503418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925797700881958 2.3814005851745605 25.606586456298828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792503833770752 2.2131783962249756 23.924287796020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916252613067627 2.1059889793395996 22.851516723632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921348810195923 2.1323554515838623 23.115690231323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.1650357246398926 23.4420108795166
  batch 40 loss: 1.7916548252105713, 2.1650357246398926, 23.4420108795166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921545505523682 2.2509963512420654 24.3021183013916
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792366862297058 1.9901394844055176 21.69375991821289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920317649841309 2.1451516151428223 23.243547439575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 2.2614781856536865 24.406400680541992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924554347991943 2.1559829711914062 23.352285385131836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792146921157837 2.2233078479766846 24.025224685668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920475006103516 2.1052613258361816 22.84465980529785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79189932346344 2.057021379470825 22.362112045288086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916067838668823 2.397368907928467 25.765296936035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920315265655518 2.0893986225128174 22.686017990112305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791092038154602 2.374192953109741 25.533021926879883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923274040222168 2.281865358352661 24.610980987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922134399414062 2.190939426422119 23.70160675048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925351858139038 1.9523582458496094 21.316118240356445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918342351913452 2.2439067363739014 24.23090171813965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925193309783936 2.213163137435913 23.924150466918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792676329612732 2.2252116203308105 24.0447940826416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926563024520874 2.167894124984741 23.47159767150879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930684089660645 2.4335854053497314 26.128921508789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792118787765503 2.409376382827759 25.885881423950195
  batch 60 loss: 1.792118787765503, 2.409376382827759, 25.885881423950195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792698621749878 2.3330466747283936 25.123165130615234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916845083236694 2.0485689640045166 22.277374267578125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925093173980713 2.1292078495025635 23.08458709716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921090126037598 2.346463203430176 25.25674057006836
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792638897895813 1.7845216989517212 19.637855529785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918589115142822 2.492938756942749 26.72124671936035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 2.279658794403076 24.588363647460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915709018707275 2.101161241531372 22.80318260192871
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918943166732788 2.118028402328491 22.972179412841797
Total LOSS train 23.766527674748346 valid 24.27124309539795
CE LOSS train 1.7922266776745135 valid 0.4479735791683197
Contrastive LOSS train 2.1974300953058097 valid 0.5295071005821228
EPOCH 288:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792206883430481 2.337749481201172 25.169702529907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915486097335815 2.554126501083374 27.332813262939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792441487312317 2.297285795211792 24.765300750732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923475503921509 2.3261382579803467 25.053730010986328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920514345169067 2.2596020698547363 24.388071060180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792827844619751 2.2264175415039062 24.057003021240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918391227722168 2.345989465713501 25.251733779907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918975353240967 2.022953510284424 22.02143096923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.2412729263305664 24.20446014404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917407751083374 2.0114030838012695 21.905771255493164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926726341247559 2.317824602127075 24.970918655395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923814058303833 2.535163402557373 27.14401626586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929112911224365 1.7338488101959229 19.131399154663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925924062728882 2.1060853004455566 22.853445053100586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911088466644287 2.253957509994507 24.330684661865234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926807403564453 2.42462158203125 26.038896560668945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925471067428589 2.165381908416748 23.446367263793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792391061782837 2.391623020172119 25.708620071411133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792297601699829 2.072481632232666 22.517112731933594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915951013565063 2.186413049697876 23.655725479125977
  batch 20 loss: 1.7915951013565063, 2.186413049697876, 23.655725479125977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924247980117798 2.070748805999756 22.499914169311523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924128770828247 2.204151153564453 23.833925247192383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921957969665527 1.9493926763534546 21.286123275756836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926658391952515 2.287747859954834 24.67014503479004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923651933670044 2.4556832313537598 26.349197387695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792472004890442 2.4468278884887695 26.260751724243164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928242683410645 2.259168863296509 24.384511947631836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 1.8538753986358643 20.330615997314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793267846107483 2.120743751525879 23.00070571899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 2.040358304977417 22.195104598999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925341129302979 2.0590386390686035 22.382919311523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914156913757324 2.328217029571533 25.073585510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792407512664795 2.229839324951172 24.090801239013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925255298614502 2.321239471435547 25.004920959472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924814224243164 2.16361927986145 23.428672790527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792579174041748 2.311831474304199 24.9108943939209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925032377243042 2.129246234893799 23.084964752197266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916251420974731 2.092341899871826 22.715045928955078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921347618103027 1.9961978197097778 21.754114151000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.057568073272705 22.367334365844727
  batch 40 loss: 1.7916548252105713, 2.057568073272705, 22.367334365844727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921545505523682 2.2394769191741943 24.18692398071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923662662506104 1.9989150762557983 21.781517028808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79203200340271 2.3861658573150635 25.653690338134766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 2.1954805850982666 23.746423721313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924553155899048 1.8837329149246216 20.629785537719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921468019485474 2.3447675704956055 25.239822387695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920472621917725 2.1397135257720947 23.18918228149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791899561882019 2.0224971771240234 22.016870498657227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916069030761719 2.2696549892425537 24.488157272338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920316457748413 1.8375639915466309 20.167673110961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791092038154602 2.522801160812378 27.01910400390625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923275232315063 2.365474224090576 25.447071075439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922132015228271 2.272087812423706 24.513092041015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925348281860352 2.279923677444458 24.59177017211914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918343544006348 2.278090000152588 24.57273292541504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792518973350525 1.9933192729949951 21.725711822509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926758527755737 2.486027240753174 26.65294647216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926563024520874 2.0920627117156982 22.71328353881836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930680513381958 2.628920316696167 28.082271575927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792118787765503 2.3285157680511475 25.0772762298584
  batch 60 loss: 1.792118787765503, 2.3285157680511475, 25.0772762298584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926980257034302 2.2464592456817627 24.25728988647461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791684627532959 1.9571453332901 21.36313819885254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792508840560913 2.2495229244232178 24.287738800048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921090126037598 2.0052249431610107 21.844358444213867
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792638897895813 1.3991951942443848 15.784590721130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918589115142822 2.224766254425049 24.039520263671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917742729187012 2.1969566345214844 23.761341094970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915711402893066 2.074679374694824 22.53836441040039
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918941974639893 2.0077290534973145 21.869184494018555
Total LOSS train 23.732367280813364 valid 23.05210256576538
CE LOSS train 1.7922265016115628 valid 0.4479735493659973
Contrastive LOSS train 2.19401407792018 valid 0.5019322633743286
EPOCH 289:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922066450119019 2.228133201599121 24.073537826538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791548490524292 2.4070212841033936 25.86176109313965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924411296844482 2.173088788986206 23.52332878112793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923462390899658 2.2447915077209473 24.24026107788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920502424240112 2.2152342796325684 23.944393157958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928273677825928 2.1559219360351562 23.352046966552734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.397552728652954 25.767364501953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918962240219116 2.0130083560943604 21.921979904174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.173203706741333 23.523767471313477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 2.1228203773498535 23.019943237304688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926710844039917 2.323842763900757 25.031099319458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923797369003296 2.5040247440338135 26.832626342773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929092645645142 2.203873872756958 23.831647872924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792590856552124 2.2551019191741943 24.343610763549805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911080121994019 2.310859203338623 24.899700164794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926791906356812 2.3571183681488037 25.363862991333008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925454378128052 2.2289631366729736 24.082176208496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923892736434937 2.343832492828369 25.230712890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922955751419067 2.0921244621276855 22.71354103088379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915937900543213 2.0204694271087646 21.996288299560547
  batch 20 loss: 1.7915937900543213, 2.0204694271087646, 21.996288299560547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924230098724365 2.0882833003997803 22.675256729125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792411208152771 2.151315450668335 23.305566787719727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921944856643677 2.1665728092193604 23.457921981811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926645278930664 1.639129877090454 18.183963775634766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923637628555298 2.179457902908325 23.586942672729492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924710512161255 2.379746437072754 25.589935302734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928229570388794 2.4079465866088867 25.872289657592773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 2.0136303901672363 21.928165435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932661771774292 2.132490396499634 23.1181697845459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915207147598267 2.1824138164520264 23.615659713745117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925326824188232 2.4801735877990723 26.594266891479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914150953292847 2.2694191932678223 24.485605239868164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924062013626099 2.2038187980651855 23.830595016479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925243377685547 2.1565587520599365 23.358112335205078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792480230331421 2.1733412742614746 23.525894165039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792577862739563 2.2126612663269043 23.919191360473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925022840499878 2.391500949859619 25.707509994506836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791624903678894 2.3203814029693604 24.995437622070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792134404182434 2.3786582946777344 25.578718185424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654348373413 2.11428165435791 22.934471130371094
  batch 40 loss: 1.791654348373413, 2.11428165435791, 22.934471130371094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921539545059204 2.201101064682007 23.803165435791016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923650741577148 2.185765027999878 23.65001678466797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792030930519104 2.337907552719116 25.171106338500977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916179895401 2.105809450149536 22.849712371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924542427062988 2.2840194702148438 24.632648468017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921459674835205 2.2603065967559814 24.395212173461914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920469045639038 1.8185166120529175 19.97721290588379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 1.7981125116348267 19.77302360534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916064262390137 2.269505500793457 24.486661911010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920305728912354 2.128086566925049 23.072895050048828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791092038154602 2.1784987449645996 23.576080322265625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79232656955719 2.3320934772491455 25.11326026916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922122478485107 2.0744874477386475 22.537086486816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79253351688385 2.26777720451355 24.470306396484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918332815170288 2.4269297122955322 26.06113052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925182580947876 2.1743149757385254 23.535667419433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926747798919678 2.5420730113983154 27.21340560913086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926547527313232 1.8504070043563843 20.296724319458008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930666208267212 2.535806894302368 27.151134490966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792117714881897 2.334033966064453 25.132457733154297
  batch 60 loss: 1.792117714881897, 2.334033966064453, 25.132457733154297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926968336105347 2.257686138153076 24.369558334350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.14168119430542 23.20849609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925078868865967 2.2360808849334717 24.153316497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921080589294434 2.3752591609954834 25.544700622558594
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926372289657593 1.7958109378814697 19.75074577331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918577194213867 2.214961051940918 23.94146728515625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917733192443848 2.1240713596343994 23.032485961914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915703058242798 1.8356807231903076 20.148378372192383
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918931245803833 1.748113989830017 19.273033142089844
Total LOSS train 23.903739224947415 valid 21.598841190338135
CE LOSS train 1.7922254452338586 valid 0.4479732811450958
Contrastive LOSS train 2.211151381639334 valid 0.4370284974575043
EPOCH 290:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792205572128296 2.1120424270629883 22.912630081176758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915477752685547 2.4496378898620605 26.287927627563477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924396991729736 2.093898057937622 22.731420516967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923460006713867 1.9492595195770264 21.284942626953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049765586853 2.230794668197632 24.09999656677246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792825698852539 2.067257881164551 22.465404510498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 1.98312509059906 21.623088836669922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918962240219116 2.3086905479431152 24.878803253173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917290925979614 2.267115354537964 24.46288299560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917399406433105 2.1238434314727783 23.030174255371094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926710844039917 2.4736850261688232 26.529521942138672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923799753189087 2.366122245788574 25.453601837158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792909026145935 2.1856417655944824 23.649328231811523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925909757614136 2.099769353866577 22.790285110473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911088466644287 2.310944080352783 24.900548934936523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792678713798523 2.396019458770752 25.752872467041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925453186035156 2.2578394412994385 24.370939254760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923892736434937 2.3830511569976807 25.622900009155273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792296290397644 2.1333155632019043 23.125452041625977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791594386100769 2.0791285037994385 22.58287811279297
  batch 20 loss: 1.791594386100769, 2.0791285037994385, 22.58287811279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924233675003052 2.104801654815674 22.840438842773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79241144657135 2.1968820095062256 23.761232376098633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921948432922363 2.0296285152435303 22.08847999572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926645278930664 2.11391019821167 22.931766510009766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923638820648193 2.535240411758423 27.14476776123047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924706935882568 2.318089008331299 24.973360061645508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928229570388794 2.430359363555908 26.096416473388672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918628454208374 1.7595171928405762 19.387035369873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793265700340271 2.282404899597168 24.6173152923584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 2.1349692344665527 23.14121437072754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925328016281128 2.4158201217651367 25.950733184814453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791416049003601 2.3337137699127197 25.12855339050293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924065589904785 2.175347328186035 23.545879364013672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792523980140686 1.9724044799804688 21.516569137573242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924801111221313 2.016784191131592 21.960323333740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792577862739563 2.3562302589416504 25.35487937927246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925022840499878 2.274291515350342 24.535417556762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916253805160522 2.100696325302124 22.7985897064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792134404182434 2.4096992015838623 25.889127731323242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.155447244644165 23.346126556396484
  batch 40 loss: 1.7916548252105713, 2.155447244644165, 23.346126556396484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921538352966309 2.246866464614868 24.260818481445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923650741577148 1.971818208694458 21.510547637939453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920305728912354 2.234989643096924 24.141925811767578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916178703308105 1.8078749179840088 19.8703670501709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924535274505615 2.1868975162506104 23.661428451538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921453714370728 2.2429685592651367 24.221830368041992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920459508895874 2.0699498653411865 22.491544723510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.2121098041534424 23.912996292114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916061878204346 2.2231903076171875 24.023509979248047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920302152633667 1.799163818359375 19.783668518066406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910922765731812 2.1834166049957275 23.62525749206543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792325735092163 2.1650164127349854 23.442489624023438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922117710113525 2.11049747467041 22.897186279296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533278465271 2.1568009853363037 23.360544204711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918332815170288 2.159014940261841 23.381982803344727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925177812576294 1.6846158504486084 18.6386775970459
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926743030548096 2.4674839973449707 26.467514038085938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 2.247223138809204 24.26488494873047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930654287338257 2.458442449569702 26.377490997314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921174764633179 2.2856547832489014 24.648666381835938
  batch 60 loss: 1.7921174764633179, 2.2856547832489014, 24.648666381835938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792696237564087 2.2096941471099854 23.889636993408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916837930679321 2.0054006576538086 21.84568977355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925074100494385 2.2206923961639404 23.999431610107422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921074628829956 2.347991704940796 25.272024154663086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926366329193115 1.7149561643600464 18.942197799682617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 2.4234635829925537 26.0264949798584
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917732000350952 2.4616479873657227 26.408252716064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915703058242798 2.1785593032836914 23.577163696289062
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918932437896729 2.2262187004089355 24.054080963134766
Total LOSS train 23.60763288644644 valid 25.01649808883667
CE LOSS train 1.79222524532905 valid 0.4479733109474182
Contrastive LOSS train 2.1815407569591816 valid 0.5565546751022339
EPOCH 291:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922052145004272 2.270897388458252 24.501178741455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915480136871338 2.6527187824249268 28.318735122680664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924394607543945 2.026393413543701 22.056373596191406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792345643043518 2.1100497245788574 22.89284324645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792049765586853 2.1324408054351807 23.116456985473633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928253412246704 2.1759798526763916 23.552623748779297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.0204758644104004 21.99659538269043
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918962240219116 2.1329715251922607 23.121612548828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.225100040435791 24.042728424072266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 2.295231342315674 24.74405288696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926700115203857 2.3572006225585938 25.364675521850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923794984817505 2.5237507820129395 27.02988624572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929081916809082 2.3716201782226562 25.509109497070312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925901412963867 2.3023040294647217 24.815631866455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79110848903656 2.2813124656677246 24.60423469543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926775217056274 2.325244188308716 25.04511833190918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925443649291992 2.280017137527466 24.592716217041016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923885583877563 2.277038812637329 24.562776565551758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792295217514038 2.105862617492676 22.850921630859375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915934324264526 2.0410268306732178 22.201862335205078
  batch 20 loss: 1.7915934324264526, 2.0410268306732178, 22.201862335205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924222946166992 2.0056047439575195 21.848468780517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924107313156128 2.0778276920318604 22.57068634033203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921937704086304 1.851682186126709 20.309017181396484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926629781723022 2.216090440750122 23.953567504882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792363166809082 2.5088260173797607 26.88062286376953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792470097541809 2.172410011291504 23.516571044921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792822003364563 2.2236387729644775 24.02920913696289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918623685836792 2.25028395652771 24.294702529907227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932647466659546 2.2792391777038574 24.585657119750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915210723876953 2.1115779876708984 22.90730094909668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925320863723755 2.2025113105773926 23.817644119262695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914154529571533 2.421318292617798 26.00459861755371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792405366897583 1.8838173151016235 20.630578994750977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925230264663696 1.931430697441101 21.106828689575195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792479395866394 2.4991989135742188 26.784467697143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925769090652466 2.122544288635254 23.018020629882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925010919570923 2.352508306503296 25.317584991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916247844696045 2.1472063064575195 23.263687133789062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921336889266968 2.360761880874634 25.399751663208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791654109954834 1.8277511596679688 20.06916618347168
  batch 40 loss: 1.791654109954834, 1.8277511596679688, 20.06916618347168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921531200408936 2.225417137145996 24.046323776245117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792364478111267 1.9733330011367798 21.525693893432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920308113098145 2.134488582611084 23.136917114257812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916179895401 2.0222365856170654 22.01398468017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924532890319824 1.9544413089752197 21.33686637878418
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921456098556519 2.1906943321228027 23.69908905029297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920461893081665 1.7612833976745605 19.40488052368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918986082077026 1.857269287109375 20.364591598510742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916066646575928 2.2049367427825928 23.840974807739258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920304536819458 2.14705228805542 23.262554168701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910925149917603 2.3106191158294678 24.89728355407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792325735092163 2.169786214828491 23.490188598632812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792211890220642 2.1089508533477783 22.8817195892334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925329208374023 2.2630248069763184 24.42278289794922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918336391448975 2.1071786880493164 22.86362075805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925173044204712 2.118939161300659 22.981908798217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792673945426941 2.3747668266296387 25.540342330932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 2.1674203872680664 23.46685791015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930651903152466 2.262024164199829 24.413307189941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921174764633179 2.3430707454681396 25.22282600402832
  batch 60 loss: 1.7921174764633179, 2.3430707454681396, 25.22282600402832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926957607269287 2.1335394382476807 23.128089904785156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916839122772217 2.1643447875976562 23.435131072998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925074100494385 2.1991021633148193 23.78352928161621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921078205108643 2.2991409301757812 24.783517837524414
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926366329193115 1.934072732925415 21.133363723754883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918583154678345 2.2740180492401123 24.532039642333984
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.0973494052886963 22.76526641845703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791570782661438 1.873711109161377 20.5286808013916
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791893482208252 1.8914387226104736 20.706279754638672
Total LOSS train 23.604686326246995 valid 22.133066654205322
CE LOSS train 1.7922248381834764 valid 0.447973370552063
Contrastive LOSS train 2.181246146788964 valid 0.4728596806526184
EPOCH 292:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922053337097168 2.131242513656616 23.104631423950195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915483713150024 2.3856098651885986 25.647645950317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924394607543945 2.037986993789673 22.17230987548828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923455238342285 2.0423355102539062 22.215700149536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920500040054321 2.079786539077759 22.589914321899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928248643875122 2.127047538757324 23.06330108642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79183828830719 2.292442798614502 24.716264724731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918965816497803 2.1108267307281494 22.900163650512695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.098375082015991 22.775480270385742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917401790618896 2.0821874141693115 22.613615036010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926698923110962 2.5234246253967285 27.026914596557617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792379379272461 2.236668348312378 24.1590633392334
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929075956344604 2.289504289627075 24.687950134277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792589783668518 2.1000783443450928 22.793373107910156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791109323501587 2.417224168777466 25.963350296020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792677640914917 2.4335696697235107 26.128374099731445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925444841384888 2.2720701694488525 24.513246536254883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923885583877563 2.2558226585388184 24.350616455078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922953367233276 2.076507806777954 22.557373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791594386100769 2.2230186462402344 24.021780014038086
  batch 20 loss: 1.791594386100769, 2.2230186462402344, 24.021780014038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924224138259888 1.8827240467071533 20.61966323852539
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924106121063232 2.144087076187134 23.233280181884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792194128036499 1.9323700666427612 21.115894317626953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926632165908813 2.0392110347747803 22.18477439880371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923632860183716 2.3794798851013184 25.5871639251709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924699783325195 1.9405125379562378 21.197593688964844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928215265274048 2.2997336387634277 24.790159225463867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 2.179795980453491 23.58982276916504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932640314102173 2.3572845458984375 25.36610984802246
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915215492248535 2.2813761234283447 24.605281829833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925320863723755 2.474029302597046 26.532825469970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914159297943115 2.309333562850952 24.88475227355957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792405605316162 2.1646716594696045 23.439123153686523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925231456756592 2.4802567958831787 26.595090866088867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924796342849731 2.3173470497131348 24.96595001220703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925771474838257 1.9987808465957642 21.780385971069336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792501449584961 2.239208936691284 24.18459129333496
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916252613067627 2.2441296577453613 24.232921600341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921340465545654 2.2187702655792236 23.979835510253906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.144841194152832 23.240066528320312
  batch 40 loss: 1.7916548252105713, 2.144841194152832, 23.240066528320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921534776687622 2.3164353370666504 24.956506729125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923648357391357 1.9600359201431274 21.392723083496094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792030930519104 2.2533576488494873 24.325607299804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916187047958374 2.3623220920562744 25.414838790893555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924535274505615 2.0489249229431152 22.28170394897461
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921456098556519 2.1135365962982178 22.92751121520996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920465469360352 1.8970468044281006 20.762516021728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918988466262817 2.2665038108825684 24.456937789916992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916070222854614 2.261744737625122 24.409053802490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920308113098145 2.106369972229004 22.855730056762695
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791093111038208 2.3877451419830322 25.66854476928711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792325735092163 2.1334686279296875 23.127012252807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792211890220642 2.0874781608581543 22.666994094848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792533040046692 2.2071478366851807 23.864011764526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918339967727661 2.4717624187469482 26.509458541870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925171852111816 2.203251600265503 23.82503318786621
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79267418384552 2.444783926010132 26.240514755249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926537990570068 2.0815279483795166 22.607933044433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793065071105957 2.3975377082824707 25.76844024658203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921178340911865 2.2754297256469727 24.546415328979492
  batch 60 loss: 1.7921178340911865, 2.2754297256469727, 24.546415328979492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926961183547974 2.3354697227478027 25.14739418029785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916847467422485 2.122252941131592 23.01421546936035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792507529258728 2.0103864669799805 21.896371841430664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921080589294434 2.09814190864563 22.773527145385742
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926366329193115 1.7296793460845947 19.08942985534668
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918585538864136 2.0529000759124756 22.320859909057617
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791774034500122 2.089045524597168 22.68222999572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915712594985962 1.8759214878082275 20.550785064697266
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918940782546997 1.8185983896255493 19.97787857055664
Total LOSS train 23.76391968360314 valid 21.382938385009766
CE LOSS train 1.7922250215823834 valid 0.4479735195636749
Contrastive LOSS train 2.1971694652850813 valid 0.45464959740638733
EPOCH 293:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922056913375854 2.1367576122283936 23.15978240966797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915489673614502 2.285064458847046 24.642194747924805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792439341545105 2.249598503112793 24.288423538208008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923457622528076 2.2362351417541504 24.154695510864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920503616333008 2.2365453243255615 24.15750503540039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928249835968018 2.192283868789673 23.71566390991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918387651443481 2.393747568130493 25.72931480407715
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918970584869385 2.0443367958068848 22.23526382446289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917306423187256 2.2082743644714355 23.874475479125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917406558990479 2.1225991249084473 23.017730712890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926701307296753 2.3090286254882812 24.88295555114746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923798561096191 2.326998710632324 25.062366485595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929078340530396 2.192652940750122 23.719436645507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925901412963867 2.4060864448547363 25.85345458984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911102771759033 2.3840482234954834 25.631593704223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926779985427856 2.1329452991485596 23.12213134765625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925446033477783 2.314202070236206 24.934566497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792388916015625 2.3584752082824707 25.377140045166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922958135604858 2.0957579612731934 22.749876022338867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915951013565063 2.154967784881592 23.34127426147461
  batch 20 loss: 1.7915951013565063, 2.154967784881592, 23.34127426147461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792422890663147 1.9780112504959106 21.57253646850586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792410969734192 2.1639273166656494 23.431684494018555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921944856643677 1.8620517253875732 20.41271209716797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792663335800171 2.1687092781066895 23.479755401611328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923636436462402 2.484978675842285 26.64215087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924703359603882 2.3112778663635254 24.905248641967773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928217649459839 2.3185923099517822 24.978744506835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918633222579956 1.839415431022644 20.186016082763672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932640314102173 1.8429621458053589 20.222885131835938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79152250289917 2.2699334621429443 24.49085807800293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925323247909546 2.518704414367676 26.979576110839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791416883468628 2.3572885990142822 25.364301681518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924057245254517 2.1508543491363525 23.300949096679688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925233840942383 2.2001991271972656 23.794513702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924796342849731 2.320857286453247 25.001052856445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925771474838257 2.3889663219451904 25.682241439819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792501449584961 2.299443244934082 24.78693389892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916254997253418 2.214061975479126 23.9322452545166
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792134165763855 2.4700686931610107 26.492820739746094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916553020477295 2.297107219696045 24.762727737426758
  batch 40 loss: 1.7916553020477295, 2.297107219696045, 24.762727737426758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792153000831604 2.315629482269287 24.948448181152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923643589019775 2.231893301010132 24.111297607421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920305728912354 2.1757495403289795 23.54952621459961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 2.2561655044555664 24.353273391723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924528121948242 2.101168394088745 22.80413818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921452522277832 2.1796789169311523 23.58893394470215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792046070098877 2.045523166656494 22.247276306152344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918987274169922 2.1999566555023193 23.791465759277344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916070222854614 2.3341360092163086 25.132966995239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920308113098145 2.101365566253662 22.805686950683594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910938262939453 2.1971850395202637 23.7629451751709
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923256158828735 2.318622589111328 24.978551864624023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922117710113525 2.1604864597320557 23.397075653076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925325632095337 2.2389168739318848 24.181699752807617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918338775634766 2.4832215309143066 26.624048233032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925169467926025 2.1808347702026367 23.60086441040039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926733493804932 2.3769896030426025 25.56256866455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926533222198486 2.2203705310821533 23.99635887145996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793063998222351 2.5240941047668457 27.03400421142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921172380447388 2.2725937366485596 24.518054962158203
  batch 60 loss: 1.7921172380447388, 2.2725937366485596, 24.518054962158203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792695164680481 2.2632265090942383 24.42496109008789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916843891143799 1.9119541645050049 20.911226272583008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925065755844116 2.082850694656372 22.621013641357422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921078205108643 2.3428149223327637 25.220258712768555
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926361560821533 1.95920729637146 21.38471031188965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918585538864136 2.1691300868988037 23.4831600189209
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 1.9859795570373535 21.651569366455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915713787078857 1.8497117757797241 20.28868865966797
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918938398361206 1.718088150024414 18.972774505615234
Total LOSS train 24.05571007361779 valid 21.099048137664795
CE LOSS train 1.7922250986099244 valid 0.44797345995903015
Contrastive LOSS train 2.2263484936494096 valid 0.4295220375061035
EPOCH 294:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922054529190063 2.1643877029418945 23.43608283996582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915493249893188 2.453991651535034 26.331466674804688
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924392223358154 2.172027111053467 23.512710571289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792345404624939 2.1704509258270264 23.496854782104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920502424240112 2.284210681915283 24.6341552734375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928242683410645 2.2406654357910156 24.199478149414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918386459350586 2.3486366271972656 25.27820587158203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918968200683594 2.307053327560425 24.862430572509766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917299270629883 2.0287346839904785 22.07907485961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917406558990479 2.063164234161377 22.423381805419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926692962646484 2.2699036598205566 24.4917049407959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923791408538818 2.5899527072906494 27.691905975341797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929071187973022 2.224055767059326 24.033466339111328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792589545249939 2.2190234661102295 23.982824325561523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911102771759033 2.2872138023376465 24.663249969482422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926768064498901 2.2042360305786133 23.835037231445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792543888092041 2.2986106872558594 24.778650283813477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923885583877563 2.2433879375457764 24.226268768310547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922953367233276 2.045595645904541 22.24825096130371
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915953397750854 2.034525156021118 22.1368465423584
  batch 20 loss: 1.7915953397750854, 2.034525156021118, 22.1368465423584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924224138259888 2.140375852584839 23.196182250976562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924106121063232 2.146615505218506 23.25856590270996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792194128036499 1.9441566467285156 21.233760833740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926627397537231 2.2371325492858887 24.163990020751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923632860183716 2.4459338188171387 26.2517032623291
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924696207046509 2.197521686553955 23.767684936523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792820692062378 2.1746275424957275 23.539094924926758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791862964630127 2.018878936767578 21.98065185546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932628393173218 1.961164951324463 21.404911041259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915222644805908 2.2220191955566406 24.011714935302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925316095352173 2.4321632385253906 26.114164352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914170026779175 2.397747755050659 25.76889419555664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924052476882935 2.1570942401885986 23.36334800720215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792522668838501 1.8773949146270752 20.566471099853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792479157447815 2.2942962646484375 24.735441207885742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925763130187988 2.301734209060669 24.809917449951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925008535385132 2.0600178241729736 22.39267921447754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791625738143921 2.1625123023986816 23.416748046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921338081359863 2.1827147006988525 23.619279861450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791655421257019 2.1693692207336426 23.4853458404541
  batch 40 loss: 1.791655421257019, 2.1693692207336426, 23.4853458404541
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792153239250183 2.227020502090454 24.06235694885254
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923643589019775 2.11368727684021 22.929237365722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792030930519104 2.3044509887695312 24.83654022216797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791618824005127 2.179614305496216 23.58776092529297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924530506134033 2.0115511417388916 21.9079647064209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921451330184937 2.26332950592041 24.425439834594727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920458316802979 1.9072118997573853 20.864164352416992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918989658355713 2.2304601669311523 24.096500396728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791607141494751 2.2804176807403564 24.595783233642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792030930519104 2.1385889053344727 23.177919387817383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910940647125244 2.326204776763916 25.05314064025879
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923252582550049 2.4520421028137207 26.312746047973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922115325927734 2.2523651123046875 24.31586265563965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792532205581665 1.9024604558944702 20.817136764526367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918344736099243 2.0990805625915527 22.78264045715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925167083740234 1.994076132774353 21.733278274536133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926729917526245 2.4857900142669678 26.65057373046875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926533222198486 1.9735925197601318 21.528579711914062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930636405944824 2.3838069438934326 25.631134033203125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921172380447388 2.31384015083313 24.930519104003906
  batch 60 loss: 1.7921172380447388, 2.31384015083313, 24.930519104003906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926950454711914 2.1416258811950684 23.208953857421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916849851608276 1.965046763420105 21.44215202331543
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925065755844116 2.154162645339966 23.33413314819336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921075820922852 2.3274216651916504 25.066322326660156
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926361560821533 1.7405647039413452 19.198284149169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918586730957031 2.3728270530700684 25.520130157470703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.318445920944214 24.976234436035156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915716171264648 1.9973963499069214 21.765533447265625
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791893482208252 2.059216260910034 22.384056091308594
Total LOSS train 23.69094954270583 valid 23.66148853302002
CE LOSS train 1.7922248436854435 valid 0.447973370552063
Contrastive LOSS train 2.1898724831067597 valid 0.5148040652275085
EPOCH 295:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922052145004272 2.3057820796966553 24.850027084350586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915489673614502 2.450897216796875 26.300521850585938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924387454986572 2.1342971324920654 23.13541030883789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923449277877808 2.0675551891326904 22.467897415161133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920500040054321 2.1625304222106934 23.417354583740234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928224802017212 2.1101949214935303 22.894771575927734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791838526725769 2.3015294075012207 24.807130813598633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918957471847534 2.013444662094116 21.926342010498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791729211807251 2.266669273376465 24.45842170715332
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739821434021 2.0116360187530518 21.908100128173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926682233810425 2.3727564811706543 25.520233154296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923779487609863 2.2573447227478027 24.365825653076172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792905569076538 2.0827836990356445 22.620742797851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792588233947754 2.0996553897857666 22.789142608642578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911100387573242 2.2529451847076416 24.320560455322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926759719848633 2.3735344409942627 25.528018951416016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925426959991455 2.2984907627105713 24.777450561523438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923872470855713 2.392688751220703 25.719274520874023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922940254211426 1.9088462591171265 20.880756378173828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915945053100586 2.1127164363861084 22.918758392333984
  batch 20 loss: 1.7915945053100586, 2.1127164363861084, 22.918758392333984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924212217330933 2.076233148574829 22.554752349853516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924091815948486 2.069134473800659 22.483755111694336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792193055152893 1.5811848640441895 17.604042053222656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926615476608276 2.127324104309082 23.065902709960938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792362093925476 2.5108306407928467 26.90066909790039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792468786239624 2.0627238750457764 22.419708251953125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792819857597351 2.257103204727173 24.36385154724121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918624877929688 2.1446664333343506 23.238527297973633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932615280151367 2.178551197052002 23.578773498535156
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791521668434143 2.2356419563293457 24.147939682006836
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.323150396347046 25.02403450012207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914166450500488 2.355433225631714 25.345748901367188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924045324325562 2.239833354949951 24.190738677978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925217151641846 2.417086124420166 25.963382720947266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924777269363403 2.1779916286468506 23.57239532470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925751209259033 2.328845739364624 25.08103370666504
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924996614456177 2.2773983478546143 24.566482543945312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916252613067627 2.263867139816284 24.4302978515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921329736709595 2.252542734146118 24.31756019592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916548252105713 2.139928102493286 23.190935134887695
  batch 40 loss: 1.7916548252105713, 2.139928102493286, 23.190935134887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921526432037354 2.294567823410034 24.737831115722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792363166809082 2.1547229290008545 23.33959197998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920300960540771 2.135010004043579 23.14212989807129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916183471679688 2.1739046573638916 23.530664443969727
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924515008926392 1.9054996967315674 20.847448348999023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921444177627563 2.2296302318573 24.08844757080078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920454740524292 2.097990036010742 22.77194595336914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 1.9979642629623413 21.771541595458984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916067838668823 2.1249594688415527 23.041202545166016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920297384262085 2.1428985595703125 23.22101593017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910938262939453 2.212094783782959 23.91204261779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923245429992676 2.1733105182647705 23.525428771972656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922108173370361 1.9605964422225952 21.398176193237305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925307750701904 2.2005202770233154 23.797733306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918330430984497 1.9479857683181763 21.271692276000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925156354904175 1.6776939630508423 18.569454193115234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792671799659729 2.4407548904418945 26.200220108032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926517724990845 2.133855104446411 23.131202697753906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930622100830078 2.2920289039611816 24.713350296020508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921167612075806 2.24641752243042 24.25629234313965
  batch 60 loss: 1.7921167612075806, 2.24641752243042, 24.25629234313965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926937341690063 2.293635845184326 24.729053497314453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916840314865112 2.212761163711548 23.919294357299805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925053834915161 2.0830302238464355 22.6228084564209
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921067476272583 2.142392158508301 23.216028213500977
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926342487335205 1.659892201423645 18.391557693481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 2.295624256134033 24.74810028076172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917735576629639 2.1852571964263916 23.644344329833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915711402893066 1.9174154996871948 20.96572494506836
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918927669525146 1.9502257108688354 21.29414939880371
Total LOSS train 23.47374502328726 valid 22.663079738616943
CE LOSS train 1.7922239046830397 valid 0.44797319173812866
Contrastive LOSS train 2.1681521012232854 valid 0.48755642771720886
EPOCH 296:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922041416168213 2.174137830734253 23.53358268737793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791548728942871 2.391918182373047 25.710731506347656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924376726150513 2.2549326419830322 24.341764450073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923442125320435 2.1079680919647217 22.872026443481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.036132335662842 22.15337371826172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928224802017212 2.2498674392700195 24.29149627685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918379306793213 2.196213960647583 23.753976821899414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918959856033325 2.2978641986846924 24.770538330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 2.0734260082244873 22.525989532470703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917400598526 2.0171210765838623 21.96295166015625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926673889160156 2.3398261070251465 25.190929412841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923775911331177 2.3903162479400635 25.695539474487305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792905330657959 2.1882612705230713 23.675518035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925877571105957 2.349635601043701 25.288944244384766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911109924316406 2.3888285160064697 25.67939567565918
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926757335662842 2.4027035236358643 25.81970977783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925430536270142 2.3326809406280518 25.119352340698242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923870086669922 2.287753105163574 24.669918060302734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922941446304321 2.1884424686431885 23.67671775817871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915945053100586 2.1397013664245605 23.188610076904297
  batch 20 loss: 1.7915945053100586, 2.1397013664245605, 23.188610076904297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924206256866455 2.1491501331329346 23.28392219543457
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924089431762695 1.8475786447525024 20.26819610595703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921931743621826 1.988185167312622 21.674043655395508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926608324050903 2.092116355895996 22.713825225830078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923616170883179 2.485520124435425 26.647563934326172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924678325653076 2.1948931217193604 23.741397857666016
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928186655044556 2.4626224040985107 26.419042587280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918621301651 2.197206974029541 23.763931274414062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932602167129517 2.442920207977295 26.222463607788086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915221452713013 2.229599952697754 24.087522506713867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792529821395874 2.4262988567352295 26.055519104003906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914167642593384 2.3895957469940186 25.687374114990234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924034595489502 2.1753034591674805 23.545438766479492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925211191177368 2.2683987617492676 24.47650718688965
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924771308898926 2.3571360111236572 25.36383628845215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925745248794556 2.3913216590881348 25.70578956604004
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79249906539917 2.201535940170288 23.807859420776367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916251420974731 2.0284814834594727 22.076440811157227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921327352523804 2.0326621532440186 22.118755340576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916547060012817 1.917739748954773 20.969051361083984
  batch 40 loss: 1.7916547060012817, 1.917739748954773, 20.969051361083984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921522855758667 2.2235987186431885 24.028139114379883
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792362928390503 2.010786533355713 21.900226593017578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920295000076294 2.3069634437561035 24.861663818359375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916184663772583 2.093773603439331 22.729354858398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792451024055481 2.1463961601257324 23.25641441345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921444177627563 2.360884189605713 25.400985717773438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920454740524292 2.2486300468444824 24.27834701538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918976545333862 2.0924835205078125 22.716732025146484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916065454483032 2.2345871925354004 24.13747787475586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920291423797607 2.1839165687561035 23.631193161010742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910939455032349 2.4151480197906494 25.94257354736328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923238277435303 2.1838669776916504 23.630992889404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792210578918457 1.9620401859283447 21.412612915039062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925301790237427 2.201899528503418 23.811525344848633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918330430984497 2.332160234451294 25.113435745239258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925145626068115 2.298518180847168 24.77769660949707
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926709651947021 2.499803066253662 26.79070281982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792650818824768 2.2455925941467285 24.24857521057129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930608987808228 2.3862507343292236 25.655567169189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921159267425537 2.258437395095825 24.376489639282227
  batch 60 loss: 1.7921159267425537, 2.258437395095825, 24.376489639282227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926924228668213 2.1810030937194824 23.602724075317383
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791683554649353 1.9793139696121216 21.584823608398438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925045490264893 2.172175645828247 23.51426124572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921063899993896 2.3818535804748535 25.610641479492188
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926334142684937 1.8513410091400146 20.30604362487793
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791857361793518 2.067852258682251 22.470378875732422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917732000350952 2.103635311126709 22.828126907348633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915709018707275 1.9154906272888184 20.94647789001465
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.791892647743225 1.8968931436538696 20.76082420349121
Total LOSS train 23.997980734018178 valid 21.75145196914673
CE LOSS train 1.7922234828655537 valid 0.4479731619358063
Contrastive LOSS train 2.2205757232812733 valid 0.4742232859134674
EPOCH 297:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922037839889526 2.1044206619262695 22.836410522460938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791548252105713 2.5697386264801025 27.488933563232422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924370765686035 2.1959784030914307 23.752220153808594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923433780670166 2.1460626125335693 23.25296974182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792048692703247 2.09633207321167 22.75537109375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928214073181152 2.2752366065979004 24.54518699645996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791837453842163 2.2008423805236816 23.800260543823242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918955087661743 2.1926639080047607 23.718534469604492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917293310165405 1.9247022867202759 21.03875160217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917397022247314 2.1084020137786865 22.875761032104492
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926667928695679 2.3156144618988037 24.94881248474121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923773527145386 2.4047346115112305 25.839723587036133
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792904257774353 2.2028229236602783 23.82113265991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792587399482727 2.312079429626465 24.913381576538086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911107540130615 2.236423969268799 24.155349731445312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926747798919678 2.3659143447875977 25.451818466186523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925418615341187 2.3831067085266113 25.623607635498047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792386770248413 2.287459135055542 24.66697883605957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792293667793274 1.951409101486206 21.306385040283203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915947437286377 2.1973321437835693 23.764917373657227
  batch 20 loss: 1.7915947437286377, 2.1973321437835693, 23.764917373657227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924202680587769 2.040786027908325 22.200279235839844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79240882396698 2.145420551300049 23.246612548828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792192816734314 1.9109914302825928 20.90210723876953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926605939865112 2.195054292678833 23.743202209472656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923613786697388 2.2947096824645996 24.739459991455078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924680709838867 2.359156847000122 25.384037017822266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792818307876587 2.3538365364074707 25.3311824798584
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918621301651 2.2464585304260254 24.256446838378906
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793259620666504 2.3227686882019043 25.020946502685547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79152250289917 2.090261697769165 22.69413948059082
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925297021865845 2.4063448905944824 25.855979919433594
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791417121887207 2.1344106197357178 23.13552474975586
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924035787582397 2.1261837482452393 23.054241180419922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925208806991577 2.257648229598999 24.369003295898438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924772500991821 2.416722297668457 25.959699630737305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792574405670166 1.9953913688659668 21.746488571166992
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924988269805908 2.1669459342956543 23.461959838867188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916254997253418 1.7774266004562378 19.56589126586914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921327352523804 2.331489086151123 25.107025146484375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 2.368414878845215 25.47580337524414
  batch 40 loss: 1.79165518283844, 2.368414878845215, 25.47580337524414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921524047851562 2.2872300148010254 24.664451599121094
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923626899719238 1.8992016315460205 20.784378051757812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920299768447876 2.172980546951294 23.521835327148438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916189432144165 2.1163763999938965 22.95538330078125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792451024055481 2.189976692199707 23.692218780517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921440601348877 1.7648950815200806 19.44109535217285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204523563385 1.785245418548584 19.644500732421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918980121612549 2.22792387008667 24.071138381958008
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916070222854614 2.2776074409484863 24.56768035888672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920295000076294 1.9857426881790161 21.649457931518555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910948991775513 2.350644826889038 25.297544479370117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923238277435303 2.2986984252929688 24.779308319091797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792210340499878 2.174561023712158 23.537818908691406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925302982330322 2.161957263946533 23.41210174560547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918332815170288 2.3369436264038086 25.161270141601562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792514681816101 2.053642988204956 22.32894515991211
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926708459854126 1.9956330060958862 21.749000549316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926506996154785 2.237412214279175 24.166772842407227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930606603622437 2.4971840381622314 26.76490020751953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921161651611328 2.217796802520752 23.970083236694336
  batch 60 loss: 1.7921161651611328, 2.217796802520752, 23.970083236694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926925420761108 2.2054836750030518 23.8475284576416
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916843891143799 2.2491185665130615 24.28287124633789
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925046682357788 1.9729114770889282 21.52161979675293
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921066284179688 2.267662763595581 24.468734741210938
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926336526870728 1.8484556674957275 20.277189254760742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918579578399658 2.3087453842163086 24.87931251525879
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917736768722534 2.279818058013916 24.58995246887207
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915716171264648 1.8967748880386353 20.759319305419922
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918931245803833 1.7601337432861328 19.393230438232422
Total LOSS train 23.636374869713418 valid 22.4054536819458
CE LOSS train 1.7922233709922204 valid 0.4479732811450958
Contrastive LOSS train 2.1844151460207426 valid 0.4400334358215332
EPOCH 298:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922039031982422 2.1980719566345215 23.772924423217773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915489673614502 2.612208127975464 27.913631439208984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924370765686035 2.0247511863708496 22.039949417114258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923437356948853 2.2050974369049072 23.84331703186035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.1270039081573486 23.062088012695312
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928214073181152 2.2532551288604736 24.32537269592285
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918380498886108 2.224093198776245 24.03277015686035
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791896104812622 2.0795600414276123 22.58749771118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917296886444092 2.1982781887054443 23.774511337280273
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917404174804688 2.0391454696655273 22.183195114135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926671504974365 2.363895893096924 25.431625366210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923774719238281 2.2421233654022217 24.213611602783203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792904257774353 1.9946725368499756 21.7396297454834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925870418548584 2.018709897994995 21.979686737060547
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911118268966675 2.181173801422119 23.602848052978516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926746606826782 2.25356125831604 24.32828712463379
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925420999526978 2.3483662605285645 25.276203155517578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923868894577026 2.312525749206543 24.917644500732422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922941446304321 2.0905795097351074 22.698089599609375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791595458984375 2.2490663528442383 24.282258987426758
  batch 20 loss: 1.791595458984375, 2.2490663528442383, 24.282258987426758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924203872680664 2.1920065879821777 23.712486267089844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924087047576904 2.140777587890625 23.200183868408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921929359436035 2.033865213394165 22.130844116210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926604747772217 2.086052894592285 22.653188705444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923616170883179 1.861287236213684 20.405235290527344
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924680709838867 2.195324182510376 23.745708465576172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928184270858765 2.481318473815918 26.606002807617188
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918626070022583 1.7544007301330566 19.33586883544922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932592630386353 1.8169482946395874 19.96274185180664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915230989456177 2.231102466583252 24.10254669189453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792529582977295 2.3848490715026855 25.641021728515625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914179563522339 2.338528871536255 25.176706314086914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924036979675293 2.032491683959961 22.117321014404297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925212383270264 2.206027030944824 23.85279083251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924773693084717 2.306471586227417 24.857192993164062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925745248794556 2.118868827819824 22.98126220703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924991846084595 2.1772656440734863 23.565155029296875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916259765625 2.177384376525879 23.56546974182129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792133092880249 2.284670352935791 24.638835906982422
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916557788848877 2.263373613357544 24.425392150878906
  batch 40 loss: 1.7916557788848877, 2.263373613357544, 24.425392150878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921524047851562 2.258794069290161 24.38009262084961
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792362928390503 1.9288560152053833 21.080923080444336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920304536819458 2.251028299331665 24.30231285095215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916197776794434 1.755698561668396 19.34860610961914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79245126247406 2.1584484577178955 23.376935958862305
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792144775390625 2.2145841121673584 23.937986373901367
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920457124710083 2.2925097942352295 24.717144012451172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791898488998413 1.9512213468551636 21.30411148071289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916077375411987 2.157029151916504 23.36189842224121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920304536819458 2.110598564147949 22.89801597595215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910959720611572 2.1840126514434814 23.631221771240234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923245429992676 2.1830341815948486 23.622665405273438
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922108173370361 2.208230495452881 23.8745174407959
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925306558609009 2.071931838989258 22.51184844970703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918341159820557 2.287832021713257 24.670154571533203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925149202346802 2.044287919998169 22.235393524169922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926710844039917 2.4387574195861816 26.18024444580078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926509380340576 2.013075113296509 21.92340087890625
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930606603622437 2.4151253700256348 25.944313049316406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792116641998291 2.2821147441864014 24.613264083862305
  batch 60 loss: 1.792116641998291, 2.2821147441864014, 24.613264083862305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79269278049469 2.10050368309021 22.7977294921875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916849851608276 2.1356990337371826 23.1486759185791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792504906654358 2.031298875808716 22.105493545532227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921072244644165 2.4212183952331543 26.004291534423828
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792633295059204 1.573394536972046 17.526578903198242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791858434677124 2.1318235397338867 23.11009407043457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917741537094116 2.086160182952881 22.653377532958984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915722131729126 1.834686279296875 20.13843536376953
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918938398361206 1.7819855213165283 19.61174774169922
Total LOSS train 23.41813718355619 valid 21.378413677215576
CE LOSS train 1.7922237084462092 valid 0.44797345995903015
Contrastive LOSS train 2.1625913638335006 valid 0.4454963803291321
EPOCH 299:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922041416168213 2.125113010406494 23.043333053588867
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915494441986084 2.398658275604248 25.778133392333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792436957359314 2.0501160621643066 22.293596267700195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923434972763062 2.2070915699005127 23.863258361816406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920491695404053 2.3219268321990967 25.01131820678711
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928208112716675 2.2685134410858154 24.477954864501953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79183828830719 2.4311153888702393 26.102991104125977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918959856033325 2.1643075942993164 23.434972763061523
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917301654815674 2.1660025119781494 23.45175552368164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917405366897583 2.100332021713257 22.795061111450195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926666736602783 2.3271052837371826 25.063720703125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792377233505249 2.2828474044799805 24.620851516723633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929033041000366 2.0529825687408447 22.322729110717773
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925866842269897 2.361114263534546 25.403730392456055
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911123037338257 2.347092866897583 25.262041091918945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792673945426941 2.3673715591430664 25.466388702392578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79254150390625 2.3585760593414307 25.3783016204834
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923864126205444 2.365543842315674 25.447824478149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792293667793274 2.0919976234436035 22.712268829345703
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915953397750854 2.065877914428711 22.450374603271484
  batch 20 loss: 1.7915953397750854, 2.065877914428711, 22.450374603271484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924201488494873 2.0599215030670166 22.391633987426758
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924083471298218 2.144282579421997 23.235233306884766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921924591064453 2.113938570022583 22.931577682495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926597595214844 2.321441411972046 25.0070743560791
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923611402511597 2.501521110534668 26.807571411132812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924671173095703 2.1805875301361084 23.598342895507812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792817234992981 2.4377856254577637 26.17067527770996
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918623685836792 2.24845552444458 24.276416778564453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.793257713317871 2.4380578994750977 26.17383575439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915228605270386 1.8167176246643066 19.958698272705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925293445587158 2.4041595458984375 25.834125518798828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791418194770813 2.381533145904541 25.606748580932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792403221130371 2.08028507232666 22.595252990722656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925204038619995 1.8705830574035645 20.498350143432617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792476773262024 1.9440796375274658 21.233272552490234
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925738096237183 2.4255855083465576 26.048429489135742
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924985885620117 2.1135523319244385 22.928020477294922
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916259765625 2.191389560699463 23.705520629882812
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921326160430908 2.305631160736084 24.848445892333984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916556596755981 2.3603880405426025 25.395536422729492
  batch 40 loss: 1.7916556596755981, 2.3603880405426025, 25.395536422729492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921520471572876 2.2003328800201416 23.795480728149414
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923623323440552 1.9000904560089111 20.79326629638672
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920300960540771 2.198695659637451 23.778987884521484
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916195392608643 2.1426773071289062 23.218393325805664
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924505472183228 2.066460132598877 22.457050323486328
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921440601348877 2.0477426052093506 22.26957130432129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79204523563385 2.0553481578826904 22.34552764892578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918983697891235 2.1638307571411133 23.430206298828125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916076183319092 2.10588002204895 22.850406646728516
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792029857635498 2.1580984592437744 23.373014450073242
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910963296890259 2.3230273723602295 25.02136993408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923234701156616 2.2873456478118896 24.665781021118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792210340499878 1.918094277381897 20.97315216064453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792529582977295 2.0839498043060303 22.632028579711914
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918338775634766 2.54290771484375 27.220911026000977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925138473510742 2.086080312728882 22.653316497802734
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926700115203857 2.406877040863037 25.861440658569336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926502227783203 2.1517221927642822 23.309871673583984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930593490600586 2.444793701171875 26.240997314453125
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921158075332642 2.2370216846466064 24.16233253479004
  batch 60 loss: 1.7921158075332642, 2.2370216846466064, 24.16233253479004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792691707611084 2.314335346221924 24.936044692993164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916847467422485 2.0267324447631836 22.059009552001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925041913986206 2.1820590496063232 23.613094329833984
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921067476272583 2.3834619522094727 25.626726150512695
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.792633295059204 1.7917418479919434 19.710052490234375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918572425842285 2.0453715324401855 22.245573043823242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917730808258057 1.943005084991455 21.22182273864746
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915712594985962 1.6700396537780762 18.491968154907227
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918919324874878 1.5737563371658325 17.529455184936523
Total LOSS train 23.85574464064378 valid 19.872204780578613
CE LOSS train 1.7922232774587779 valid 0.44797298312187195
Contrastive LOSS train 2.2063521440212543 valid 0.39343908429145813
EPOCH 300:
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922025918960571 1.9376811981201172 21.16901397705078
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915488481521606 2.666156053543091 28.453109741210938
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792435884475708 2.049300193786621 22.285438537597656
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792341947555542 2.2010498046875 23.802839279174805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920477390289307 2.4289121627807617 26.08116912841797
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7928192615509033 2.1264877319335938 23.057697296142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918373346328735 2.3460052013397217 25.251890182495117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918950319290161 1.999912977218628 21.791025161743164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917288541793823 1.9968284368515015 21.760013580322266
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791739583015442 1.9694533348083496 21.48627471923828
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926647663116455 2.0289413928985596 22.08207893371582
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923758029937744 1.972748041152954 21.519855499267578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7929023504257202 1.7514704465866089 19.307605743408203
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925854921340942 2.2794368267059326 24.58695411682129
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7911121845245361 2.130052328109741 23.091636657714844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792672872543335 2.475280284881592 26.54547691345215
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925403118133545 2.3053126335144043 24.845666885375977
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923853397369385 2.277656316757202 24.56894874572754
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922923564910889 2.1841113567352295 23.633405685424805
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915948629379272 2.323899745941162 25.030593872070312
  batch 20 loss: 1.7915948629379272, 2.323899745941162, 25.030593872070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924189567565918 1.9168413877487183 20.960832595825195
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924070358276367 2.141594648361206 23.208354949951172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792191505432129 2.0941834449768066 22.734024047851562
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926586866378784 2.0787999629974365 22.580657958984375
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923601865768433 2.2317748069763184 24.110109329223633
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792466402053833 2.2673439979553223 24.465906143188477
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792816400527954 2.1512863636016846 23.305679321289062
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918620109558105 2.2837324142456055 24.629186630249023
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7932567596435547 2.0670831203460693 22.464088439941406
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791522741317749 2.220635175704956 23.997875213623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925279140472412 2.264941930770874 24.44194793701172
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7914177179336548 2.158261299133301 23.37403106689453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924022674560547 2.2097201347351074 23.889604568481445
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792519211769104 2.3136775493621826 24.92929458618164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924755811691284 1.8957606554031372 20.75008201599121
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925726175308228 1.906461477279663 20.857187271118164
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924973964691162 2.1312761306762695 23.10525894165039
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791625738143921 2.1964166164398193 23.75579261779785
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792131781578064 2.305476188659668 24.846893310546875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79165518283844 2.060285806655884 22.394512176513672
  batch 40 loss: 1.79165518283844, 2.060285806655884, 22.394512176513672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921513319015503 2.039029121398926 22.18244171142578
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923613786697388 1.979202151298523 21.584383010864258
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920290231704712 2.218153238296509 23.973560333251953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916189432144165 2.145167827606201 23.243297576904297
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7924495935440063 1.9685490131378174 21.47793960571289
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.79214346408844 2.1584434509277344 23.376577377319336
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7920445203781128 2.2002885341644287 23.79492950439453
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918974161148071 2.355135202407837 25.34324836730957
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791607141494751 2.3848230838775635 25.63983726501465
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792028784751892 2.117250919342041 22.964536666870117
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7910958528518677 2.374642848968506 25.537525177001953
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7923226356506348 2.246398448944092 24.25630760192871
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7922090291976929 2.030477285385132 22.096982955932617
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925283908843994 2.137939929962158 23.171926498413086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7918325662612915 2.247354030609131 24.26537322998047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792513132095337 2.045696973800659 22.249483108520508
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926688194274902 2.348007917404175 25.272748947143555
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.792648434638977 2.1563360691070557 23.356008529663086
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7930577993392944 2.4748570919036865 26.541629791259766
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921154499053955 2.0868349075317383 22.660465240478516
  batch 60 loss: 1.7921154499053955, 2.0868349075317383, 22.660465240478516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7926905155181885 2.3643910884857178 25.436601638793945
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7916843891143799 2.2263574600219727 24.055259704589844
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7925031185150146 2.098076343536377 22.773265838623047
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7921056747436523 1.852725625038147 20.31936264038086
attention x_bar :  torch.Size([6, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([6, 2048, 7, 7])
Loss :  1.7926316261291504 1.742218255996704 19.214813232421875
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.791857361793518 2.4575586318969727 26.367443084716797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7917733192443848 2.3853759765625 25.645532608032227
attention x_bar :  torch.Size([10, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([10, 2048, 7, 7])
Loss :  1.7915717363357544 2.0709521770477295 22.501094818115234
attention x_bar :  torch.Size([5, 2048, 6, 7, 7])
attention after mean x_bar :  torch.Size([5, 2048, 7, 7])
Loss :  1.7918925285339355 2.1093199253082275 22.88509178161621
Total LOSS train 23.383700297429012 valid 24.349790573120117
CE LOSS train 1.7922222852706908 valid 0.4479731321334839
Contrastive LOSS train 2.1591477907620944 valid 0.5273299813270569
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 0.134 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 0.243 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 0.423 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 0.571 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 0.681 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 0.767 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 0.931 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 1.048 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 1.181 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 1.275 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 1.400 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 1.517 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 1.642 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 1.767 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 1.884 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 2.009 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 2.009 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 2.009 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 2.134 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 2.251 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 2.376 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 2.501 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 2.501 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 2.618 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 2.743 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 2.868 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 2.986 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.111 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.111 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.236 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 3.353 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.400 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.478 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.603 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: | 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: / 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: - 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb: \ 3.717 MB of 3.717 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train contrastive loss ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train cross-entropy loss ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               train loss ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     val contrastive loss ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:   val cross-entropy loss ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 val loss ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            best_val_loss 18.08505
wandb:                    epoch 300
wandb:                       lr 0.0
wandb:   train contrastive loss 2.15915
wandb: train cross-entropy loss 1.79222
wandb:               train loss 23.3837
wandb:     val contrastive loss 0.52733
wandb:   val cross-entropy loss 0.44797
wandb:                 val loss 24.34979
wandb: 
wandb: Synced leafy-terrain-30: https://wandb.ai/harsh21122/part_segmentation/runs/1220jycj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230209_234622-1220jycj/logs
